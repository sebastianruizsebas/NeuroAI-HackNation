[
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "1 CHAPTER Introduction TESTThe main focus of machine learning is making decisions or predictions based on data. Thereareanumberofotherfieldswithsignificantoverlapintechnique,butdifferencein focus: in economics and psychology, the goal is to discover underlying causal processes Thisstoryparaphrased andinstatisticsitistofindamodelthatfitsadatasetwell.Inthosefields,theendproduct fromaposton9/4/12 atandrewgelman.com isamodel. Inmachinelearning,weoftenfitmodels,butasameanstotheendofmaking"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "goodpredictionsordecisions. As machine-learning (ML) methods have improved in their capability and scope, ML has become the best way, measured in terms of speed, human engineering time, and ro- bustness,tomakemanyapplications.Greatexamplesarefacedetectionandspeechrecog- nitionandmanykindsoflanguage-processingtasks.Almostanyapplicationthatinvolves understandingdataorsignalsthatcomefromtherealworldcanbebestaddressedusing machinelearning."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Onecrucialaspectofmachinelearningapproachestosolvingproblemsisthathuman andoftenundervalued engineeringplaysanimportantrole. Ahumanstillhastoframetheproblem: acquireand organizedata,designaspaceofpossiblesolutions,selectalearningalgorithmanditspa- rameters,applythealgorithmtothedata,validatetheresultingsolutiontodecidewhether it’sgoodenoughtouse,etc. Thesestepsareofgreatimportance. Theconceptualbasisoflearningfromdataistheproblemofinduction:Whydowethink BertrandRussellismy that previously seen"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "data will help us predict the future? This is a serious philosophical hero. –lpk problemoflongstanding. Wewilloperationalizeitbymakingassumptions,suchasthat alltrainingdataareIID(independentandidenticallydistributed)andthatquerieswillbe drawnfromthesamedistributionasthetrainingdata,orthattheanswercomesfromaset ofpossibleanswersknowninadvance. Ingeneral,weneedtosolvethesetwoproblems: estimation: Whenwehavedatathatarenoisyreflectionsofsomeunderlyingquan- • tity of interest, we have to aggregate"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "the data and make estimates or predictions aboutthequantity. Howdowedealwiththefactthat,forexample,thesametreat- mentmayendupwithdifferentresultsondifferenttrials? Howcanwepredicthow wellanestimatemaycomparetofutureresults? generalization:Howcanwepredictresultsofasituationorexperimentthatwehave • neverencounteredbeforeinourdataset? 4MIT6.036 Fall2019 5 Wecandescribeproblemsandtheirsolutionsusingsixcharacteristics, threeofwhich characterizetheproblemandthreeofwhichcharacterizethesolution: 1."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Problemclass:Whatisthenatureofthetrainingdataandwhatkindsofquerieswill bemadeattestingtime? 2. Assumptions: What do we know about the source of the data or the form of the solution? 3. Evaluation criteria: What is the goal of the prediction or estimation system? How will the answers to individual queries be evaluated? How will the overall perfor- manceofthesystembemeasured? 4. Modeltype: Willanintermediatemodelbemade? Whataspectsofthedatawillbe modeled? Howwillthemodelbeusedtomakepredictions?"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "5. Modelclass:Whatparticularparametricclassofmodelswillbeused?Whatcriterion willweusetopickaparticularmodelfromthemodelclass? 6. Algorithm: What computational process will be used to fit the model to the data and/ortomakepredictions? Withoutmakingsomeassumptionsaboutthenatureoftheprocessgeneratingthedata,we cannotperformgeneralization. Inthefollowingsections,weelaborateontheseideas. Don’tfeelyouhave tomemorizeallthese kindsoflearning,etc. 1 Problem class Wejustwantyouto haveaveryhigh-level"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "viewof(partof)the Therearemanydifferentproblemclassesinmachinelearning. Theyvaryaccordingtowhat breadthofthefield. kindofdataisprovidedandwhatkindofconclusionsaretobedrawnfromit. Fivestan- dardproblemclassesaredescribedbelow,toestablishsomenotationandterminology. In this course, we will focus on classification and regression (two examples of super- visedlearning),andwilltouchonreinforcementlearningandsequencelearning. 1.1 Supervisedlearning"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Theideaofsupervisedlearningisthatthelearningsystemisgiveninputsandtoldwhich specificoutputsshouldbeassociatedwiththem. Wedivideupsupervisedlearningbased onwhethertheoutputsaredrawnfromasmallfiniteset(classification)oralargefiniteor continuousset(regression). 1.1.1 Classification Training data D is in the form of a set of pairs {(x(1),y(1)),...,(x(n),y(n))} where x(i) n representsanobjecttobeclassified,mosttypicallyad-dimensionalvectorofrealand/or discretevalues, andy(i)"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "isanelementofadiscretesetofvalues. Theyvaluesaresome- Manytextbooksusex i timescalledtargetvalues. andt insteadofx(i) i Aclassificationproblemisbinaryortwo-classify(i)isdrawnfromasetoftwopossible andy(i). Wefindthat notationsomewhatdif- values;otherwise,itiscalledmulti-class. ficulttomanagewhen The goal in a classification problem is ultimately, given a new input value x(n+1), to x(i) isitselfavectorand predictthevalueofy(n+1). weneedtotalkabout"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Classificationproblemsareakindofsupervisedlearning,becausethedesiredoutput(or itselements. Theno- class)y(i)isspecifiedforeachofthetrainingexamplesx(i). tationweareusingis standardinsomeother partsofthemachine- learningliterature. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 6 1.1.2 Regression Regressionislikeclassification,exceptthaty(i) R k. ∈ 1.2 Unsupervisedlearning Unsupervisedlearningdoesn’tinvolvelearningafunctionfrominputstooutputsbasedon asetofinput-outputpairs."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Instead,oneisgivenadatasetandgenerallyexpectedtofind somepatternsorstructureinherentinit. 1.2.1 Densityestimation Givensamplesx(1),...,x(n) R D drawnIIDfromsomedistributionPr(X),thegoalisto IIDstandsforindepen- ∈ predicttheprobabilityPr(x(n+1))ofanelementdrawnfromthesamedistribution.Density dentandidenticallydis- tributed,whichmeans estimation sometimes plays a role as a “subroutine” in the overall learning method for thattheelementsinthe supervisedlearning,aswell. setarerelatedinthe"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "sensethattheyallcome fromthesameunder- 1.2.2 Clustering lyingprobabilitydistri- Givensamplesx(1),...,x(n) R D,thegoalistofindapartitioning(or“clustering”)ofthe bution,butnotinany ∈ otherways. samplesthatgroupstogethersamplesthataresimilar.Therearemanydifferentobjectives, dependingonthedefinitionofthesimilaritybetweensamplesandexactlywhatcriterion is to be used (e.g., minimize the average distance between elements inside a cluster and"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "maximizetheaveragedistancebetweenelementsacrossclusters).Othermethodsperform a“soft”clustering,inwhichsamplesmaybeassigned0.9membershipinoneclusterand 0.1inanother.Clusteringissometimesusedasastepindensityestimation,andsometimes tofindusefulstructureindata. 1.2.3 Dimensionalityreduction Given samples x(1),...,x(n) R D, the problem is to re-represent them as points in a d- ∈ dimensionalspace,whered<D. Thegoalistypicallytoretaininformationinthedataset"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "thatwill,e.g.,allowelementsofoneclasstobediscriminatedfromanother. Dimensionality reduction is a standard technique which is particularly useful for vi- sualizingorunderstandinghigh-dimensionaldata. Ifthegoalisultimatelytoperformre- gressionorclassificationonthedataafterthedimensionalityisreduced,itisusuallybestto articulateanobjectivefortheoverallpredictionproblemratherthantofirstdodimension- ality reduction without knowing which dimensions will be important for the prediction task. 1.3"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Reinforcementlearning In reinforcement learning, the goal is to learn a mapping from input values x to output values y, but without a direct supervision signal to specify which output values y are bestforaparticularinput. Thereisnotrainingsetspecifiedapriori. Instead,thelearning problemisframedasanagentinteractingwithanenvironment,inthefollowingsetting: Theagentobservesthecurrentstate,x(0). • Itselectsanaction,y(0). • Itreceivesareward,r(0),whichdependsonx(0)andpossiblyy(0). •"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Theenvironmenttransitionsprobabilisticallytoanewstate,x(1),withadistribution • thatdependsonlyonx(0)andy(0). LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 7 Theagentobservesthecurrentstate,x(1). • ... • The goal is to find a policy π, mapping x to y, (that is, states to actions) such that some long-termsumoraverageofrewardsrismaximized. Thissettingisverydifferentfromeithersupervisedlearningorunsupervisedlearning, becausetheagent’sactionchoicesaffectbothitsrewardanditsabilitytoobservetheenvi-"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "ronment. Itrequirescarefulconsiderationofthelong-termeffectsofactions,aswellasall oftheotherissuesthatpertaintosupervisedlearning. 1.4 Sequencelearning Insequencelearning,thegoalistolearnamappingfrominputsequencesx ,...,x tooutput 0 n sequences y ,...,y . The mapping is typically represented as a state machine, with one 1 m function f used to compute the next hidden internal state given the input, and another functiongusedtocomputetheoutputgiventhecurrenthiddenstate."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Itissupervisedinthesensethatwearetoldwhatoutputsequencetogenerateforwhich inputsequence,buttheinternalfunctionshavetobelearnedbysomemethodotherthan directsupervision,becausewedon’tknowwhatthehiddenstatesequenceis. 1.5 Othersettings Therearemanyotherproblemsettings. Hereareafew. In semi-supervised learning, we have a supervised-learning training set, but there may be an additional set of x(i) values with no known y(i). These values can still be used to"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "improvelearningperformanceiftheyaredrawnfromPr(X)thatisthemarginalofPr(X,Y) thatgovernstherestofthedataset. Inactivelearning,itisassumedtobeexpensivetoacquirealabely(i)(imagineaskinga humantoreadanx-rayimage),sothelearningalgorithmcansequentiallyaskforparticular inputsx(i)tobelabeled,andmustcarefullyselectqueriesinordertolearnaseffectivelyas possiblewhileminimizingthecostoflabeling. Intransferlearning(alsocalledmeta-learning),therearemultipletasks,withdatadrawn"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "fromdifferent,butrelated,distributions. Thegoalisforexperiencewithprevioustasksto applytolearningacurrenttaskinawaythatrequiresdecreasedexperiencewiththenew task. 2 Assumptions Thekindsofassumptionsthatwecanmakeaboutthedatasourceorthesolutioninclude: Thedataareindependentandidenticallydistributed. • ThedataaregeneratedbyaMarkovchain. • Theprocessgeneratingthedatamightbeadversarial. • The “true” model that is generating the data can be perfectly described by one of •"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "someparticularsetofhypotheses. Theeffectofanassumptionisoftentoreducethe“size”or“expressiveness”ofthespaceof possiblehypothesesandthereforereducetheamountofdatarequiredtoreliablyidentify anappropriatehypothesis. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 8 3 Evaluation criteria Oncewehavespecifiedaproblemclass,weneedtosaywhatmakesanoutputorthean- swertoaquerygood,giventhetrainingdata. Wespecifyevaluationcriteriaattwolevels:"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "howanindividualpredictionisscored,andhowtheoverallbehaviorofthepredictionor estimationsystemisscored. The quality of predictions from a learned model is often expressed in terms of a loss function. A loss function L(g,a) tells you how much you will be penalized for making a guessgwhentheanswerisactuallya. Therearemanypossiblelossfunctions. Hereare somefrequentlyusedexamples: 0-1Lossappliestopredictionsdrawnfromfinitedomains. Iftheactualvaluesare • drawnfromacontin- 0 ifg=a uousdistribution,the"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "L(g,a)= probabilitytheywould (cid:14)1 otherwise everbeequaltosome predictedgis0(except forsomeweirdcases). Squaredloss • L(g,a)=(g−a)2 Linearloss • L(g,a)=|g−a| Asymmetric loss Consider a situation in which you are trying to predict whether • someoneishavingaheartattack. Itmightbemuchworsetopredict“no”whenthe answerisreally“yes”,thantheotherwayaround. 1 ifg=1anda=0 L(g,a)= 10 ifg=0anda=1 0 otherwise  Anygivenpredictionrulewillusuallybeevaluatedbasedonmultiplepredictionsand"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "thelossofeachone. Atthislevel,wemightbeinterestedin: Minimizingexpectedlossoverallthepredictions(alsoknownasrisk) • Minimizingmaximumloss: thelossoftheworstprediction • Minimizing or bounding regret: how much worse this predictor performs than the • bestonedrawnfromsomeclass Characterizingasymptoticbehavior:howwellthepredictorwillperforminthelimit • ofinfinitetrainingdata Findingalgorithmsthatareprobablyapproximatelycorrect: theyprobablygenerate • ahypothesisthatisrightmostofthetime."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "Thereisatheoryofrationalagencythatarguesthatyoushouldalwaysselecttheaction thatminimizestheexpectedloss. Thisstrategywill,forexample,makeyouthemostmoney in the long run, in a gambling setting. Expected loss is also sometimes called risk in the Ofcourse,thereare machine-learningliterature,butthattermmeansotherthingsineconomicsorotherparts othermodelsforac- tionselectionandit’s ofdecisiontheory,sobecareful...it’sriskytouseit. Wewill,mostofthetime,concentrate clearthatpeopledonot onthiscriterion."
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "always(ormaybeeven often)selectactionsthat followthisrule. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 9 4 Model type Recall that the goal of a machine-learning system is typically to estimate or generalize, basedondataprovided.Below,weexaminetheroleofmodel-makinginmachinelearning. 4.1 Nomodel In some simple cases, in response to queries, we can generate predictions directly from the training data, without the construction of any intermediate model. For example, in regression or"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "classification, we might generate an answer to a new query by averaging answerstorecentqueries,asinthenearestneighbormethod. 4.2 Predictionrule Thistwo-stepprocessismoretypical: 1. “Fit”amodeltothetrainingdata 2. Usethemodeldirectlytomakepredictions Inthepredictionrulesettingofregressionorclassification, themodelwillbesomehy- pothesis or prediction rule y = h(x;θ) for some functional form h. The idea is that θ is avectorofoneormoreparametervaluesthatwillbedeterminedbyfittingthemodelto the"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "training data and then be held fixed. Given a new x(n+1), we would then make the predictionh(x(n+1);θ). Wewritef(a;b)tode- The fitting process is often articulated as an optimization problem: Find a value of θ scribeafunctionthatis usuallyappliedtoasin- thatminimizessomecriterioninvolvingθandthedata. Anoptimalstrategy,ifweknew gleargumenta,butisa the actual underlying distribution on our data, Pr(X,Y) would be to predict the value of memberofaparamet- y that minimizes the expected loss, which"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "is also known as the test error. If we don’t have ricfamilyoffunctions, that actual underlying distribution, or even an estimate of it, we can take the approach withtheparticularfunc- tiondeterminedbypa- of minimizing the training error: that is, finding the prediction rule h that minimizes the rametervalueb. So, averagelossonourtrainingdataset. So,wewouldseekθthatminimizes forexample,wemight writeh(x;p) = xp to 1 n describeafunctionofa E n (θ)= n L(h(x(i);θ),y(i)) , singleargumentthatis X i=1"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "parameterizedbyp. wherethelossfunctionL(g,a)measureshowbaditwouldbetomakeaguessofgwhen theactualvalueisa. Wewillfindthatminimizingtrainingerroraloneisoftennotagoodchoice:itispossible toemphasizefittingthecurrentdatatoostronglyandendupwithahypothesisthatdoes notgeneralizewellwhenpresentedwithnewxvalues. 5 Model class and parameter fitting AmodelclassMisasetofpossiblemodels,typicallyparameterizedbyavectorofparam- eters Θ. What assumptions will we make about the form of the model? When solving a"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "regression problem using a prediction-rule approach, we might try to find a linear func- tion h(x;θ,θ ) = θTx+θ that fits our data well. In this example, the parameter vector 0 0 Θ=(θ,θ ). 0 Forproblemtypessuchasdiscriminationandclassification,therearehugenumbersof modelclassesthathavebeenconsidered...we’llspendmuchofthiscourseexploringthese modelclasses,especiallyneuralnetworksmodels. Wewillalmostcompletelyrestrictour LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 10"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "attentiontomodelclasseswithafixed,finitenumberofparameters.Modelsthatrelaxthis assumptionarecalled“non-parametric”models. Howdoweselectamodelclass? Insomecases,themachine-learningpractitionerwill haveagoodideaofwhatanappropriatemodelclassis,andwillspecifyitdirectly.Inother cases, we may consider several model classes. In such situations, we are solving a model selectionproblem: model-selectionistopickamodelclassMfroma(usuallyfinite)setof"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "possiblemodelclasses;modelfittingistopickaparticularmodelinthatclass,specifiedby parametersθ. 6 Algorithm Once we have described a class of models and a way of scoring a model given data, we haveanalgorithmicproblem:whatsequenceofcomputationalinstructionsshouldwerun in order to find a good model from our class? For example, determining the parameter vectorθwhichminimizesE (θ)mightbedoneusingafamiliarleast-squaresminimization n algorithm,whenthemodelhisafunctionbeingfittosomedatax. Sometimes we"
  },
  {
    "file": "mit_ocw_ch_1.pdf",
    "chunk": "can use software that was designed, generically, to perform optimiza- tion. In many other cases, we use algorithms that are specialized for machine-learning problems,orforparticularhypothesesclasses. Somealgorithmsarenoteasilyseenastryingtooptimizeaparticularcriterion. Infact, thefirstalgorithmwestudyforfindinglinearclassifiers,theperceptronalgorithm,hasthis character. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "10 CHAPTER Sequential models So far, we have limited our attention to domains in which each output y is assumed to havebeengeneratedasafunctionofanassociatedinputx,andourhypotheseshavebeen “pure”functions,inwhichtheoutputdependsonlyontheinput(andtheparameterswe havelearnedthatgovernthefunction’sbehavior). Inthenextfewweeks,wearegoingto considercasesinwhichourmodelsneedtogobeyondfunctions. Inrecurrentneuralnetworks,thehypothesisthatwelearnisnotafunctionofasingle •"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "input,butofthewholesequenceofinputsthatthepredictorhasreceived. Inreinforcementlearning,thehypothesisiseitheramodelofadomain(suchasagame) • as a recurrent system or a policy which is a pure function, but whose loss is deter- minedbythewaysinwhichthepolicyinteractswiththedomainovertime. Beforeweengagewiththoseformsoflearning,wewillstudymodelsofsequentialor recurrentsystemsthatunderliethelearningmethods. 1 State machines Astatemachineisadescriptionofaprocess(computational, physical,"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "economic)interms Thisissuchapervasive ofitspotentialsequencesofstates. ideathatithasbeen givenmanynamesin Thestateofasystemisdefinedtobeallyouwouldneedtoknowaboutthesystemto manysubareasofcom- predictitsfuturetrajectoriesaswellaspossible. Itcouldbethepositionandvelocityofan puterscience,control objectorthelocationsofyourpiecesonagameboard,orthecurrenttrafficdensitiesona theory,physics,etc., highwaynetwork. including: automaton, transducer,dynamicalsys- Formally,wedefineastatemachineas(S,X,Y,s"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": ",f,g)where 0 tem,system,etc. Sisafiniteorinfinitesetofpossiblestates; • Thereareahugenum- Xisafiniteorinfinitesetofpossibleinputs; berofmajorandminor • variationsontheideaof Yisafiniteorinfinitesetofpossibleoutputs; astatemachine. We’ll • justworkwithonespe- s Sistheinitialstateofthemachine; cificoneinthissection 0 • ∈ andanotheroneinthe f : S X Sisatransitionfunction,whichtakesaninputandapreviousstateand next,butdon’tworryif • × → youseeothervariations producesanextstate; outintheworld!"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "67MIT6.036 Fall2019 68 g:S Yisanoutputfunction,whichtakesastateandproducesanoutput. • → Thebasicoperationofthestatemachineistostartwithstates 0 ,theniterativelycompute Insomecases,wewill fort>1: pickastartingstate fromasetordistribu- tion. s =f(s ,x ) t t−1 t y =g(s ) t t Thediagrambelowillustratesthisprocess. Notethatthe“feedback”connectionof s backintofhastobebufferedordelayedbyonetimestep—-otherwisewhatitis t computingwouldnotgenerallybewelldefined. x y t f g t − s t−1"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "So,givenasequenceofinputsx ,x ,... themachinegeneratesasequenceofoutputs 1 2 g(f(s ,x )),g(f(f(s ,x ),x )),... . 0 1 0 1 2 y1 y2 Wesometimessaythatthem|ach{iznetr}an|sduces{szequenc}exintosequencey. Theoutputat timetcanhavedependenceoninputsfromsteps1tot. Onecommonformisfinitestatemachines,inwhichS,X,andYareallfinitesets.Theyare oftendescribedusingstatetransitiondiagramssuchastheonebelow,inwhichnodesstand forstatesandarcsindicatetransitions. Nodesarelabeledbywhichoutputtheygenerate"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "andarcsarelabeledbywhichinputcausesthetransition. Allcomputerscanbe described,atthedigital One can verify that the state machine below reads binary strings and determines level,asfinitestatema- chines. Big,butfinite! the parity of the number of zeros in the given string. Check for yourself that all inputtedbinarystringsendinstateS ifandonlyiftheycontainanevennumberof 1 zeros. Anothercommonstructurethatissimplebutpowerfulandusedinsignalprocessing andcontrolislineartime-invariant(LTI)systems."
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Inthiscase,S = R m,X = R l andY = R n, andfandgarelinearfunctionsoftheirinputs. Indiscretetime,theycanbedefinedbya lineardifferenceequation,like y[t]=3y[t−1]+6y[t−2]+5x[t]+3x[t−2] , LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 69 (where y[t] is y at time t) and can be implemented using state to store relevant previous inputandoutputinformation. We will study recurrent neural networks which are a lot like a non-linear version of an LTIsystem,withtransitionandoutputfunctions f(s,x)=f"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "(Wsxx+Wsss+Wss) 1 0 g(s)=f (W0s+W0) 2 0 definedbyweightmatrices Wsx :m ‘ × Wss :m m × Wss :m 1 0 × W0 :n m × W0 :n 1 0 × and activation functions f and f . We will see that it’s actually possible to learn weight 1 2 valuesforarecurrentneuralnetworkusinggradientdescent. 2 Markov decision processes AMarkovdecisionprocess(MDP)isavariationonastatemachineinwhich: Thetransitionfunctionisstochastic,meaningthatitdefinesaprobabilitydistribution Recallthatstochastic •"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "overthenextstategiventhepreviousstateandinput,buteachtimeitisevaluatedit isanotherwordfor probabilistic;wedon’t drawsanewstatefromthatdistribution. say“random”because thatcanbeinterpreted Theoutputisequaltothestate(thatisgistheidentityfunction). • intwoways,bothof whichareincorrect. We Somestates(orstate-actionpairs)aremoredesirablethanothers. • don’tpickthetransi- tionfunctionitselfat AnMDPcanbeusedtomodelinteractionwithanoutside“world,”suchasasingle-player randomfromadistri- game. bution."
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Thetransition WewillfocusonthecaseinwhichSandXarefinite, andwillcalltheinputsetAfor functiondoesn’tpick itsoutputuniformlyat actions(ratherthanX). Theideaisthatanagent(arobotoragame-player)canmodelits random. environmentasanMDPandtrytochooseactionsthatwilldrivetheprocessintostatesthat havehighscores. Thereisaninterest- Formally,anMDPis S,A,T,R,γ where: ingvariationon MDPs, h i calledapartiallyobserv- T :S A S Risatransitionmodel,where able MDP,inwhichthe • × × → outputisalsodrawn"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "fromadistributionde- T(s,a,s 0)=P(S t =s 0 |S t−1 =s,A t−1 =a) , pendingonthestate. specifyingaconditionalprobabilitydistribution; Andthereisaninter- esting,directextension R:S A Risarewardfunction,whereR(s,a)specifieshowdesirableitistobein totwo-playerzero-sum • × → statesandtakeactiona;and games,suchasChess andGo. γ [0,1]isadiscountfactor,whichwe’lldiscussinsection2.2. • ∈ Thenotationhereuses Apolicyisafunctionπ:S Athatspecifieswhatactiontotakeineachstate. capitalletters,likeS,to →"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "standforrandomvari- ablesandsmallletters tostandforconcrete values. SoS hereisa t LastUpdated: 12/18/1911:56:05 randomvariablethat cantakeonelementsof Sasvalues.MIT6.036 Fall2019 70 2.1 Finite-horizonsolutions GivenanMDP,ourgoalistypicallytofindapolicythatisoptimalinthesensethatitgets as much total reward as possible, in expectation over the stochastic transitions that the domainmakes. Inthissection,wewillconsiderthecasewherethereisafinitehorizonH,"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "indicatingthetotalnumberofstepsofinteractionthattheagentwillhavewiththeMDP. 2.1.1 Evaluatingagivenpolicy Before we can talk about how to find a good policy, we have to specify a measure of the goodnessofapolicy. Wewilldosobydefiningforagiven MDP policyπandhorizonh, the“horizonhvalue”ofastate, Vh(s). Wedothisbyinductiononthehorizon, whichis π thenumberofstepslefttogo. Thebasecaseiswhentherearenostepsremaining,inwhichcase,nomatterwhatstate we’rein,thevalueis0,so V0(s)=0 . π Then,"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "thevalueofapolicyinstatesathorizonh+1isequaltotherewarditwillgetin statesplusthenextstate’sexpectedhorizonhvalue. So, startingwithhorizons1and2, andthenmovingtothegeneralcase,wehave: V1(s)=R(s,π(s))+0 π V π 2(s)=R(s,π(s))+ T(s,π(s),s 0) · R(s 0,π(s 0)) X s0 . . . V π h(s)=R(s,π(s))+ T(s,π(s),s 0) · V π h−1(s 0) X s0 Thesumovers 0 isanexpectedvalue: itconsidersallpossiblenextstatess 0 ,andcomputes anaverageoftheir(h−1)-horizonvalues,weightedbytheprobabilitythatthetransition"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "functionfromstateswiththeactionchosenbythepolicy,π(s),assignstoarrivinginstate s . 0 Study Question: What is s0 T(s,a,s 0 ) for any particular s and a? Then wecan saythat a pPolicyπ is betterthan policyπ for horizonh, i.e. π > π , 1 2 1 h 2 if and only if for all s ∈ S, V π h 1 (s) > V π h 2 (s) and there exists at least one s ∈ S such that Vh(s)>Vh(s). π1 π2 2.1.2 Findinganoptimalpolicy HowcanwegoaboutfindinganoptimalpolicyforanMDP?Wecouldimagineenumerat-"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "ingallpossiblepoliciesandcalculatingtheirvaluefunctionsasintheprevioussectionand pickingthebestone...butthat’stoomuchwork! Thefirstobservationtomakeisthat,inafinite-horizonproblem,thebestactiontotake dependsonthecurrentstate,butalsoonthehorizon: imaginethatyouareinasituation whereyoucouldreachastatewithreward5inonesteporastatewithreward10intwo steps.Ifyouhaveatleasttwostepstogo,thenyou’dmovetowardthereward10state,but ifyouonlyhavesteplefttogo,youshouldgointhedirectionthatwillallowyoutogain5!"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Onewaytofindanoptimalpolicyistocomputeanoptimalaction-valuefunction,Q. We defineQh(s,a)tobetheexpectedvalueof startinginstates, • LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 71 executingactiona,and • continuingforh−1morestepsexecutinganoptimalpolicyfortheappropriatehori- • zononeachstep. SimilartoourdefinitionofV forevaluatingapolicy,wedefinetheQfunctionrecursively accordingtothehorizon. Theonlydifferenceisthat, oneachstepwithhorizonh, rather"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "thanselectinganactionspecifiedbyagivenpolicy,weselectthevalueofathatwillmaxi- mizetheexpectedQhvalueofthenextstate. Q0(s,a)=0 Q1(s,a)=R(s,a)+0 Q2(s,a)=R(s,a)+ T(s,a,s 0)maxR(s 0,a 0) X s0 a0 . . . Qh(s,a)=R(s,a)+ T(s,a,s 0)maxQh−1(s 0,a 0) X s0 a0 We can solve for the values of Q with a simple recursive algorithm called value iteration which just computes Qh starting from horizon 0 and working backward to the desired horizonH. GivenQ,anoptimalpolicyiseasytofind: π ∗h (s)=argmaxQh(s,a) . a"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Theremaybemultiplepossibleoptimalpolicies. Dynamic programming (somewhat counter-intuitively, dynamic programming is neitherreally“dynamic”noratypeof“programming”aswetypicallyunderstand it.) is a technique for designing efficient algorithms. Most methods for solving MDPsorcomputingvaluefunctionsrelyondynamicprogrammingtobeefficient. Theprincipleofdynamicprogrammingistocomputeandstorethesolutionstosimple sub-problems that can be re-used later in the computation. It is a very important"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "toolinouralgorithmictoolbox. Let’sconsiderwhatwouldhappenifwetriedtocomputeQ4(s,a)forall(s,a)by directlyusingthedefinition: TocomputeQ4(s ,a )foranyone(s ,a ),wewouldneedtocomputeQ3(s,a) i j i j • forall(s,a)pairs. TocomputeQ3(s ,a )foranyone(s ,a ),we’dneedtocomputeQ2(s,a)for i j i j • all(s,a)pairs. TocomputeQ2(s ,a )foranyone(s ,a ),we’dneedtocomputeQ1(s,a)for i j i j • all(s,a)pairs. Luckily,thosearejustourR(s,a)values. • So,ifwehavenstatesandmactions,thisisO((mn)3)work—thatseemslikeway"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "toomuch,especiallyasthehorizonincreases! Butobservethatwereallyonlyhave mnhvaluesthatneedtobecomputed,Qh(s,a)forallh,s,a.Ifwestartwithh=1, compute and store those values, then using and reusing the Qh−1(s,a) values to computetheQh(s,a)values,wecandoallthiscomputationintimeO(mnh),which ismuchbetter! LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 72 2.2 Infinite-horizonsolutions Itisactuallymoretypicaltoworkinaregimewheretheactualfinitehorizonisnotknown. This is called the infinite horizon"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "version of the problem, when you don’t know when the gamewillbeover! However,ifwetriedtosimplytakeourdefinitionofQh aboveandset h= ,wewouldbeintrouble,becauseitcouldwellbethattheQ valuesforallactions wouldbeinfinite,andtherewouldbenowaytoselectoneovertheother. ∞ T∞herearetwostandardwaystodealwiththisproblem.Oneistotakeakindofaverage over all time steps, but this can be a little bit tricky to think about. We’ll take a different approach, which is to consider the discounted infinite horizon. We"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "select a discount fac- tor 0 < γ < 1. Instead of trying to find a policy that maximizes expected finite-horizon undiscountedvalue, h E R t |π,s 0 , \" # t=0 X wewilltrytofindonethatmaximizestheexpectedinfinitehorizondiscountedvalue,which is E γtR t |π,S 0 =E R 0 +γR 1 +γ2R 2 +...|π,s 0 . \" ∞ # t=0 X (cid:2) (cid:3) Note that the t indices here are not the number of steps to go, but actually the number ofstepsforwardfromthestartingstate(thereisnosensiblenotionof“stepstogo”inthe"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "infinitehorizoncase). Therearetwogoodintuitivemotivationsfordiscounting. Oneisrelatedtoeconomic theory and the present value of money: you’d generally rather have some money today than that same amount of money next week (because you could use it now or invest it). Theotheristothinkofthewholeprocessterminating,withprobability1−γoneachstep oftheinteraction.Thisvalueistheexpectedamountofrewardtheagentwouldgainunder thisterminatingmodel. 2.2.1 Evaluatingapolicy Wewillstart, again,"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "byevaluatingapolicy, butnowintermsoftheexpecteddiscounted infinite-horizonvaluethattheagentwillgetintheMDPifitexecutesthatpolicy.Wedefine thevalueofastatesunderpolicyπas V π (s)=E[R 0 +γR 1 +γ2R 2 + |π,S 0 =s]=E[R 0 +γ(R 1 +γ(R 2 +γ...)))|π,S 0 =s] . ··· Becausetheexpectationofalinearcombinationofrandomvariablesisthelinearcombina- tionoftheexpectations,wehave V π (s)=E[R 0 |π,S 0 =s]+γE[R 1 +γ(R 2 +γ...)))|π,S 0 =s] =R(s,π(s))+γ T(s,π(s),s 0)V π (s 0) X s0 Thisissocool! Inadis-"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Youcouldwritedownoneoftheseequationsforeachofthen=|S|states. Therearen countedmodel,ifyou findthatyousurvived unknownsV (s).Thesearelinearequations,andsoit’seasytosolvethemusingGaussian π thisroundandlanded eliminationtofindthevalueofeachstateunderthispolicy. insomestates 0,then youhavethesameex- pectedfuturelifetimeas 2.2.2 Findinganoptimalpolicy youdidbefore. Sothe valuefunctionthatis The best way of behaving in an infinite-horizon discounted MDP is not time-dependent: relevantinthatstateis"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "at every step, your expected future lifetime, given that you have survived until now, is exactlythesameoneas 1/(1−γ). instates. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 73 Study Question: Verify this fact: if, on every day you wake up, there is a probabil- ity of 1−γ that today will be your last day, then your expected lifetime is 1/(1−γ) days. AnimportanttheoremaboutMDPsis: thereexistsastationaryoptimalpolicyπ ∗ (there Stationarymeansthat maybemorethanone)suchthatforalls"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Sandallotherpoliciesπ,wehave itdoesn’tchangeover ∈ time;theoptimalpolicy V π∗ (s)>V π (s) . inafinite-horizon MDP isnon-stationary. TherearemanymethodsforfindinganoptimalpolicyforanMDP.Wewillstudyavery popularandusefulmethodcalledvalueiteration.Itisalsoimportanttous,becauseitisthe basisofmanyreinforcement-learningmethods. DefineQ (s,a)tobetheexpectedinfinite-horizondiscountedvalueofbeinginstates, ∗ executingactiona,andexecutinganoptimalpolicyπ thereafter. Usingsimilarreasoning ∗"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "totherecursivedefinitionofV ,wecanexpressthisvaluerecursivelyas π Q ∗(s,a)=R(s,a)+γ T(s,a,s 0)maxQ ∗(s 0,a 0) . X s0 a0 This is also a set of equations, one for each (s,a) pair. This time, though, they are not linear, and so they are not easy to solve. But there is a theorem that says they have a uniquesolution! Ifweknewtheoptimalaction-valuefunction,thenwecouldderiveanoptimalpolicy π as ∗ π ∗(s)=argmaxQ ∗(s,a) . a Study Question: The optimal value function is unique, but the optimal policy is"
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "not. Think of a situation in which there is more than one optimal policy. We can iteratively solve for the Q values with the value iteration algorithm, shown ∗ below: VALUE-ITERATION(S,A,T,R,γ,(cid:15)) 1 fors S,a A: ∈ ∈ 2 Q (s,a)=0 old 3 whileTrue: 4 fors S,a A: ∈ ∈ 5 Q (s,a)=R(s,a)+γ T(s,a,s )max Q (s ,a ) new s0 0 a0 old 0 0 6 ifmax |Q (s,a)−Q (s,a)|<(cid:15): s,a old new P 7 returnQ new 8 Q :=Q old new 2.2.3 Theory Therearealotofnicetheoreticalresultsaboutvalueiteration."
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "Forsomegiven(notneces- sarilyoptimal)Qfunction,defineπ (s)=argmax Q(s,a). Q a • Afterexecutingvalueiterationwithparameter(cid:15), k V πQnew −V π∗k max <(cid:15). Thisisnewnotation! Giventwofunctions • Thereisavalueof(cid:15)suchthat fandf 0,wewrite k Q old −Q new k max <(cid:15)= ⇒ π Qnew =π ∗ m k f a − x x f |f 0 ( k x m ) ax − to f 0 ( m x e )| a . n It measuresthemaximum Asthealgorithmexecutes, V −V decreasesmonotonicallyoneachitera- • k πQnew π∗k max absolutedisagreement tion."
  },
  {
    "file": "mit_ocw_ch_10.pdf",
    "chunk": "betweenthetwofunc- tionsatanyinputx. Thealgorithmcanbeexecutedasynchronously,inparallel: aslongasall(s,a)pairs • areupdatedinfinitelyofteninaninfiniterun,itstillconvergestooptimalvalue. Thisisveryimportant forreinforcementlearn- ing. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "11 CHAPTER Reinforcement learning Sofar, allthelearningproblemswehavelookedathavebeensupervised: thatis, foreach training input x(i), we are told which value y(i) should be the output. A very different problem setting is reinforcement learning, in which the learning system is not directly told whichoutputsgowithwhichinputs. Instead,thereisaninteractionoftheform: Learnerobservesinputs(i) • Learnergeneratesoutputa(i) • Learnerobservesrewardr(i) • Learnerobservesinputs(i+1) •"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "Learnergeneratesoutputa(i+1) • Learnerobservesrewardr(i+1) • ... • Thelearnerissupposedtofindapolicy,mappingstoa,thatmaximizesexpectedreward overtime. Learner state action reward Environment Thisproblemsettingisequivalenttoanonlinesupervisedlearningunderthefollowing assumptions: 1. The space of possible outputs is binary (e.g. {+1,−1}) and the space of possible re- wardsisbinary(e.g. {+1,−1}); 2. s(i)isindependentofallpreviouss(j)anda(j);and 3. r(i)dependsonlyons(i)anda(i)."
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "Inthiscase,foranyexperiencetuple(s(i),a(i),r(i)),wecangenerateasupervisedtraining example,whichisequalto(s(i),a(i))ifr(i) =+1and(s(i),−a(i))otherwise. Study Question: What supervised-learning loss function would this objective corre- spond to? 74MIT6.036 Fall2019 75 Reinforcementlearningismoreinterestingwhenthesepropertiesdonothold. When werelaxassumption1above, wehavetheclassofbanditproblems, whichwewilldiscuss in section 1. If we relax assumption 2, but assume that the environment that the"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "agent is interacting with is an MDP, so that s(i) depends only on s(i−1) and a(i−1) then we are in the classical reinforcement-learning setting, which we discuss in section 2. Weakening theassumptionsfurther,forinstance,notallowingthelearnertoobservethecurrentstate completelyandcorrectly,makestheproblemintoapartiallyobservedMDP(POMDP),which issubstantiallymoredifficult,andbeyondthescopeofthisclass. 1 Bandit problems Abasicbanditproblemisgivenby AsetofactionsA; • AsetofrewardvaluesR;and •"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "AprobabilisticrewardfunctionR : A Dist(R)whereR(a)isdrawnfromaprob- • → abilitydistributionoverpossiblerewardvaluesinRconditionedonwhichactionis selected. Eachtimetheagenttakesanaction,anewvalueisdrawnfromthisdistri- bution. The most typical bandit problem has R = {0,1} and |A| = k. This is called a k-armed banditproblem. Thereisalotofmathematicalliteratureonoptimalstrategiesfork-armed Why? BecauseinEn- bandit problems under various assumptions. The important question is usually one of"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "glishslang,“one-armed bandit”isanamefor exploration versus exploitation. Imagine that you have tried each action 10 times, and now aslotmachine(anold- youhaveanestimatepˆ fortheexpectedvalueofR(a ).Whicharmshouldyoupicknext? j j stylegamblingmachine Youcould whereyouputacoin intoaslotandthenpull exploit your knowledge, and choose the arm with the highest value of pˆ j on all future itsarmtoseeifyouget trials;or apayoff.) becauseithas onearmandtakesyour explorefurther,"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "bytryingsomeorallactionsmoretimes, hopingtogetbetterestimates money! Whatwehave ofthep values. hereisasimilarsort j ofmachine,butwithk Thetheoryultimatelytellsusthat, thelongerourhorizonH(or, similarly, closerto1our arms. discount factor), the more time we should spend exploring, so that we don’t converge prematurelyonabadchoiceofaction. Study Question: Why is it that “bad” luck during exploration is more dangerous than “good” luck? Imagine that there is an action that generates reward value 1"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "with probability 0.9, but the first three times you try it, it generates value 0. How might that cause difficulty? Why is this more dangerous than the situation when an action that generates reward value 1 with probability 0.1 actually generates reward 1 on the first three tries? Thereisasettingofsu- Notethatwhatmakesthisaverydifferentkindofproblemfromthebatchsupervised pervisedlearning,called activelearning,wherein- learningsettingisthat: steadofbeinggivena The agent gets to influence what"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "data it gets (selecting a gives it another sample trainingset,thelearner j • getstoselectvaluesof fromr ),and j xandtheenvironment givesbackalabely; The agent is penalized for mistakes it makes while it is learning (if it is trying to • theproblemofpicking maximizetheexpectedsumofr itgetswhilebehaving). t goodxvaluestoquery isinteresting,butthe Inacontextualbanditproblem,youhavemultiplepossiblestates,drawnfromsomeset problemofderivinga S,andaseparatebanditproblemassociatedwitheachone."
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "hypothesisfrom(x,y) Banditproblemswillbeanessentialsub-componentofreinforcementlearning. pairsisthesameasthe supervisedproblemwe havebeenstudying. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 76 2 Sequential problems Inthemoretypical(anddifficult!)case,wecanthinkofourlearningagentinteractingwith an MDP, where it knows S and A, but not T(s,a,s 0 ) or R(s,a). The learner can interact with the environment by selecting actions. So, this is somewhat like a contextual bandit problem,"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "butmorecomplicated, becauseselectinganactioninfluencesnotonlywhatthe immediaterewardwillbe,butalsowhatstatethesystemendsupinatthenexttimestep and,therefore,whatadditionalrewardsmightbeavailableinthefuture. A reinforcement-learning (RL) algorithm is a kind of a policy that depends on the whole historyofstates,actions,andrewardsandselectsthenextactiontotake. Thereareseveral differentwaystomeasurethequalityofanRLalgorithm,including: Ignoringther"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "valuesthatitgetswhilelearning,butconsiderhowmanyinteractions t • with the environment are required for it to learn a policy π : S A that is nearly → optimal. Maximizingtheexpecteddiscountedsumoftotalrewardswhileitislearning. • Mostofthefocusisonthefirstcriterion,becausethesecondoneisverydifficult. Thefirst criterionisreasonablewhenthelearningcantakeplacesomewheresafe(imaginearobot learning, inside the robot factory, where it can’t hurt itself too badly) or in a simulated environment. Approaches"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "to reinforcement-learning differ significantly according to what kind of hypothesisormodeltheylearn.Inthefollowingsections,wewillconsiderseveraldifferent approaches. 2.1 Model-basedRL TheconceptuallysimplestapproachtoRListoestimateRandT fromthedatawehavegot- tensofar,andthenusethoseestimates,togetherwithanalgorithmforsolvingMDPs(such asvalueiteration)tofindapolicythatisnear-optimalgiventhecurrentmodelestimates. Assumethatwehavehadsomesetofinteractionswiththeenvironment,whichcanbe"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "characterizedasasetoftuplesoftheform(s(t),a(t),r(t),s(t+1))). WecanestimateT(s,a,s )usingasimplecountingstrategy, 0 #(s,a,s )+1 Tˆ(s,a,s 0)= 0 . #(s,a)+|S| Here,#(s,a,s )representsthenumberoftimesinourdatasetwehavethesituationwhere 0 s =s,a =a,s =s and#(s,a)representsthenumberoftimesinourdatasetwehave t t t+1 0 thesituationwheres =s,a =a. t t Study Question: Prove to yourself that #(s,a)= s0 #(s,a,s 0 ). Adding1and|S|tothenumeratoranddenominaPtor,respectively,areaformofsmooth- ing called the"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "Laplace correction. It ensures that we never estimate that a probability is 0, andkeepsusfromdividingby0.Astheamountofdatawegatherincreases,theinfluence ofthiscorrectionfadesaway. WealsoestimatetherewardfunctionR(s,a): r|s,a Rˆ(s,a)= #(s,a) P where r|s,a= r(t) . X {t|st=Xs,at=a} LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 77 Thisisjusttheaverageoftheobservedrewardsforeachs,apair. Wecannowsolvethe MDP (S,A,Tˆ,Rˆ)tofindanoptimalpolicyusingvalueiteration,"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "oruseafinite-depthexpecti-maxsearchtofindanactiontotakeforaparticularstate. This technique is effective for problems with small state and action spaces, where it is not too hard to get enough experience to estimate T and R well; but it is difficult to generalizethismethodtohandlecontinuous(orverylargediscrete)statespaces,andisa topicofcurrentresearch. 2.2 Policysearch A very different strategy is to search directly for a good policy, without first (or ever!) estimating the transition and reward"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "models. The strategy here is to define a functional formf(s;θ)=aforthepolicy,whereθrepresentstheparameterswelearnfromexperience. Wechooseftobedifferentiable,andoftenletf(s;θ)=P(a),aprobabilitydistributionover ourpossibleactions. Now,wecantrainthepolicyparametersusinggradientdescent: When θ has relatively low dimension, we can compute a numeric estimate of the • gradientbyrunningthepolicymultipletimesforθ (cid:15),andcomputingtheresulting ± rewards. When θ has higher dimensions (e.g., it is a"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "complicated neural network), there are • morecleveralgorithms,e.g.,onecalledREINFORCE,buttheycanoftenbedifficultto gettoworkreliably. Policysearchisagoodchoicewhenthepolicyhasasimpleknownform,butthemodel wouldbemuchmorecomplicatedtoestimate. 2.3 Valuefunctionlearning Themostpopularclassofalgorithmslearnsneitherexplicittransitionandrewardmodels nor a direct policy, but instead concentrates on learning a value function. It is a topic of current research to describe exactly under what"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "circumstances value-function-based ap- proaches are best, and there are a growing number of methods that combine value func- tions, transitionandrewardmodelsandpoliciesintoacomplexlearningalgorithminan attempttocombinethestrengthsofeachapproach. Wewillstudytwovariationsonvalue-functionlearning,bothofwhichestimatetheQ function. 2.3.1 Q-learning Thisisthemosttypicalwayofperformingreinforcementlearning.Recallthevalue-iteration update: Thethingthatmoststu- Q(s,a)=R(s,a)+γ T(s,a,s 0)maxQ(s 0,a 0)"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "dentsseemtogetcon- X s0 a0 f d u o se v d al a u b e o i u te t r i a s ti w on he a n nd we WewilladaptthisupdatetotheRLscenario,wherewedonotknowthetransitionfunction whenwedoQlearning. T orrewardfunctionR. Valueiterationassumes youknowT andRand justneedtocomputeQ. InQlearning,wedon’t knoworevendirectly estimateT andR: we estimateQdirectlyfrom experience! LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 78 Q-LEARNING(S,A,s 0 ,γ,α) 1 fors S,a A: ∈ ∈ 2 Q[s,a]=0 3 s = s //OrdrawansrandomlyfromS 0 4"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "whileTrue: 5 a = select_action(s,Q) 6 r,s = execute(a) 0 7 Q[s,a]=(1−α)Q[s,a]+α(r+γmax Q[s ,a ]) a0 0 0 8 s = s 0 Here, α represents the “learning rate,” which needs to decay for convergence purposes, butinpracticeisoftensettoaconstant. Notethattheupdatecanberewrittenas Q[s,a] = Q[s,a]−α Q[s,a]−(r+γmaxQ[s 0,a 0]) , (cid:18) a0 (cid:19) whichlookssomethinglikeagradientupdate!Thisisoftencalledtemporaldifferencelearn- Itisactuallynotagra- ingmethod,"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "becausewemakeanupdatebasedonthedifferencebetweenthecurrentes- dientupdate,butlater, whenweconsiderfunc- timated value of taking action a in state s, which is Q[s,a], and the “one-step” sampled tionapproximation,we valueoftakingains,whichisr+γmax Q[s ,a ]. a0 0 0 willtreatitasifitwere. Youcanseethismethodasacombinationoftwodifferentiterativeprocessesthatwe havealreadyseen: thecombinationofanoldestimatewithanewsampleusingarunning"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "averagewithalearningrateα,andthedynamic-programmingupdateofaQvaluefrom valueiteration. Ouralgorithmaboveincludesaprocedurecalledselect_action,which,giventhecurrent states,hastodecidewhichactiontotake. IftheQvalueisestimatedveryaccuratelyand theagentisbehavingintheworld,thengenerallywewouldwanttochoosetheapparently optimal action argmax a A Q(s,a). But, during learning, the Q value estimates won’t be ∈ verygoodandexplorationisimportant.However,exploringcompletelyatrandomisalso"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "usuallynotthebeststrategywhilelearning,becauseitisgoodtofocusyourattentionon thepartsofthestatespacethatarelikelytobevisitedwhenexecutingagoodpolicy(nota stupidone). Atypicalaction-selectionstrategyisthe(cid:15)-greedystrategy: withprobability1−(cid:15),chooseargmax a A Q(s,a) • ∈ withprobability(cid:15),choosetheactiona Auniformlyatrandom • ∈ Q-learning has the surprising property that it is guaranteed to converge to the actual optimal Q function under fairly weak conditions! Any exploration"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "strategy is okay as long as it tries every action infinitely often on an infinite run (so that it doesn’t converge prematurelytoabadactionchoice). Q-learning can be very sample-inefficient: imagine a robot that has a choice between movingtotheleftandgettingarewardof1, thenreturningtoitsinitialstate, ormoving to the right and walking down a 10-step hallway in order to get a reward of 1000, then returningtoitsinitialstate. +1 +1000 -1 robot 1 2 3 4 5 6 7 8 9 10 LastUpdated:"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "12/18/1911:56:05MIT6.036 Fall2019 79 Thefirsttimetherobotmovestotherightandgoesdownthehallway,itwillupdatethe Qvalueforthelaststateonthehallwaytohaveahighvalue,butitwon’tyetunderstand that moving to the right was a good choice. The next time it moves down the hallway it updates the value of the state before the last one, and so on. After 10 trips down the hallway,itnowcanseethatitisbettertomovetotherightthantotheleft. More concretely, consider the vector of Q values Q(0 : 10, right),"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "representing the Q valuesformovingrightateachofthepositions0,...,9. Then,forα=1andγ=0.9, Q(i, right)=R(i, right)+0.9 maxQ(i+1,a) · a StartingwithQvaluesof0, Q(0)(0:10, right)= 0 0 0 0 0 0 0 0 0 0 0 (cid:2) (cid:3) Since the only nonzero reward from moving right is R(9, right) = 1000, after our robot Weareviolatingour makesitdownthehallwayonce,ournewQvectoris usualnotationalcon- ventionshere,andwrit- Q(1)(0:10, right)= 0 0 0 0 0 0 0 0 0 1000 0 ingQ(i) tomeanthe Qvaluefunctionthat (cid:2) (cid:3)"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "resultsaftertherobot After making its way down the hallway again, Q(8, right) = 0+0.9 Q(9, right) = 900 · runsallthewayto updates: theendofthehallway, Q(2)(0:10, right)= 0 0 0 0 0 0 0 0 900 1000 0 whenexecutingthepol- icythatalwaysmoves Similarly, (cid:2) (cid:3) totheright. Q(3)(0:10, right)= 0 0 0 0 0 0 0 810 900 1000 0 Q(4)(0:10, right)=(cid:2)0 0 0 0 0 0 729 810 900 1000 (cid:3)0 . . (cid:2) (cid:3) . Q(10)(0:10, right)= 387.4 420.5 478.3 531.4 590.5 656.1 729 810 900 1000 0 , (cid:2)"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "(cid:3) andtherobotfinallyseesthevalueofmovingrightfromposition0. Wecanseehowthis interactswiththeex- Study Question: Determine the Q value functions that will result from updates due ploration/exploitation to the robot always executing the “move left” policy. dilemma: fromtheper- spectiveofs ,itwill 0 seem,foralongtime, thatgettingtheimmedi- 2.3.2 Functionapproximation aterewardof1isabet- teridea,anditwould In our Q-learning algorithm above, we essentially keep track of each Q value in a"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "table, beeasytoconvergeon indexedbysanda. WhatdowedoifSand/orAarelarge(orcontinuous)? thatasastrategywith- WecanuseafunctionapproximatorlikeaneuralnetworktostoreQvalues.Forexam- outexploringthelong ple, we could design a neural network that takes in inputs s and a, and outputs Q(s,a). hallwaysufficiently. Wecantreatthisasaregressionproblem,optimizingthesquaredBellmanerror,withloss: 2 Q(s,a)−(r+γmaxQ(s 0,a 0)) , (cid:18) a0 (cid:19) whereQ(s,a)isnowtheoutputoftheneuralnetwork."
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "Thereareactuallyseveraldifferentarchitecturalchoicesforusinganeuralnetworkto approximateQvalues: Onenetworkforeachactiona ,thattakessasinputandproducesQ(s,a )asoutput; j j • OnesinglenetworkthattakessasinputandproducesavectorQ(s, ),consistingof • · theQvaluesforeachaction;or LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 80 One single network that takes s,a concatenated into a vector (if a is discrete, we • wouldprobablyuseaone-hotencoding,unlessithadsomeusefulinternalstructure)"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "andproducesQ(s,a)asoutput. Forcontinuousaction Thefirsttwochoicesareonlysuitablefordiscrete(andnottoobig)actionsets.Thelast spaces,itisincreasingly populartouseaclass choicecanbeappliedforcontinuousactions,butthenitisdifficulttofindargmaxA Q(s,a). ofmethodscalledactor- TherearenotmanytheoreticalguaranteesaboutQ-learningwithfunctionapproxima- criticmethods,which tionand,indeed,itcansometimesbefairlyunstable(learningtoperformwellforawhile, combinepolicyand andthengettingsuddenlyworse,"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "forexample). Butithasalsohadsomesignificantsuc- value-functionlearning. Wewon’tgetintothem cesses. indetailhere,though. Oneformofinstabilitythatwedoknowhowtoguardagainstiscatastrophicforgetting. In standard supervised learning, we expect that the training x values were drawn inde- pendentlyfromsomedistribution. Butwhenalearningagent,suchasarobot,ismoving And,infact,werou- throughanenvironment,thesequenceofstatesitencounterswillbetemporallycorrelated. tinelyshuffletheirorder"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "inthedatafile,anyway. This can mean that while it is in the dark, the neural-network weight-updates will make theQfunction“forget”thevaluefunctionforwhenit’slight. Forexample,itmight One way to handle this is to use experiencereplay, where we save our (s,a,r,s 0 ) expe- spend12hoursina riences in a replay buffer. Whenever we take a step in the world, we add the (s,a,r,s 0 ) darkenvironmentand to the replay buffer and use it to do a Q-learning update. Then we also randomly select"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "then12inalightone. somenumberoftuplesfromthereplaybuffer,anddoQ-learningupdatesbasedonthem, as well. In general it may help to keep a sliding window of just the 1000 most recent ex- periences in the replay buffer. (A larger buffer will be necessary for situations when the optimalpolicymightvisitalargepartofthestatespace,butweliketokeepthebuffersize smallformemoryreasonsandalsosothatwedon’tfocusonpartsofthestatespacethat are irrelevant for the optimal policy.) The idea is that it will help you"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "propagate reward valuesthroughyourstatespacemoreefficientlyifyoudotheseupdates. Youcanseeitas doingsomethinglikevalueiteration,butusingsamplesofexperienceratherthanaknown model. 2.3.3 FittedQ-learning AnalternativestrategyforlearningtheQfunctionthatissomewhatmorerobustthanthe standardQ-learningalgorithmisamethodcalledfittedQ. FITTED-Q-LEARNING(A,s 0 ,γ,α,(cid:15),m) 1 s = s //OrdrawansrandomlyfromS 0 2 D={} 3 initializeneural-networkrepresentationofQ 4 whileTrue: 5 D"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "=experiencefromexecuting(cid:15)-greedypolicybasedonQformsteps new 6 D=D D representedas(s,a,r,s )tuples new 0 ∪ 8 7 D s f u o p r = ea { c ( h x( t i u ) p ,y le (i ( ) s ) , } a w ,r h , e s re )( x i ( ) i) = D (s,a)andy(i) =r+γmax a0∈ A Q(s 0 ,a 0 ) 0 ∈ 9 re-initializeneural-networkrepresentationofQ 10 Q=supervised_NN_regression(D ) sup Here,wealternatebetweenusingthepolicyinducedbythecurrentQfunctiontogather abatchofdataD ,addingittoouroveralldatasetD,andthenusingsupervisedneural- new"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "networktrainingtolearnarepresentationoftheQvaluefunctiononthewholedataset. This method does not mix the dynamic-programming phase (computing new Q values based on old ones) with the function approximation phase (training the neural network) andavoidscatastrophicforgetting.Theregressiontraininginline9typicallyusessquared LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 81 error as a loss function and would be trained until the fit is good (possibly measured on held-outdata). LastUpdated:"
  },
  {
    "file": "mit_ocw_ch_11.pdf",
    "chunk": "12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "12 CHAPTER Recurrent Neural Networks Inchapter8westudiedneuralnetworksandhowwecantraintheweightsofanetwork, basedondata, sothatitwilladaptintoafunctionthatapproximatestherelationshipbe- tweenthe(x,y)pairsinasupervised-learningtrainingset. Insection1ofchapter10,we studiedstate-machinemodelsanddefinedrecurrentneuralnetworks(RNNs)asaparticular type of state machine, with a multidimensional vector of real values as the state. In this"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "chapter,we’llseehowtousegradient-descentmethodstotraintheweightsofan RNN so that it performs a transduction that matches as closely as possible a training set of input- outputsequences. 1 RNN model Recall that the basic operation of the state machine is to start with some state s , then 0 iterativelycomputefort>1:: s =f(s ,x ) t t−1 t y =g(s ) t t as illustrated in the diagram below (remembering that there needs to be a delay on the feedbackloop): x y t f g t − s t−1 So,givenasequenceofinputsx"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": ",x ,... themachinegeneratesasequenceofoutputs 1 2 g(f(s ,x )),g(f(f(s ,x ),x ,)),... . 0 1 0 1 2 y1 y2 | {z } | {z } 82MIT6.036 Fall2019 83 Arecurrentneuralnetworkisastatemachinewithneuralnetworksconstitutingfunctions fandg: f(s,x)=f (Wsxx+Wsss+Wss) 1 0 g(s)=f (WOs+WO) . 2 0 Theinputs,outputs,andstatesareallvector-valued: Weareverysorry! This coursematerialhas x :‘ 1 evolvedfromdifferent t × sources,whichused s t :m × 1 WTxintheforward y :v 1 . passforregularfeed- t × forwardNNsandWx"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "fortheforwardpassin RNNs. Thisinconsis- Theweightsinthenetwork,then,are tencydoesn’tmakeany technicaldifference,but Wsx :m ‘ isapotentialsourceof × confusion. Wss :m m × Wss :m 1 0 × WO :v m × WO :v 1 0 × withactivationfunctionsf 1 andf 2 . Finally,theoperationoftheRNNisdescribedby s =f (Wsxx +Wsss +Wss) t 1 t t−1 0 y =f WOs +WO . t 2 t 0 (cid:0) (cid:1) Study Question: Check dimensions here to be sure it all works out. Remember that we apply f and f elementwise. 1 2 2 Sequence-to-sequence RNN"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "Now, how can we train an RNN to model a transduction on sequences? This problem is sometimescalledsequence-to-sequencemapping. Youcanthinkofitasakindofregression problem: givenaninputsequence,learntogeneratethecorrespondingoutputsequence. Onewaytothinkof Atrainingsethastheform x(1),y(1) ,..., x(q),y(q) ,where trainingasequence classifieristoreduceit x(i)andy(i)arelengthn((cid:2)i(cid:0))sequenc(cid:1)es; (cid:0) (cid:1)(cid:3) toatransductionprob- • lem,wherey =1ifthe t"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "sequencesinthesamepairarethesamelength;andsequencesindifferentpairsmay sequencex 1 ,...,x t isa • positiveexampleofthe havedifferentlengths. classofsequencesand −1otherwise. Next,weneedalossfunction.Westartbydefiningalossfunctiononsequences.There are many possible choices, but usually it makes sense just to sum up a per-element loss functiononeachoftheoutputvalues,wherepisthepredictedsequenceandyistheactual one: n(i) Loss p(i),y(i) = Loss p(i) ,y(i) . seq elt t t (cid:16) (cid:17) X t=1"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "(cid:16) (cid:17) The per-element loss function Loss will depend on the type of y and what informa- elt t tion it is encoding, in the same way as for a supervised network.. Then, letting θ = Soitcouldbe NLL, squaredloss,etc. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 84 Wsx,Wss,WO,Wss,WO ,ouroverallobjectiveistominimize 0 0 (cid:0) (cid:1) q J(θ)= Loss RNN(x(i);θ),y(i) , seq X i=1 (cid:16) (cid:17) whereRNN(x;θ)istheoutputsequencegenerated,giveninputsequencex. Itistypicaltochoosef 1"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "tobetanhbutanynon-linearactivationfunctionisusable. We Rememberthatitlooks choosef toalignwiththetypesofouroutputsandthelossfunction,justaswewoulddo likeasigmoidbut 2 rangesfrom-1to+1. inregularsupervisedlearning. 3 Back-propagation through time Now the fun begins! We can find θ to minimize J using gradient descent. We will work throughthesimplestmethod,back-propagationthroughtime(BPTT),indetail. Thisisgener- allynotthebestmethodtouse,butit’srelativelyeasytounderstand. Insection5wewill"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "sketchalternativemethodsthatareinmuchmorecommonuse. Calculusreminder:totalderivativeMostofusarenotverycarefulaboutthediffer- encebetweenthepartialderivativeandthetotalderivative.Wearegoingtouseanice examplefromtheWikipediaarticleonpartialderivativestoillustratethedifference. Thevolumeofacircularconedependsonitsheightandradius: πr2h V(r,h)= . 3 Thepartialderivativesofvolumewithrespecttoheightandradiusare ∂V 2πrh ∂V πr2 = and = . ∂r 3 ∂h 3 They measure the change in V assuming everything is held"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "constant except the singlevariablewearechanging. Nowassumethatwewanttopreservethecone’s proportionsinthesensethattheratioofradiustoheightstayconstant,thenwecan’t reallychangeonewithoutchangingtheother. Inthiscase,wereallyhavetothink aboutthetotalderivative,whichsumsthe“paths”alongwhichrmightinfluenceV: dV ∂V ∂V dh = + dr ∂r ∂h dr 2πrh πr2dh = + 3 3 dr dV ∂V ∂V dr = + dh ∂h ∂r dh πr2 2πrh dr = + 3 3 dh Justtobecompletelyconcrete,let’sthinkofarightcircularconewithafixedangle α = tanr/h, so that"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "if we change r or h then α remains constant. So we have r=htan−1α;letconstantc=tan−1α,sonowr=ch. Now,weknowthat dV 2πrh πr21 = + dr 3 3 c dV πr2 2πrh = + c dh 3 3 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 85 TheBPTTprocessgoeslikethis: (1) Sampleatrainingpairofsequences(x,y);lettheirlengthben. (2) “Unroll\"theRNNtobelengthn(pictureforn=3below),andinitializes : 0 Now,wecanseeourproblemasoneofperformingwhatisalmostanordinaryback- propagationtrainingprocedureinafeed-forwardneuralnetwork,"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "butwiththedif- ferencethattheweightmatricesaresharedamongthelayers. Inmanyways,thisis similartowhatendsuphappeninginaconvolutionalnetwork, exceptintheconv- net,theweightsarere-usedspatially,andhere,theyarere-usedtemporally. (3) Dotheforwardpass,tocomputethepredictedoutputsequencep: z1 =Wsxx +Wsss +Wss t t t−1 0 s =f (z1) t 1 t z2 =WOs +WO t t 0 p =f (z2) t 2 t (4) Dobackwardpasstocomputethegradients. ForbothWssandWsxweneedtofind dL n dL seq u = dW dW u X =1 (12.1) Letting L = L (p ,y ) and"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "using the total derivative, which is a sum over all the u elt u u waysinwhichWaffectsL ,wehave u n n ∂L ∂s u t = ∂s · ∂W t u X =1X t=1 (12.2) Re-organizing,wehave n n ∂s ∂L t u = ∂W · ∂s t X t=1 u X =1 (12.3) LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 86 Becauses onlyaffectsL ,L ,...,L , t t t+1 n n n ∂s ∂L t u = ∂W · ∂s t X t=1 u X =t n n ∂s t ∂L t ∂L u = + (12.4) ∂W · ∂s ∂s X t=1   t u X =t+1 t      δst    δst isthedependenceofthelossonstepsaftert|ont{hzesta}teattimet."
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "Thatis,δst ishow muchwecanblame Wecancomputethisbackwards,withtgoingfromndownto1. Thetrickiestpartis states forallthefuture t figuringouthowearlystatescontributetolaterlosses. Wedefinefutureloss elementlosses. n F = Loss (p ,y ) , t elt u u u X =t+1 so ∂F δst = t . ∂s t Atthelaststage,F n =0soδsn =0. Now,workingbackwards, n ∂ δst−1 = ∂s Loss elt (p u ,y u ) t−1 u=t X n ∂s ∂ = t Loss (p ,y ) ∂s · ∂s elt u u t−1 t u=t X n ∂s ∂ = t Loss (p ,y )+ Loss (p ,y ) ∂s t−1 · ∂s t \" elt t t elt u u # u X"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "=t+1 ∂s ∂Loss (p ,y ) = t elt t t +δst ∂s · ∂s t−1 (cid:20) t (cid:21) Now, wecanusethechainruleagaintofindthedependenceoftheelementlossat timetonthestateatthatsametime, ∂Loss (p ,y ) ∂z2 ∂Loss (p ,y ) elt t t = t elt t t , ∂s ∂s · ∂z2 t t t (m 1) (m v) (v 1) × × × andthedependenceo | fthes { ta z teatti } met | o { n z} the|statea{tzthepre}vioustime,notingthat we are performing an elementwise multiplication between WT and the vector of f10 ss values,∂s t /∂z1 t : Therearetwowaysto ∂ ∂ s s t ="
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "∂ ∂ s z1 t · ∂ ∂ z s t 1 =WssT ∗ f10(z1 t ) . t h h e i r n e k ,w ab e o t u a t ke ∂s t t h / e ∂z v t i : ew t−1 t−1 t thatitisanm 1vector notdot! × (m m) (m m) (m 1) andwemultiplyeach × × × | {z } columnofWT byit. Puttingthisalltogeth|er{,zw}eend|u{zp}with|{z} Another,equallygood, view,isthatitisanm δst−1 =WssT f10(z1) WOT∂L t +δst mdiagonalmatrix,wit × h ∗ t · ∂z2 thevaluesalongthe (cid:18) t (cid:19) ∂st diagonal,andthenthis ∂st−1 ∂Ft−1 operationisamatrix | {z } ∂st multiply. Oursoftware"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "| {z } implementationwill LastUpdated: 12/18/1911:56:05 takethefirstview.MIT6.036 Fall2019 87 We’re almost there! Now, we can describe the actual weight updates. Using equa- tion12.4andrecallingthedefinitionofδst = ∂F t /∂s t , asweiteratebackwards, we canaccumulatethetermsinequation12.4togetthegradientforthewholeloss: dL ∂F ∂z1 ∂s ∂F seq += t−1 = t t t−1 dWss ∂Wss ∂Wss∂z1 ∂s t t dL ∂F ∂z1 ∂s ∂F seq += t−1 = t t t−1 dWsx ∂Wsx ∂Wsx∂z1 ∂s t t We can handle WO separately; it’s easier because it"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "does not effect future losses in thewaythattheotherweightmatricesdo: ∂L n ∂L n ∂L ∂z2 seq = t = t t ∂WO ∂WO ∂z2 · ∂WO X t=1 X t=1 t Assuming we have ∂Lt = (p − y ), (which ends up being true for squared loss, ∂z2 t t t softmax-NLL,etc.),thenoneachiteration ∂L seq +=(p −y ) sT ∂WO t t · t v 1 1 m v m × × × | {z } |{z} | {z } Whew! Study Question: Derive the updates for the offsets Wss and WO. 0 0 4 Training a language model Alanguagemodelisjusttrainedonasetofinputsequences,(c(i) ,c(i)"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": ",...,c(i)),andisused 1 2 ni topredictthenextcharacter,givenasequenceofprevioustokens: A“token”isgenerallya characteroraword. c =RNN(c ,c ,...,c ) t 1 2 t−1 Wecanconvertthistoasequence-to-sequencetrainingproblembyconstructingadata setof(x,y)sequencepairs,wherewemakeupnewspecialtokens,startandend,tosignal thebeginningandendofthesequence: x=( start ,c ,c ,˙,c ) 1 2 n h i y=(c ,c ,..., end ) 1 2 h i 5 Vanishing gradients and gating mechanisms"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "Let’stakeacarefullookatthebackwardpropagationofthegradientalongthesequence: ∂s ∂Loss (p ,y ) δst−1 = t elt t t +δst . ∂s · ∂s t−1 (cid:20) t (cid:21) Consideracasewhereonlytheoutputattheendofthesequenceisincorrect,butitdepends critically,viatheweights,ontheinputattime1. Inthiscase,wewillmultiplythelossat stepnby ∂s ∂s ∂s 2 3 n . ∂s · ∂s ···∂s 1 2 n−1 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 88 In general, this quantity will either grow or shrink exponentially with the length of the"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "sequence,andmakeitverydifficulttotrain. Study Question: The last time we talked about exploding and vanishing gradients, it was to justify per-weight adaptive step sizes. Why is that not a solution to the prob- lem this time? Animportantinsightthatreallymaderecurrentnetworksworkwellonlongsequences istheideaofgating. 5.1 Simplegatedrecurrentnetworks Acomputeronlyeverupdatessomepartsofitsmemoryoneachcomputationcycle. We cantakethisideaanduseittomakeournetworksmoreabletoretainstatevaluesovertime"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "andtomakethegradientsbetter-behaved. Wewilladdanewcomponenttoournetwork, calledagatingnetwork. Letg beam 1vectorofvaluesandletWgx andWgs bem l t × × andm mweightmatrices,respectively. Wewillcomputeg t as Itcanhaveanoffset, × too,butweareomitting g =sigmoid(Wgxx +Wgss ) itforsimplicity. t t t−1 andthenchangethecomputationofs tobe t s =(1−g ) s +g f (Wsxx +Wsss +Wss) , t t ∗ t−1 t ∗ 1 t t−1 0 where iscomponent-wisemultiplication. Wecansee,here,thattheoutputofthegating ∗"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "networkisdeciding,foreachdimensionofthestate,howmuchitshouldbeupdatednow. This mechanism makes it much easier for the network to learn to, for example, “store” some information in some dimension of the state, and then not change it during future stateupdates, orchangeitonlyundercertainconditionsontheinputorotheraspectsof thestate. Study Question: Why is it important that the activation function for g be a sigmoid? 5.2 Longshort-termmemory"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "Theideaofgatingnetworkscanbeappliedtomakeastate-machinethatisevenmorelike a computer memory, resulting in a type of network called an LSTM for “long short-term memory.” We won’t go into the details here, but the basic idea is that there is a memory Yetanotherawesome cell (really, our state vector) and three (!) gating networks. The input gate selects (using nameforaneuralnet- work! a “soft” selection as in the gated network above) which dimensions of the state will be updatedwithnewvalues;"
  },
  {
    "file": "mit_ocw_ch_12.pdf",
    "chunk": "theforgetgatedecideswhichdimensionsofthestatewillhave itsoldvaluesmovedtoward0,andtheoutputgatedecideswhichdimensionsofthestate willbeusedtocomputetheoutputvalue. Thesenetworkshavebeenusedinapplications like language translation with really amazing results. A diagram of the architecture is shownbelow: LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 89 LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "13 CHAPTER Recommender systems Theproblemofchoosingitemsfromalargesettorecommendtoausercomesupinmany contexts, including music services, shopping, and online advertisements. As well as be- inganimportantapplication,itisinterestingbecauseithasseveralformulations,someof whichtakeadvantageofaparticularinterestingstructureintheproblem. Concretely,wecanthinkaboutacompanylikeNetflix,whichrecommendsmoviestoits users.Netflixknowstheratingsgivenbymanydifferentpeopletomanydifferentmovies,"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "andknowsyourratingsonasmallsubsetofallpossiblemovies. Howshoulditusethis datatorecommendamovieforyoutowatchtonight? Therearetwoprevailingapproachestothisproblem. Thefirst,content-basedrecommen- dation, is formulated as a supervised learning problem. The second, collaborative filtering, introducesanewlearningproblemformulation. 1 Content-based recommendations Incontent-basedrecommendation,wetrytolearnapredictor,f,thatusesthemoviesthat"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "youhaveratedsofarastrainingdata,findahypothesisthatmapsamovieintoaprediction ofwhatratingyouwouldgiveit,andthenreturnsomemovieswithhighpredictedratings. Thefirststepisdesigningrepresentationsfortheinputandoutput. It’sactuallyprettydifficulttodesignagoodfeaturerepresentationformovies.Reason- ableapproachesmightconstructfeaturesbasedonthemovie’sgenre,length,mainactors, director, location, or even ratings given by some standard critics or aggregation sources. Thisdesignprocesswouldyield φ:movie"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "vector . → Movieratingsaregenerallygivenintermsofsomenumberofstars,sotheoutputdo- mainmightbe{1,2,3,4,5}. It’snotappropriateforone-hotencodingontheoutput,and pretending that these are real values is also not entirely sensible. Nevertheless, we will treattheoutputasifit’sinR. Thermometercoding mightbereasonable, Study Question: What is the disadvantage of using one-hot? What is the disadvan- butit’shardtosay tage of using R? withouttryingit. Some moreadvancedtech- niquestrytopredict"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "rankings(wouldIpre- fermovieAovermovie 90 B)ratherthanrawrat- ings.MIT6.036 Fall2019 91 Nowthatwehaveanencoding,wecanmakeatrainingsetbasedonyourpreviousrat- ingsofmovies.Here,x(i)representstheithmovie,φ(x(i))givesourfeaturerepresentation of the ith movie, and y(i) = rating(x(i)) is your rating for the ith movie. If you rated j moviessofar,ourresultingtrainingsetlookslike D = φ(x(1)),rating(x(1)) , φ(x(2)),rating(x(2)) ,..., φ(x(j)),rating(x(j)) a (cid:10)(cid:16) (cid:17) (cid:16) (cid:17)"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "(cid:16) (cid:17)(cid:11) The next step is to pick a loss function. This is closely related to the choice of output encoding. Sincewedecidedtotreattheoutputasareal,wecanformulatetheproblemas aregressionfromφ R,withLoss(p,y)= 1(y−p)2Wewillgenerallyneedtoregularize → 2 because we typically have a very small amount of data (unless you really watch a lot of movies!). Finally, we need to pick a hypothesis space. The simplest thing would be to make it"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "linear,butyoucoulddefinitelyusesomethingfancier,likeaneuralnetwork. Ifweputallthistogether,withalinearhypothesisspace,weendupwiththeobjective 1 λ J(θ)= (y(i)−θTφ(x(i))−θ )2+ θ 2 . 2 0 2k k i X∈ Da This is our old friend, ridge regression, and can be solved analytically or with gradient descent. 2 Collaborative filtering Therearetwodifficultieswithcontent-basedrecommendationsystems: It’shardtodesignagoodfeaturesettorepresentmovies. • They only use your previous movie ratings, but don’t have a"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "way to use the vast • majorityoftheirdata,whichisratingsfromotherpeople. In collaborative filtering, we’ll try to use all the ratings that other people have made of moviestohelpmakebetterpredictionsforyou. Intuitively,wecanseethisprocessasfindingthekindsofpeoplewholikethekindsof moviesIlike,andthenpredictingthatIwilllikeothermoviesthattheylike. Infact,there’sathird Formally, wewillstartbyconstructingadatamatrixY, whereY representsthescore strategythatisre- ai allydirectlybasedon"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "givenbyuseratomoviei. So,ifwehavenusersandmmovies,Yhasshapen m. × thisidea,inwhichwe concretelytrytofind otheruserswhoare our“nearestneighbors” inmoviepreferences, andthenpredictmovies theylike. Theapproach wediscussherehas similarmotivationsbut ismorerobust. Wewillinfactnotactu- allyrepresentthewhole datamatrixexplicitly— itwouldbetoobig. Butit’susefultothink about. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 92 m movies 5 3 1 2 · · · 4 . . . sresu n Y is very sparse (most entries are"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "empty). So, we will think of our training data-set IntheNetflixchal- as a set of tuples {(a,i,r)}, where a is the index assigned to a particular user, i is the lengedataset,thereare 400,000usersand17,000 index assigned to a particular movie, and r is user a’s rating of movie i. We will use movies. Only1%ofthe D={(a,i):Y isnon-empty}asthesetofindicesforwhichwehavearating. ai datamatrixisfilled. WearegoingtotrytofindawaytouseDtopredictvaluesformissingentries. LetX beourpredictedmatrixofratings."
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Now,weneedtofindalossfunctionthatrelatesXand Y,sothatwecantrytooptimizeittofindagoodpredictivemodel. Idea #1 Following along with our previous approaches to designing loss functions, we mightwanttosaythatourpredictionsX shouldagreewithourdataY ,andthenadd ai ai someregularization,yieldinglossfunction 1 Loss(X,Y)= (X −Y )2+ X2 . 2 ai ai ai (aX,i) ∈ D allX(a,i) Thisisabadidea! ItwillsetX =0forall(a,i) D. ai 6∈ Study Question: Convince yourself of that!"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Weneedtofindadifferentkindofregularizationthatwillforcesomegeneralizationto unseenentries. Linearalgebraidea: Therankofamatrixisthemaximumnumberoflinearlyin- dependentrowsinthematrix(whichisequaltothemaximumnumberoflinearly independentcolumnsinthematrix). Ifann mmatrixXisrank1,thenthereexistUandV ofshapesn 1andm 1, × × × respectively,suchthat X=UVT . IfXisrankk,thenthereexistUandV ofshapen kandm k,respectively,such × × that X=UVT . LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 93 Idea#2"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Findtherank1matrixXthatfitstheentriesinYaswellaspossible.Thisisamuch lower-dimensionalrepresentation(ithasm+nparametersratherthanm nparameters) · andthesameparameterissharedamongmanypredictions,soitseemslikeitmighthave bettergeneralizationpropertiesthanourpreviousidea. So,wewouldneedtofindvectorsUandV suchthat U(1) U(1)V(1) U(1)V(m) ··· UVT =  . . .  V(1) V(m) =  . . . ... . . .  =X . ··· U(n) U(n)V(1) U(n)V(m)  (cid:2) (cid:3)  ···     "
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "And,sincewe’reusingsquaredloss,ourobjectivefunctionwouldbe 1 J(U,V)= (U(a)V(i)−Y )2 . ai 2 (aX,i) ∈ D Now,howcanwefindtheoptimalvaluesofUandV? Wecouldtakeinspirationfrom our work on linear regression and see what the gradients of J are with respect to the pa- rametersinUandV. Forexample, ∂J = (U(a)V(i)−Y )V(i) . ∂U(a) ai {i|(aX,i) ∈ D} WecouldgetanequationlikethisforeachparameterU(a)orV(i). Wedon’tknowhowto getanimmediateanalyticsolutiontothissetofequationsbecausetheparametersUand V are"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "multiplied by one another in the predictions, so the model does not have a linear dependenceontheparameters. Wecouldapproachthisproblemusinggradientdescent, though,andwe’lldothatwitharelatedmodelinthenextsection. But, before we talk about optimization, let’s think about the expressiveness of this model. It has one parameter per user (the elements of U) and one parameter per movie (theelementsofV),andthepredictedratingistheproductofthesetwo.Itcanreallyrepre-"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "sentonlyeachuser’sgeneralenthusiasmandeachmovie’sgeneralpopularity,andpredict theuser’sratingofthemovietobetheproductofthesevalues. Study Question: What if we had two users, 1 and 2, and two movies, A and B. Can you find U,V that represents the data set (1,A,1),(1,B,5),(2,A,5),(2,B,1) well? Idea #3 If using a rank 1 decomposition of the matrix is not expressive enough, maybe wecantryarankkdecomposition!Inthiscase,wewouldtrytofindann kmatrixUand × anm kmatrixV thatminimize × 1 J(U,V)= (U(a)"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "V(i)−Y )2 . ai 2 · (aX,i) ∈ D LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 94 V(i) VT U(a) X = U(a) V(i) ai · U Here, the length k vector U(a) is the ath row of U, and represents the k “features” of person a. Likewise, the length k vector V(i) is the ith row of V, and represents the k “features” of movie i. Performing the matrix multiplication X = UVT, we see what the predictionforpersonaandmovieiisX =U(a) V(i). ai · The total number of parameters that we have is nk+mk. But, it is a redundant"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "rep- resentation. We have 1 extra scaling parameter when k = 1, and k2 extra parameters in general. So,wereallyeffectivelyhavenk+mk−k2“degreesoffreedom.” Study Question: Imagine k = 3. If we were to take the matrix U and multiply the first column by 2, the second column by 3 and the third column by 4, to make a new matrix U , what would we have to do to V to get a V so that U V T = UVT? How 0 0 0 0 does this question relate to the comments above about redundancy?"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Itisstillusefultoaddoffsetstoourpredictions,sowewillincludeann 1vectorb U × andanm 1vectorb ofoffsetparameters,andperformregularizationontheparameters V × inUandV. Soourfinalobjectivebecomes n m 1 λ 2 λ 2 J(U,V)= (U(a) V(i)+b(a)+b(i)−Y )2+ U(a) + V(i) . 2 · U V ai 2 2 (aX,i) ∈ D a X =1(cid:13) (cid:13) (cid:13) (cid:13) X i=1(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Study Question: What would be an informal interpretation of b(a) ? Of b(i) ? U V 2.1 Optimization"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Nowthatwehaveanobjective,it’stimetooptimize!Therearetworeasonableapproaches to finding U, V, b , and b that optimize this objective: alternating least squares (ALS), U V whichbuildsonouranalyticalsolutionapproachforlinearregression,andstochasticgra- dient descent (SGD), which we have used in the context of neural networks and other models. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 95 2.1.1 Alternatingleastsquares Oneinterestingthingtonoticeisthat, ifweweretofixUandb , thenfindingthemini- U"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "mizingV andb isalinearregressionproblemthatwealreadyknowhowtosolve. The V same is true if we were to fix V and b , and seek U and b . So, we will consider an al- V U gorithmthattakesalternatingstepsofthisform: wefixU,b ,initiallyrandomly,findthe U bestV,b ;thenfixthoseandfindthebestU,b ,etc. V U Thisisakindofoptimizationsometimescalled“coordinatedescent,”becauseweonly improvethemodelinone(or,inthiscase,asetof)coordinatesoftheparameterspaceata"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "time.Generally,coordinatedescenthassimilarkindsofconvergencepropertiesasgradient descent,anditcannotguaranteethatwefindaglobaloptimum. Itisanappealingchoice inthisproblembecauseweknowhowtodirectlymovetotheoptimalvaluesofonesetof coordinatesgiventhattheotherisfixed. Moreconcretely,we: 1. InitializeV andb atrandom V 2. Foreachain1,2,...,n: ConstructalinearregressionproblemtofindU(a)tominimize • 1 2 λ 2 U(a) V(i)+b(a)+b(i)−Y + U(a) . 2 · U V ai 2 {i|(aX,i) ∈ D}(cid:16) (cid:17) (cid:13) (cid:13)"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "(cid:13) (cid:13) (cid:13) (cid:13) Recall minimizing the least squares objective (we are ignoring the offset and • regularizerinthefollowingsoyoucanseethebasicidea): (Wθ−T)T(Wθ−T) . Inthisscenario, – θ=U(a)isthek 1parametervectorthatwearetryingtofind, × – T isam 1vectoroftargetvalues(forthem moviesahasrated),and a a × – Wisthem kmatrixwhoserowsaretheV(i)whereahasratedmoviei. a × ThesolutiontotheleastsquaresproblemusingridgeregressionisournewU(a) andb(a) . U 3. Foreachiin1,2,...,m"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "ConstructalinearregressionproblemtofindV(i)andb(i) tominimize • V 1 2 λ 2 U(a) V(i)+b(a)+b(i)−Y + V(i) 2 · U V ai 2 {i|(aX,i) ∈ D}(cid:16) (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Now,θ=V(i) isak 1parametervector,T isan 1targetvector(forthen i i • × × usersthathaveratedmoviei),andW isthen kmatrixwhoserowsarethe i × U(a)whereihasbeenratedbyusera. Again,wesolveusingridgeregressionforanewvalueofV(i)andb(i) . V 4."
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "Alternatebetweensteps2and3,optimizingUandV,andstopafterafixednumber ofiterationsorwhenthedifferencebetweensuccessiveparameterestimatesissmall. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 96 2.1.2 Stochasticgradientdescent Finally,wecanapproachthisproblemusingstochasticgradientdescent.It’seasiertothink aboutifwereorganizetheobjectivefunctiontobe 1 2 2 2 J(U,V)= U(a) V(i)+b(a)+b(i)−Y +λ(a) U(a) +λ(i) V(i) 2 · U V ai U V (aX,i) ∈ D(cid:18)(cid:16) (cid:17) (cid:13) (cid:13) (cid:13) (cid:13)"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "(cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:13) (cid:13) (cid:13) (cid:13) where λ λ λ(a) = = U #times(a,_) D 1 ∈ {i|(a,i) ∈ D} λ λ λ(i) = = P V #times(_,i) D 1 ∈ {a|(a,i) ∈ D} P Then, ∂J(U,V) = U(a) V(i)+b(a)+b(i)−Y V(i)+λ(a)U(a) ∂U(a) · U V ai U {i|(aX,i) ∈ D}h(cid:16) (cid:17) i ∂J(U,V) = U(a) V(i)+b(a)+b(i)−Y ∂b(a) · U V ai U {i|(aX,i) ∈ D}(cid:16) (cid:17) WecansimilarlyobtaingradientswithrespecttoV(i)andb(i) . V Then,todogradientdescent,wedrawanexample(a,i,Y )fromDatrandom,anddo ai"
  },
  {
    "file": "mit_ocw_ch_13.pdf",
    "chunk": "gradientupdatesonU(a),b(a) ,V(i),andb(i) . U V Study Question: Why don’t we update the other parameters, such as U(a0) for some other user a 0 or V(i0) for some other movie i 0 ? LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "14 CHAPTER Non-parametric methods 1 Intro Wewillcontinuetobroadentheclassofmodelsthatwecanfittoourdata.Neuralnetworks haveadaptablecomplexity,inthesensethatwecantrydifferentstructuralmodelsanduse crossvalidationtofindonethatworkswellonourdata. Wenowturntomodelsthatautomaticallyadapttheircomplexitytothetrainingdata. Thenamenon-parametricmethodsismisleading: itisreallyaclassofmethodsthatdoesnot haveafixedparameterizationinadvance. Somenon-parametricmodels,suchasdecision"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "trees,whichwemightcallsemi-parametricmethods,canbeseenasdynamicallyconstructing somethingthatendsuplookinglikeamoretraditionalparametricmodel,butwheretheac- tualtrainingdataaffectsexactlywhattheformofthemodelwillbe.Othernon-parametric methods, such as nearest-neighbor, rely directly on the data to make predictions and do notcomputeamodelthatsummarizesthedata. Thesemi-parametricmethodstendtohavetheformofacompositionofsimplemodels. We’lllookat:"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Treemodels:partitiontheinputspaceandusedifferentsimplepredictionsondifferent • regionsofthespace;thisincreasesthehypothesisspace. Additivemodels: trainseveraldifferentclassifiersonthewholespaceandaveragethe • answers;thisdecreasestheestimationerror. Boostingisawaytoconstructanadditivemodelthatdecreasesbothestimationandstruc- turalerror,butwewon’taddressitinthisclass. Whyarewestudyingthesemethods,intheheydayofneuralnetworks? Theyarefasttoimplementandhavefewornohyper-parameterstotune. •"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Theyoftenworkaswellorbetterthanmorecomplicatedmethods. • Bothcanbeeasiertoexplaintoahumanuser:decision-treesarefairlydirectlyhuman- • interpretableandnearestneighbormethodscanjustifytheirdecisiontosomeextent byshowingafewtrainingexamplesthatthepredictionwasbasedon. 97MIT6.036 Fall2019 98 2 Trees Theideahereisthatwewouldliketofindapartitionoftheinputspaceandthenfitvery simplemodelstopredicttheoutputineachpiece. Thepartitionisdescribedusinga(typi-"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "callybinary)“decisiontree,”whichrecursivelysplitsthespace. Thesemethodsdifferby: Theclassofpossiblewaystosplitthespaceateachnode; thesearegenerallylinear • splits,eitheralignedwiththeaxesofthespace,orsometimesmoregeneralclassifiers. The class of predictors within the partitions; these are often simply constants, but • maybemoregeneralclassificationorregressionmodels. The way in which we control the complexity of the hypothesis: it would be within •"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "thecapacityofthesemethodstohaveaseparatepartitionforeachindividualtraining example. Thealgorithmformakingthepartitionsandfittingthemodels. • Theprimaryadvantageoftreemodelsisthattheyareeasilyinterpretablebyhumans. This is important in application domains, such as medicine, where there are human ex- pertswhooftenultimatelymakecriticaldecisionsandwhoneedtofeelconfidentintheir understandingofrecommendationsmadebyanalgorithm. These methods are most appropriate for domains where the input space is"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "not very high-dimensionalandwheretheindividualinputfeatureshavesomesubstantiallyuseful information individually or in small groups. They would not be good for image input, but might be good in cases with, for example, a set of meaningful measurements of the conditionofapatientinthehospital. We’ll concentrate on the CART/ID3 family of algorithms, which were invented inde- pendentlyinthestatisticsandtheartificialintelligencecommunities.Theyworkbygreed-"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "ilyconstructingapartition,wherethesplitsareaxisalignedandbyfittingaconstantmodel in the leaves. The interesting questions are how to select the splits and how to control complexity. Theregressionandclassificationversionsareverysimilar. 2.1 Regression Thepredictorismadeupof a partition function, π, mapping elements of the input space into exactly one of M • regions,R ,...,R ,and 1 M acollectionofMoutputvalues,O ,oneforeachregion. m • Ifwealreadyknewadivisionofthespaceintoregions,wewouldsetO"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": ",theconstant m outputforregionR ,tobetheaverageofthetrainingoutputvaluesinthatregion;thatis: m O =average y(i) . m {i|x(i) ∈ Rm} Definetheerrorinaregionas E = (y(i)−O )2 . m m {i|x(Xi) ∈ Rm} Ideally,wewouldselectthepartitiontominimize M λM+ E , m m X =1 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 99 for some regularization constant λ. It is enough to search over all partitions of the train- ing data (not all partitions of the input space!) to optimize this, but the problem is NP- complete."
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Study Question: Be sure you understand why it’s enough to consider all partitions of the training data, if this is your objective. 2.1.1 Buildingatree So,we’llbegreedy. Weestablishacriterion,givenasetofdata,forfindingthebestsingle split of that data, and then apply it recursively to partition the space. We will select the partitionofthedatathatminimizesthesumofthemeansquarederrorsofeachpartition. GivenadatasetD,let • R+ j,s (D) = {x ∈ D | x j > s} be the set of examples in data set D whose"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "value in dimensionjisgreaterthanorequaltos; R− (D) = {x D | x < s}bethesetofexamplesinDwhosevalueindimensionjis • j,s ∈ j lessthans; yˆ+ = average y(i) be the average y value of the data points in set • j,s {i|x(i) ∈ R+ j,s (D)} R+ (D);and j,s yˆ− = average y(i) be the average y value of the data points in set • j,s {i|x(i) ∈ R− j,s (D)} R− (D). j,s Now,hereisthepseudocode. BuildTree(D,k): If|D|6k: returnLeaf(D) • • Findthedimensionjandsplitpointsthatminimizes: E R− j,s (D) +E R+ j,s (D) ."
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "ReturnNode(j,s,BuildTree(R− (D,k)),BuildTree(R+ (D,k)) • j,s j,s EachcalltoBuildTreeconsidersO(dn)splits(forddimensions,sinceweonlyneedto splitbetweeneachdatapointineachdimension);eachrequiresO(n)workwherenisthe numberofdatapointsconsidered(n=|D|ifalldatapointsinDareused). Study Question: Concretely, what would be a good set of split-points to consider for dimension j of a dataset D? 2.1.2 Pruning It might be tempting to regularize by using a somewhat large value of k, or by stopping"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "whensplittinganodedoesnotsignificantlydecreasetheerror. Oneproblemwithshort- sightedstoppingcriteriaisthattheymightnotseethevalueofasplitthatwillrequireone moresplitbeforeitseemsuseful. Study Question: Apply the decision-tree algorithm to the XOR problem in two di- mensions. What is the training-set error of all possible hypothesis based on a single split? So,wewilltendtobuildatreethatistoolarge,andthenpruneitback. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 100"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "DefinecostcomplexityofatreeT,wheremrangesoveritsleavesas |T| C (T)= E (T)+α|T| . α m m X =1 Forafixedα,wecanfindaT that(approximately)minimizesC (T)by“weakest-link” α pruning: Createasequenceoftreesbysuccessivelyremovingthebottom-levelsplitthatmini- • mizestheincreaseinoverallerror,untiltherootisreached. ReturntheT inthesequencethatminimizesthecriterion. • Wecanchooseanappropriateαusingcrossvalidation. 2.2 Classification"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Thestrategyforbuildingandpruningclassificationtreesisverysimilartothestrategyfor regressiontrees. GivenaregionR correspondingtoaleafofthetree,wewouldpicktheoutputclass m ytobethevaluethatexistsmostfrequently(themajorityvalue)inthedatapointswhosex valuesareinthatregion: O =majority y(i) . m {i|x(i) ∈ Rm} DefinetheerrorinaregionasthenumberofdatapointsthatdonothavethevalueO : m E = {i|x(i) R andy(i) =O } . m m m ∈ 6 (cid:12) (cid:12) Definetheempiricalprobabilityof(cid:12)"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "(cid:12)anitemfromclasskoccurrin(cid:12) (cid:12)ginregionmas: {i|x(i) R andy(i) =k} Pˆ =Pˆ(R )(k)= ∈ m , mk m N (cid:12) m (cid:12) (cid:12) (cid:12) whereN isthenumberoftrainingpointsinregionm. We’lldefinetheempiricalproba- m bilitiesofsplitvalues,aswell,forlateruse. Pˆ =Pˆ(R )(v)= {i|x(i) ∈ R m andx( j i) >v} mjv mj (cid:12) N (cid:12) (cid:12) m (cid:12) (cid:12) (cid:12) Splittingcriteria Inourgreedyalgorithm,weneedawaytodecidewhichsplittomake next."
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Therearemanycriteriathatexpresssomemeasureofthe“impurity”inchildnodes. Somemeasuresinclude: Misclassificationerror: • E Q (T)= m =1−Pˆ m N mOm m Giniindex: • Q (T)= Pˆ (1−Pˆ ) m mk mk k X Entropy: • Q (T)=H(R )=− Pˆ log Pˆ m m mk 2 mk k X Sothatthisiswell-definedwhenPˆ =0,wewillstipulatethat0log 0=0. 2 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 101 They are very similar, and it’s not entirely obvious which one is better. We will focus on entropy,justtobeconcrete."
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Choosingthesplitthatminimizestheentropyofthechildrenisequivalenttomaximize theinformationgainofthetestX =v,definedby j infoGain(X =v,R ) = H(R )− Pˆ H(R+ )+(1−Pˆ )H(R− ) j m m mjv j,v mjv j,v (cid:16) (cid:17) Inthetwo-classcase,allthecriteriahavethevalues 0.0 whenPˆ =0.0 m0 (cid:14)0.0 whenPˆ m0 =1.0 Therespectiveimpuritycurvesareshownbelow,wherep=pˆ : m0 Thereusedtobeendlesshagglingaboutwhichonetouse.Itseemstobetraditionaltouse: Entropytoselectwhichnodetosplitwhilegrowingthetree •"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Misclassificationerrorinthepruningcriterion • Asaconcreteexample,considerthefollowingimages: The left image depicts a set of labeled data points and the right shows a partition into regionsbyadecisiontree. Pointsabouttrees Therearemanyvariationsonthistheme: Linearregressionorotherregressionorclassificationmethodineachleaf • LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 102 Non-axis-parallelsplits: e.g.,runaperceptronforawhiletogetasplit. • What’sgoodabouttrees: Easilyinterpretable •"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Fasttotrain! • Easytohandlemulti-classclassification • Easytohandledifferentlossfunctions(justchangepredictorintheleaves) • What’sbadabouttrees: Highestimationerror:smallchangesinthedatacanresultinverybigchangesinthe • hypothesis. Oftennotthebestpredictions • Hierarchical mixture of experts Make a “soft” version of trees, in which the splits are probabilistic(soeverypointhassomedegreeofmembershipineveryleaf).Canbetrained withaformofgradientdescent. 3 Bagging"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Bootstrapaggregationisatechniqueforreducingtheestimationerrorofanon-linearpredic- tor,oronethatisadaptivetothedata. ConstructBnewdatasetsofsizenbysamplingthemwithreplacementfromD • Trainapredictoroneachone: fˆb • Regressioncase: baggedpredictoris • B 1 fˆ (x)= fˆb(x) bag B b X =1 Classification case: majority bagged predictor: let fˆb(x) be a “one-hot” vector with a • single1andK−1zeros,sothatyˆb(x)=argmax fˆb(x) . Then k k B 1 fˆ (x)= fˆb(x), bag B b X =1"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "whichisavectorcontainingtheproportionofclassifiersthatpredictedeachclassk forinputx;andthepredictedoutputis yˆ (x)=argmaxfˆ (x) . bag bag k k Therearetheoreticalargumentsshowingthatbaggingdoes,infact,reduceestimation error. However,whenwebagamodel,anysimpleintrepetabilityislost. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 103 3.1 RandomForests Random forests are collections of trees that are constructed to be de-correlated, so that using them to vote gives maximal advantage. In competitions,"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "they often have the best classificationperformanceamonglargecollectionsofmuchfanciermethods. Forb=1..B DrawabootstrapsampleD ofsizenfromD b • GrowatreeondataD byrecursivelyrepeatingthesesteps: b • – Selectmvariablesatrandomfromthedvariables – Pickthebestvariableandsplitpointamongthem – Splitthenode returntreeT b • Giventheensembleoftrees,votetomakeapredictiononanewx. 4 Nearest Neighbor Innearest-neighbormodels,wedon’tdoanyprocessingofthedataattrainingtime–we justrememberit!"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Alltheworkisdoneatpredictiontime. InputvaluesxcanbefromanydomainX(R d,documents,tree-structuredobjects,etc.). We just need a distance metric, d : X X R+, which satisfies the following, for all × → x,x ,x X: 0 00 ∈ d(x,x)=0 d(x,x 0)=d(x 0,x) d(x,x 00)6d(x,x 0)+d(x 0,x 00) Givenadata-setD={(x(i),y(i))}n ,ourpredictorforanewx Xis i=1 ∈ h(x)=y(i) where i=argmind(x,x(i)) , i thatis,thepredictedoutputassociatedwiththetrainingpointthatisclosesttothequery pointx."
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "Thissamealgorithmworksforregressionandclassification! It’safloorwaxanda The nearest neighbor prediction function can be described by a Voronoi partition (di- desserttopping! vidingthespaceupintoregionswhoseclosestpointiseachindividualtrainingpoint)as shownbelow: LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 104 Ineachregion,wepredicttheassociatedyvalue. Study Question: Convince yourself that these boundaries do represent the nearest- neighbor classifier derived from these 6 data points. There"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "are several useful variations on this method. In k-nearest-neighbors, we find the ktrainingpointsnearesttothequerypointxandoutputthemajorityyvalueforclassifi- cationortheaverageforregression. Wecanalsodolocallyweightedregressioninwhichwe fit locally linear regression models to the k nearest points, possibly giving less weight to thosethatarefartheraway. Inlargedata-sets, itisimportanttousegooddatastructures (e.g.,balltrees)toperformthenearest-neighborlook-upsefficiently(withoutlookingatall"
  },
  {
    "file": "mit_ocw_ch_14.pdf",
    "chunk": "thedatapointseachtime). LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "2 CHAPTER Linear classifiers 1 Classification A binary classifier is a mapping from R d {−1,+1}. We’ll often use the letter h (for Actually,generalclassi- → hypothesis)tostandforaclassifier,sotheclassificationprocesslookslike: fierscanhavearange whichisanydiscrete x h y . set,butwe’llworkwith → → thisspecificcasefora Real life rarely gives us vectors of real numbers; the x we really want to classify is while. usuallysomethinglikeasong,image,orperson.Inthatcase,we’llhavetodefineafunction ϕ(x),"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "whose domain is R d, where ϕ represents features of x, like a person’s height or the amountofbassinasong,andthenlettheh : ϕ(x) {−1,+1}. Inmuchofthefollowing, → we’llomitexplicitmentionofϕandassumethatthex(i)areinR d,butyoushouldalways haveinmindthatsomeadditionalprocesswasalmostsurelyrequiredtogofromtheactual inputexamplestotheirfeaturerepresentation. Insupervisedlearningwearegivenatrainingdatasetoftheform D = x(1),y(1) ,..., x(n),y(n) . n (cid:10)(cid:16) (cid:17) (cid:16) (cid:17)(cid:11)"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "Wewillassumethateachx(i)isad 1columnvector.Theintendedmeaningofthisdatais × that,whengivenaninputx(i),thelearnedhypothesisshouldgenerateoutputy(i). What makes a classifier useful? That it works well on new data; that is, that it makes good predictions on examples it hasn’t seen. But we don’t know exactly what data this Myfavoriteanalogy classifier might be tested on when we use it in the real world. So, we have to assume a istoproblemsets. We evaluateastudent’s"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "connectionbetweenthetrainingdataandtestingdata;typically,theyaredrawnindepen- abilitytogeneralizeby dentlyfromthesameprobabilitydistribution. puttingquestionsonthe GivenatrainingsetD n andaclassifierh,wecandefinethetrainingerrorofhtobe examthatwerenoton thehomework(training 1 n 1 h(x(i))=y(i) set). E (h)= 6 . n n (cid:14)0 otherwise X i=1 Fornow,wewilltrytofindaclassifierwithsmalltrainingerror(later,withsomeadded criteria)andhopeitgeneralizeswelltonewdata,andhasasmalltesterror 1 n+n0 1"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "h(x(i))=y(i) E(h)= 6 n 0 i= X n+1 (cid:14)0 otherwise 11MIT6.036 Fall2019 12 onn newexamplesthatwerenotusedintheprocessoffindingtheclassifier. 0 2 Learning algorithm AhypothesisclassHisaset(finiteorinfinite)ofpossibleclassifiers,eachofwhichrepresents amappingfromR d {−1,+1}. → A learning algorithm is a procedure that takes a data set D as input and returns an n elementhofH;itlookslike D learningalg(H) h n −→ −→ We willfind thatthe choiceof H can havea bigimpact onthe testerror ofthe h that"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "resultsfromthisprocess. Onewaytogeththatgeneralizeswellistorestrictthesize, or “expressiveness”ofH. 3 Linear classifiers We’ll start with the hypothesis class of linear classifiers. They are (relatively) easy to un- derstand,simpleinamathematicalsense,powerfulontheirown,andthebasisformany othermoresophisticatedmethods. A linear classifier in d dimensions is defined by a vector of parameters θ R d and ∈ scalarθ 0 R.So,thehypothesisclassHoflinearclassifiersinddimensionsisthesetofall ∈ vectorsinR"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "d+1. We’llassumethatθisad 1columnvector. × Givenparticularvaluesforθandθ 0 ,the classifierisdefinedby Let’sbecarefulabout dimensions. Wehave +1 ifθTx+θ >0 assumedthatxandθ h(x;θ,θ )=sign(θTx+θ )= 0 . arebothd 1column 0 0 (cid:14)−1 otherwise vectors. So × θTxis1 1, × whichinmath(butnot Rememberthatwecanthinkofθ,θ 0 asspecifyingahyperplane. ItdividesR d,thespace necessarilynumpy)is thesameasascalar. ourx(i)pointslivein,intotwohalf-spaces. Theonethatisonthesamesideasthenormal vector is the"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "positive half-space, and we classify all points in that space as positive. The half-spaceontheothersideisnegativeandallpointsinitareclassifiedasnegative. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 13 −1 Example: Lethbethelinearclassifierdefinedbyθ= ,θ =3. 1.5 0 (cid:20) (cid:21) 3 Thediagrambelowshowsseveralpointsclassifiedbyh.Inparticular,letx(1) = 2 (cid:20) (cid:21) 4 andx(2) = . −1 (cid:20) (cid:21) 3 h(x(1);θ,θ )=sign −1 1.5 +3 =sign(3)=+1 0 2 (cid:18) (cid:20) (cid:21) (cid:19)"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "(cid:2) (cid:3) 4 h(x(2);θ,θ )=sign −1 1.5 +3 =sign(−2.5)=−1 0 −1 (cid:18) (cid:20) (cid:21) (cid:19) (cid:2) (cid:3) Thus,x(1)andx(2)aregivenpositiveandnegativeclassfications,respectively. x(1) θTx+θ = 0 0 θ x(2) Study Question: What is the green vector normal to the hyperplane? Specify it as a column vector. Study Question: What change would you have to make to θ,θ if you wanted to 0 have the separating hyperplane in the same place, but to classify all the points la- beled ’+’ in the diagram"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "as negative and all the points labeled ’-’ in the diagram as positive? 4 Learning linear classifiers Now,givenadatasetandthehypothesisclassoflinearclassifiers,ourobjectivewillbeto findthelinearclassifierwiththesmallestpossibletrainingerror. Thisisawell-formedoptimizationproblem. Butit’snotcomputationallyeasy! We’ll start by considering a very simple learning algorithm. The idea is to generate It’sagoodideatothink k possible hypotheses by generating their parameter vectors at random. Then, we"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "can ofthe“stupidestpossi- ble”solutiontoaprob- evaluate the training-set error on each of the hypotheses and return the hypothesis that lem,beforetryingtoget hasthelowesttrainingerror(breakingtiesarbitrarily). clever. Here’safairly (butnotcompletely) stupidalgorithm. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 14 RANDOM-LINEAR-CLASSIFIER(D n ,k,d) 1 forj = 1tok 2 randomlysample θ(j),θ(j) from(R d,R) 0 3 j = argmin E (cid:16) θ(j),θ(j)(cid:17) ∗ j {1,...,k} n 0 ∈ 4 return θ(j∗),θ(j∗) (cid:16)"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "(cid:17) 0 (cid:16) (cid:17) Anoteaboutnotation. Thismightbenewno- tation: argmin f(x) Study Question: What do you think happens to E (h), where h is the hypothesis x n meansthevalueofx returned by RANDOM-LINEAR-CLASSIFIER, as k is increased? forwhichf(x)isthe smallest. Sometimeswe Study Question: What properties of D n do you think will have an effect on E n (h)? writeargmin x X f(x) whenwewant∈toex- plicitlyspecifytheset Xofvaluesofxover 5 Evaluating a learning algorithm whichwewanttomini-"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "mize. Howshouldweevaluatetheperformanceofaclassifierh? Thebestmethodistomeasure testerrorondatathatwasnotusedtotrainit. Howshouldweevaluatetheperformanceofalearningalgorithm?Thisistrickier.There aremanypotentialsourcesofvariabilityinthepossibleresultofcomputingtesterrorona learnedhypothesish: WhichparticulartrainingexamplesoccurredinD n • WhichparticulartestingexamplesoccurredinD • n0 Randomizationinsidethelearningalgorithmitself • Generally,wewouldliketoexecutethefollowingprocessmultipletimes:"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "Trainonanewtrainingset • Evaluateresultinghonatestingsetthatdoesnotoverlapthetrainingset • Doingthismultipletimescontrolsforpossiblepoorchoicesoftrainingsetorunfortunate randomizationinsidethealgorithmitself. One concern is that we might need a lot of data to do this, and in many applications data is expensive or difficult to acquire. We can re-use data with cross validation (but it’s hardertodotheoreticalanalysis). CROSS-VALIDATE(D,k) 1 divideDintokchunksD ,D ,...D (ofroughlyequalsize) 1 2 k 2"
  },
  {
    "file": "mit_ocw_ch_2.pdf",
    "chunk": "fori = 1tok 3 trainh onD\\D (withholdingchunkD ) i i i 4 compute“test”errorE (h )onwithhelddataD i i i 5 return 1 k E (h ) k i=1 i i P It’sveryimportanttounderstandthatcross-validationneitherdeliversnorevaluatesa singleparticularhypothesish. Itevaluatesthealgorithmthatproduceshypotheses. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "3 CHAPTER The Perceptron Firstofall,thecoolestalgorithmname! Itisbasedonthe1943modelofneuronsmadeby Well,maybe“neocogni- McCulloch and Pitts and by Hebb. It was developed by Rosenblatt in 1962. At the time, tron,”alsothenameof arealMLalgorithm,is it was not interpreted as attempting to optimize any particular criteria; it was presented cooler. directlyasanalgorithm. Therehas,since,beenahugeamountofstudyandanalysisofits convergencepropertiesandotheraspectsofitsbehavior. 1 Algorithm"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "RecallthatwehaveatrainingdatasetD n withx R d,andy {−1,+1}. ThePerceptron ∈ ∈ algorithmtrainsabinaryclassifierh(x;θ,θ )usingthefollowingalgorithmtofindθand 0 θ 0 usingτ iterativesteps: WeuseGreekletterτ hereinsteadofT sowe PERCEPTRON(τ,D n ) d tr o a n n ’ s t p c o o s n e f ! useitwith T 1 θ = 0 0 0 ··· 2 θ = 0 0 (cid:2) (cid:3) 3 fort = 1toτ 4 fori = 1ton 5 ify(i) θTx(i)+θ 0 60 6 θ = θ+y(i)x(i) (cid:0) (cid:1) 7 θ = θ +y(i) 0 0 8 returnθ,θ 0 Let’scheckdimensions."
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Intuitively,oneachstep,ifthecurrenthypothesisθ,θ classifiesexamplex(i)correctly, Rememberthatθis 0 then no change is made. If it classifies x(i) incorrectly, then it moves θ,θ 0 so that it is d y(×i) 1 is ,x a ( s i) ca is la d r. × D 1 o , es and “closer”toclassifyingx(i),y(i)correctly. everythingmatch? Notethatifthealgorithmevergoesthroughoneiterationofthelooponline4without making an update, it will never make any further updates (verify that you believe this!)"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "andsoitshouldjustterminateatthatpoint. Study Question: What is true about E if that happens? n 15MIT6.036 Fall2019 16 1 Example: Let h be the linear classifier defined by θ(0) = ,θ(0) = 1. The dia- −1 0 (cid:20) (cid:21) gram below shows several points classified by h. However, in this case, h (repre- 1 sentedbytheboldline)misclassifiesthepointx(1) = whichhaslabely(1) = 1. 3 (cid:20) (cid:21) Indeed, 1 y(1) θTx(1)+θ = 1 −1 +1=−1<0 0 3 (cid:16) (cid:17) (cid:2) (cid:3) (cid:20) (cid:21)"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "ByrunninganiterationofthePerceptronalgorithm,weupdate 2 θ(1) =θ(0)+y(1)x(1) = 2 (cid:20) (cid:21) θ(1) =θ(0)+y(1) =2 0 0 The new classifier (represented by the dashed line) now correctly classifies that point,butnowmakesamistakeonthenegativelylabeledpoint. θ(0)T x+θ(0) = 0 0 x(1) θ(0) θ(1) θ(1)T x+θ(1) = 0 0 Areallyimportantfactabouttheperceptronalgorithmisthat,ifthereisalinearclassi- fierwith0trainingerror,thenthisalgorithmwill(eventually)findit! We’lllookataproof ofthisindetail,next. 2 Offset"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Sometimes,itcanbeeasiertoimplementoranalyzeclassifiersoftheform +1 ifθTx>0 h(x;θ)= (cid:14)−1 otherwise. Withoutanexplicitoffsetterm(θ ),thisseparatormustpassthroughtheorigin,whichmay 0 appeartobelimiting. However,wecanconvertanyprobleminvolvingalinearseparator withoffsetintoonewithnooffset(butofhigherdimension)! LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 17 Consider the d-dimensional linear separator defined by θ = θ θ θ and 1 2 d ··· offsetθ . 0 (cid:2) (cid:3) toeachdatapointx"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "D,appendacoordinatewithvalue+1,yielding • ∈ T x = x x +1 new 1 ··· d (cid:2) (cid:3) define • T θ = θ θ θ new 1 ··· d 0 (cid:2) (cid:3) Then, θT x =θ x + +θ x +θ 1 new· new 1 1 ··· d d 0 · =θTx+θ 0 Thus, θ is an equivalent ((d+1)-dimensional) separator to our original, but with no new offset. Considerthedataset: X=[[1],[2],[3],[4]] Y =[[+1],[+1],[−1],[−1]] Itislinearlyseparableind = 1withθ = [−1]andθ = 2.5. Butitisnotlinearlyseparable 0 throughtheorigin! Now,let 1 2 3 4 X = new 1 1 1 1"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "(cid:20)(cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)(cid:21) Thisnewdatasetisseparablethroughtheorigin,withθ =[−1,2.5]T. new Wecanmakeasimplifiedversionoftheperceptronalgorithmifwerestrictourselves toseparatorsthroughtheorigin: Welistitherebecause thisistheversionofthe PERCEPTRON-THROUGH-ORIGIN(τ,D n ) algorithmwe’llstudyin moredetail. T 1 θ = 0 0 0 ··· 2 fort = 1toτ (cid:2) (cid:3) 3 fori = 1ton 4 ify(i) θTx(i) 60 5 θ = θ+y(i)x(i) (cid:0) (cid:1) 6 returnθ 3 Theory of"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "the perceptron Now, we’llsaysomethingformalabouthowwelltheperceptronalgorithmreallyworks. Westartbycharacterizingthesetofproblemsthatcanbesolvedperfectlybytheperceptron algorithm,andthenprovethat,infact,itcansolvetheseproblems.Inaddition,weprovide anotionofwhatmakesaproblemdifficultforperceptronandlinkthatnotionofdifficulty tothenumberofiterationsthealgorithmwilltake. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 18 3.1 Linearseparability AtrainingsetD islinearlyseparableifthereexistθ,θ"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "suchthat,foralli=1,2,...,n: n 0 y(i) θTx(i)+θ >0 . 0 (cid:16) (cid:17) Anotherwaytosaythisisthatallpredictionsonthetrainingsetarecorrect: h(x(i);θ,θ )=y(i) . 0 And,anotherwaytosaythisisthatthetrainingerroriszero: E (h)=0 . n 3.2 Convergencetheorem Thebasicresultabouttheperceptronisthat,ifthetrainingdataD islinearlyseparable, n thentheperceptronalgorithmisguaranteedtofindalinearseparator. Ifthetrainingdatais Wewillmorespecificallycharacterizethelinearseparabilityofthedatasetbythemargin"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "notlinearlyseparable, thealgorithmwillnot oftheseparator.We’llstartbydefiningthemarginofapointwithrespecttoahyperplane. beabletotellyoufor First,recallthatthedistanceofapointxtothehyperplaneθ,θ is 0 sure,infinitetime,that itisnotlinearlysepa- θTx+θ 0 rable. Thereareother . θ algorithmsthatcan k k testforlinearsepara- bilitywithrun-times Then,we’lldefinethemarginofalabeledpoint(x,y)withrespecttohyperplaneθ,θ tobe 0 O(nd/2)orO(d2n)or O(nd−1logn). θTx+θ y 0 . · θ k k"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Thisquantitywillbepositiveifandonlyifthepointxisclassifiedasybythelinearclassi- fierrepresentedbythishyperplane. Study Question: What sign does the margin have if the point is incorrectly classi- fied? Be sure you can explain why. Now, the margin of a dataset D with respect to the hyperplane θ,θ is the minimum n 0 marginofanypointwithrespecttoθ,θ : 0 θTx(i)+θ min y(i) 0 . i · θ (cid:18) k k (cid:19) Themarginispositiveifandonlyifallofthepointsinthedata-setareclassifiedcorrectly."
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Inthatcase(only!) itrepresentsthedistancefromthehyperplanetotheclosestpoint. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 19 1 Example: Lethbethelinearclassifierdefinedbyθ= ,θ =1. −1 0 (cid:20) (cid:21) Thediagrambelowshowsseveralpointsclassifiedbyh,oneofwhichismisclassi- fied. Wecomputethemarginforeachpoint: θTx+θ = 0 0 x(1) x(3) x(2) θ θTx(1)+θ −2+1 √2 y(1) 0 =1 =− · θ · √2 2 k k θTx(2)+θ 1+1 y(2) 0 =1 =√2 · θ · √2 k k θTx(3)+θ −3+1 y(3) 0 =−1 =√2 · θ · √2 k k Notethatsincepointx(1)"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "ismisclassified, itsmarginisnegative. Thusthemargin forthewholedatasetisgivenby−√2. 2 Theorem 3.1 (Perceptron Convergence). For simplicity, we consider the case where the linear separatormustpassthroughtheorigin. Ifthefollowingconditionshold: (a) thereexistsθ ∗suchthaty(i)θ∗ k T θ x ∗ ( k i) >γforalli=1,...,nandforsomeγ>0and (b) alltheexampleshaveboundedmagnitude: x(i) 6Rforalli=1,...n, (cid:13) 2(cid:13) thentheperceptronalgorithmwillmakeatmost(cid:13)R (cid:13)mistakes."
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Atthispoint,itshypothesiswill γ bealinearseparatorofthedata. (cid:16) (cid:17) Proof. Weinitializeθ(0) = 0,andletθ(k) defineourhyperplaneaftertheperceptronalgo- rithmhasmadekmistakes. Wearegoingtothinkabouttheanglebetweenthehypothesis wehavenow,θ(k)andtheassumedgoodseparatorθ .Sincetheybothgothroughtheori- ∗ gin,ifwecanshowthattheanglebetweenthemisdecreasingusefullyoneveryiteration, thenwewillgetclosetothatseparator. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 20"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "So,let’sthinkaboutthecosoftheanglebetweenthem,andrecall,bythedefinitionof dotproduct: θ(k) θ cos θ(k),θ ∗ = · ∗ θ θ(k) (cid:16) (cid:17) k ∗k We’lldividethisupintotwofactors, (cid:13) (cid:13) (cid:13) (cid:13) θ(k) θ 1 cos θ(k),θ ∗ = · ∗ , (3.1) (cid:16) (cid:17) (cid:18) k θ ∗k (cid:19) · θ(k) ! (cid:13) (cid:13) andstartbyfocusingonthefirstfactor. (cid:13) (cid:13) Without loss of generality, assume that the kth mistake occurs on the ith example x(i),y(i) . (cid:0) (cid:1) θ(k) · θ ∗ ="
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "θ(k−1)+y(i)x(i) · θ ∗ θ θ k ∗k (cid:0) k ∗k (cid:1) θ(k−1) θ y(i)x(i) θ ∗ ∗ = · + · θ θ k ∗k k ∗k θ(k−1) θ ∗ > · +γ θ k ∗k >kγ wherewehavefirstappliedthemarginconditionfrom(a)andthenappliedsimpleinduc- tion. Now, we’ll look at the second factor in equation 3.1. We note that since x(i),y(i) is classifiedincorrectly,y(i) θ(k−1)T x(i) 60. Thus, (cid:0) (cid:1) (cid:16) (cid:17) 2 2 θ(k) = θ(k−1)+y(i)x(i) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13)θ(k−1) 2 +2y(i)θ (cid:13)"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "(cid:13)(k−1)T x(i)+ x(i) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) 6 (cid:13) (cid:13) θ(k−1)(cid:13) (cid:13) +R2 (cid:13) (cid:13) (cid:13) (cid:13) 6 (cid:13) (cid:13)kR2 (cid:13) (cid:13) (cid:13) (cid:13) wherewehaveadditionallyappliedtheassumptionfrom(b)andthenagainusedsimple induction. Returningtothedefinitionofthedotproduct,wehave θ(k) θ θ(k) θ 1 1 γ cos θ(k),θ ∗ = · ∗ = · ∗ >(kγ) =√k (cid:16) (cid:17) θ(k) k θ ∗k (cid:18) k θ ∗k (cid:19) θ(k) · √kR · R"
  },
  {
    "file": "mit_ocw_ch_3.pdf",
    "chunk": "Sincethevalueofthecos(cid:13)ineis(cid:13)atmost1,wehave (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) γ 1> √k · R R 2 k6 . γ (cid:18) (cid:19) ThisresultendowsthemarginγofD withanoperationalmeaning: whenusingthe n Perceptron algorithm for classification, at most (R/γ)2 classification errors will be made, whereRisanupperboundonthemagnitudeofthetrainingvectors. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "4 CHAPTER Feature representation Linearclassifiersareeasytoworkwithandanalyze,buttheyareaveryrestrictedclassof hypotheses. If we have to make a complex distinction in low dimensions, then they are unhelpful. Ourfavoriteillustrativeexampleisthe“exclusiveor”(XOR)dataset,thedrosophilaof D.Melanogasterisa machine-learningdatasets: speciesoffruitfly,used asasimplesystemin whichtostudygenetics, since1910. There is no linear separator for this two-dimensional dataset! But, we have a trick available:"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "takealow-dimensionaldatasetandmoveit,usinganon-lineartransformation into a higher-dimensional space, and look for a linear separator there. Let’s look at an exampledatasetthatstartsin1-D: x 0 Thesepointsarenotlinearlyseparable,butconsiderthetransformationφ(x) = [x,x2]. What’salinearsepara- Putting the data in φ space, we see that it is now separable. There are lots of possible torfordatain1D?A point! separators;wehavejustshownoneofthemhere. 21MIT6.036 Fall2019 22 x2 separator x A linear"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "separator in φ space is a nonlinear separator in the original space! Let’s see howthisplaysoutinoursimpleexample. Considertheseparatorx2−1=0,whichlabels thehalf-planex2−1 > 0aspositive. Whatseparatordoesitcorrespondtointheoriginal 1-Dspace? Wehavetoaskthequestion: whichxvalueshavethepropertythatx2−1=0. Theansweris+1and−1,sothosetwopointsconstituteourseparator,backintheoriginal space. And we can use the same reasoning to find the region of 1D space that is labeled positivebythisseparator. x -1 1"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "0 This is a very general and widely useful strategy. It’s the basis for kernel methods, a powerfultechniquethatweunfortunatelywon’tgettointhisclass, andcanbeseenasa motivationformulti-layerneuralnetworks. Therearemanydifferentwaystoconstructφ. Somearerelativelysystematicanddo- main independent. We’ll look at the polynomial basis in section 1 as an example of that. Othersaredirectlyrelatedtothesemantics(meaning)oftheoriginalfeatures,andwecon- structthemdeliberatelywithourdomaininmind."
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "We’llexplorethatstrategyinsection2. 1 Polynomial basis Ifthefeaturesinyourproblemarealreadynaturallynumerical,onesystematicstrategyfor constructing a new feature space is to use a polynomial basis. The idea is that, if you are using the kth-order basis (where k is a positive integer), you include a feature for every possibleproductofkdifferentdimensionsinyouroriginalinput. Hereisatableillustratingthekthorderpolynomialbasisfordifferentvaluesofk. Order d=1 ingeneral 0 [1] [1] 1 [1,x] [1,x ,...,x"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "] 1 d 2 [1,x,x2] [1,x ,...,x ,x2,x x ,...] 1 d 1 1 2 3 [1,x,x2,x3] [1,x ,...,x2,x x ,...,x x x ,...] 1 1 1 2 1 2 3 . . . . . . . . . So, what if we try to solve the XOR problem using a polynomial basis as the feature transformation?Wecanjusttakeourtwo-dimensionaldataandtransformitintoahigher- LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 23 dimensionaldataset,byapplyingφ. Now,wehaveaclassificationproblemasusual,and wecanusetheperceptronalgorithmtosolveit. Let’stryitfork=2onourXORproblem."
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "Thefeaturetransformationis φ((x ,x ))=(1,x ,x ,x2,x x ,x2) . 1 2 1 2 1 1 2 2 Study Question: If we use perceptron to train a classifier after performing this fea- ture transformation, would we lose any expressive power if we let θ =0 (i.e. trained 0 without offset instead of with offset)? After4iterations,perceptronfindsaseparatorwithcoefficientsθ = (0,0,0,0,4,0)and θ =0. Thiscorrespondsto 0 0+0x +0x +0x2+4x x +0x2+0=0 1 2 1 1 2 2 and is plotted below, with the gray shaded region classified as"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "negative and the white regionclassifiedaspositive: Study Question: Be sure you understand why this high-dimensional hyperplane is a separator, and how it corresponds to the figure. For fun, we show some more plots below. Here is the result of running perceptron on XOR, but where the data are put in a different place on the plane. After 65 mistakes (!) itarrivesatthesecoefficients: θ = (1,−1,−1,−5,11,−5), θ = 1, whichgeneratesthis 0 separator: Thejaggednessinthe plottingoftheseparator"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "isanartifactofalazy lpkstrategyformak- ingtheseplots–thetrue curvesaresmooth. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 24 Study Question: It takes many more iterations to solve this version. Apply knowl- edge of the convergence properties of the perceptron to understand why. Hereisaharderdataset.After200iterations,wecouldnotseparateitwithasecondor third-orderbasisrepresentation. Shownbelowaretheresultsafter200iterationsforbases oforder2,3,4,and5. 2 Hand-constructing features for real"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "domains Inmanymachine-learningapplications,wearegivendescriptionsoftheinputswithmany differenttypesofattributes, includingnumbers, words, anddiscretefeatures. Animpor- LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 25 tantfactorinthesuccessofanMLapplicationisthewaythatthefeaturesarechosentobe encodedbythehumanwhoisframingthelearningproblem. 2.1 Discretefeatures Gettingagoodencodingofdiscretefeaturesisparticularlyimportant. Youwanttocreate"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "“opportunities”fortheMLsystemtofindtheunderlyingregularities. Althoughthereare machine-learningmethodsthathavespecialmechanismsforhandlingdiscreteinputs,all the methods we consider in this class will assume the input vectors x are in R d. So, we haveto figureoutsome reasonablestrategies forturningdiscrete valuesinto(vectors of) realnumbers. We’llstartbylistingsomeencodingstrategies,andthenworkthroughsomeexamples. Let’sassumewehavesomefeatureinourrawdatathatcantakeononeofkdiscretevalues. Numeric"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "Assign each of these values a number, say 1.0/k,2.0/k,...,1.0. We might • wanttothendosomefurtherprocessing,asdescribedinsection2.3.Thisisasensible strategyonlywhenthediscretevaluesreallydosignifysomesortofnumericquantity, sothatthesenumericalvaluesaremeaningful. ThermometercodeIfyourdiscretevalueshaveanaturalordering,from1,...,k,but • notanaturalmappingintorealnumbers,agoodstrategyistouseavectoroflength kbinaryvariables,whereweconvertdiscreteinputvalue0 < j 6 kintoavectorin whichthefirst"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "jvaluesare1.0 andtherestare0.0. Thisdoes notnecessarilyimply anything about the spacing or numerical quantities of the inputs, but does convey somethingaboutordering. FactoredcodeIfyourdiscretevaluescansensiblybedecomposedintotwoparts(say • the“make”and“model”ofacar),thenit’sbesttotreatthoseastwoseparatefeatures, andchooseanappropriateencodingofeachonefromthislist. One-hot code If there is no obvious numeric, ordering, or factorial structure, then •"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "thebeststrategyistouseavectoroflengthk,whereweconvertdiscreteinputvalue 0<j6kintoavectorinwhichallvaluesare0.0,exceptforthejth,whichis1.0. BinarycodeItmightbetemptingforthecomputerscientistsamongustousesome • binary code, which would let us represent k values using a vector of length logk. Thisisabadidea! Decodingabinarycodetakesalotofwork,andbyencodingyour inputsthisway,you’dbeforcingyoursystemtolearnthedecodingalgorithm. Asanexample,imaginethatwewanttoencodebloodtypes,whicharedrawnfromthe set"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "{A+,A−,B+,B−,AB+,AB−,O+,O−}. There is no obvious linear numeric scaling or evenorderingtothisset.Butthereisareasonablefactoring,intotwofeatures:{A,B,AB,O} and{+,−1}. And,infact,wecanreasonablyfactorthefirstgroupinto{A,notA},{B,notB} So,herearetwoplausibleencodingsofthewholeset: Itissensible(according toWikipedia!) totreat Usea6-Dvector,withtwodimensionstoencodeeachofthefactorsusingaone-hot Oashavingneitherfea- • encoding. tureAnorfeatureB. Use a 3-D vector, with one dimension for each factor,"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "encoding its presence as 1.0 • andabsenceas−1.0(thisissometimesbetterthan0.0). Inthiscase, AB+wouldbe (1.0,1.0,1.0)andO−wouldbe(−1.0,−1.0,−1.0). Study Question: How would you encode A+ in both of these approaches? LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 26 2.2 Text Theproblemoftakingatext(suchasatweetoraproductreview,oreventhisdocument!) and encoding it as an input for a machine-learning algorithm is interesting and compli- cated. Much later in the class, we’ll study sequential input"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "models, where, rather than havingtoencodeatextasafixed-lengthfeaturevector,wefeeditintoahypothesisword byword(orevencharacterbycharacter!). Therearesomesimplerencodingsthatworkwellforbasicapplications.Oneofthemis thebagofwords(BOW)model.Theideaistoletdbethenumberofwordsinourvocabulary (eithercomputedfromthetrainingsetorsomeotherbodyoftextordictionary). Wewill thenmakeabinaryvector(withvalues1.0and0.0)oflengthd,whereelementjhasvalue 1.0ifwordjoccursinthedocument,and0.0otherwise. 2.3"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "Numericvalues Ifsomefeatureisalreadyencodedasanumericvalue(heartrate,stockprice,distance,etc.) thenyoushouldgenerallykeepitasanumericvalue.Anexceptionmightbeasituationin whichyouknowtherearenatural“breakpoints”inthesemantics: forexample,encoding someone’sageintheUS,youmightmakeanexplicitdistinctionbetweenunderandover 18(or21),dependingonwhatkindofthingyouaretryingtopredict. Itmightmakesense todivideintodiscretebins(possiblyspacingthemclosertogetherfortheveryyoung)and"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "touseaone-hotencodingforsomesortsofmedicalsituationsinwhichwedon’texpecta linear(orevenmonotonic)relationshipbetweenageandsomephysiologicalfeatures. If you choose to leave a feature as numeric, it is typically useful to scale it, so that it tends to be in the range [−1,+1]. Without performing this transformation, if you have onefeaturewithmuchlargervaluesthananother,itwilltakethelearningalgorithmalot of work to find parameters that can put them on an equal basis. So, we might perform x−x"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": "transformation φ(x) = , where x is the average of the x(i), and σ is the standard σ deviationofthex(i).Theresultingfeaturevalueswillhavemean0andstandarddeviation 1. Thistransformationissometimescalledstandardizingavariable. Suchstandardvariables Then, of course, you might apply a higher-order polynomial-basis transformation to areoftenknownas“z- scores,”forexample,in oneormoregroupsofnumericfeatures. thesocialsciences. Study Question: Percy Eptron has a domain with 4 numeric input features, (x"
  },
  {
    "file": "mit_ocw_ch_4.pdf",
    "chunk": ",...,x ). He decides to use a representation of the form 1 4 φ(x)=PolyBasis((x ,x ),3)_PolyBasis((x ,x ),3) 1 2 3 4 where a_b means the vector a concatenated with the vector b. What is the dimen- sion of Percy’s representation? Under what assumptions about the original features is this a reasonable choice? LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "5 CHAPTER Logistic regression 1 Machine learning as optimization The perceptron algorithm was originally written down directly via cleverness and intu- ition, and later analyzed theoretically. Another approach to designing machine learning algorithmsistoframethemasoptimizationproblems,andthenusestandardoptimization algorithmsandimplementationstoactuallyfindthehypothesis.Takingthisapproachwill allowustotakeadvantageofawealthofmathematicalandalgorithmictechniqueforun-"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "derstandingandsolvingoptimizationproblems,whichwillallowustomovetohypothesis classesthataresubstantiallymorecomplexthanlinearseparators. WebeginbywritingdownanobjectivefunctionJ(Θ),whereΘstandsforalltheparame- tersinourmodel.NotethatwewillsometimeswriteJ(θ,θ )becausewhenstudyinglinear 0 classifiers,wehaveusedthesetwonamesforpartsofourwholecollectionofparameters, soΘ=(θ,θ ).WealsooftenwriteJ(Θ;D)tomakeclearthedependenceonthedataD.The 0 objective function describes how we feel about possible"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "hypotheses Θ: we will generally lookforvaluesforparametersΘthatminimizetheobjectivefunction: YoucanthinkaboutΘ ∗ hereas“thethetathat Θ ∗ =argminJ(Θ) . minimizesJ”. Θ AverycommonformforanMLobjectiveis n 1 J(Θ)= L(h(x(i);Θ),y(i)) + λ R(Θ) . (5.1) n  X i=1 loss constant regularizer   The loss tells us how unhappy w|e are ab{ozut the}predic | ti { o z n } h(|x{(iz)};Θ) that Θ makes for (x(i),y(i)). Acommonexampleisthe0-1loss,introducedinchapter1: 0 ify=h(x;Θ) L (h(x;Θ),y)= , 01 (cid:14)1"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "otherwise whichgivesavalueof0foracorrectprediction,anda1foranincorrectprediction. Inthe caseoflinearseparators,thisbecomes: 0 ify(θTx+θ )>0 L (h(x;θ,θ ),y)= 0 . 01 0 (cid:14)1 otherwise 27MIT6.036 Fall2019 28 2 Regularization If all we cared about was finding a hypothesis with small loss on the training data, we would have no need for regularization, and could simply omit the second term in the objective. But remember that our ultimate goal is to perform well on input values that we haven’t"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "trained on! It may seem that this is an impossible task, but humans and machine- learningmethodsdothissuccessfullyallthetime.Whatallowsgeneralizationtonewinput valuesisabeliefthatthereisanunderlyingregularitythatgovernsboththetrainingand testing data. We have already discussed one way to describe an assumption about such aregularity,whichisbychoosingalimitedclassofpossiblehypotheses. Anotherwayto dothisistoprovidesmootherguidance,sayingthat,withinahypothesisclass,weprefer"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "somehypothesestoothers. Theregularizerarticulatesthispreferenceandtheconstantλ sayshowmuchwearewillingtotradeofflossonthetrainingdataversuspreferenceover hypotheses. Thistrade-offisillustratedinthefigurebelow.Hypothesish has0trainingloss,butis 1 verycomplicated. Hypothesish mis-classifiestwopoints,butisverysimple. Inabsence 2 ofotherbeliefsaboutthesolution,itisoftenbettertopreferthatthesolutionbe“simpler,” andsowemightpreferh overh ,expectingittoperformbetteronfutureexamplesdrawn 2 1"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "fromthissamedistribution. Anothernicewayofthinkingaboutregularizationisthatwe Toestablishsomevo- wouldliketopreventourhypothesisfrombeingtoodependentontheparticulartraining cabulary,wesaythath 1 isoverfittothetraining datathatweweregiven: wewouldlikeforittobethecasethatifthetrainingdatawere data. changedslightly,thehypothesiswouldnotchangebymuch. h 1 h 2 Acommonstrategyforspecifyingaregularizeristousetheform 2 R(Θ)= Θ−Θ prior (cid:13) (cid:13) when we have some idea in advance that(cid:13)θ"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "ought t(cid:13)o be near some value Θ prior . In the LearnaboutBayesian absenceofsuchknowledgeadefaultistoregularizetowardzero: methodsinmachine learningtoseethethe- R(Θ)= Θ 2 . orybehindthisandcool k k results! LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 29 3 A new hypothesis class: linear logistic classifiers Forclassification,itisnaturaltomakepredictionsin{+1,−1}andusethe0−1lossfunction. However, even for simple linear classifiers, it is very difficult to find values for θ,θ that 0"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "minimizesimpletrainingerror n 1 J(θ,θ )= L(sign(θTx(i)+θ ),y(i)) . 0 n 0 X i=1 ThisproblemisNP-hard,whichprobablyimpliesthatsolvingthemostdifficultinstances The“probably”hereis ofthisproblemwouldrequirecomputationtimeexponentialinthenumberoftrainingex- notbecausewe’retoo lazytolookitup,but amples,n. actuallybecauseofa Whatmakesthisadifficultoptimizationproblemisitslackof“smoothness”: fundamentalunsolved problemincomputer- There can be two hypotheses, (θ,θ ) and (θ ,θ ), where one is closer in"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "parameter • 0 0 00 sciencetheory,known space to the optimal parameter values (θ ∗ ,θ ∗0 ), but they make the same number of as“PvsNP.” misclassificationssotheyhavethesameJvalue. Allpredictionsarecategorical: theclassifiercan’texpressadegreeofcertaintyabout • whetheraparticularinputxshouldhaveanassociatedvaluey. Forthesereasons,ifweareconsideringahypothesisθ,θ thatmakesfiveincorrectpredic- 0 tions, itisdifficulttoseehowwemightchangeθ,θ sothatitwillperformbetter, which 0"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "makesitdifficulttodesignanalgorithmthatsearchesthroughthespaceofhypothesesfor agoodone. For these reasons, we are going to investigate a new hypothesis class: linear logistic classifiers. These hypotheses are still parameterized by a d-dimensional vector θ and a scalarθ ,butinsteadofmakingpredictionsin{+1,−1},theygeneratereal-valuedoutputs 0 intheinterval(0,1). Alinearlogisticclassifierhastheform h(x;θ,θ )=σ(θTx+θ ) . 0 0 Thislooksfamiliar! What’snew?"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "Thelogisticfunction,alsoknownasthesigmoidfunction,isdefinedas 1 σ(z)= , 1+e−z andplottedbelow,asafunctionofitsinputz.Itsoutputcanbeinterpretedasaprobability, becauseforanyvalueofztheoutputisin(0,1). σ(z) 1 0.5 z −4 −3 −2 −1 1 2 3 4 Study Question: Convince yourself the output of σ is always in the interval (0,1). Why can’t it equal 0 or equal 1? For what value of z does σ(z)=0.5? What does a linear logistic classifier (LLC) look like? Let’s consider the simple case whered ="
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "1,soourinputpointssimplyliealongthexaxis. TheplotbelowshowsLLCs forthreedifferentparametersettings: σ(10x+1),σ(−2x+1),andσ(2x−3). LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 30 σ(θTx+θ ) 0 1 0.5 x −4 −3 −2 −1 1 2 3 4 Study Question: Which plot is which? What governs the steepness of the curve? What governs the x value where the output is equal to 0.5? Butwait! Rememberthatthedefinitionofaclassifierfromchapter2isthatit’samap- ping from R d {−1,+1} or to some other discrete set. So, then, it"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "seems like an LLC is → actuallynotaclassifier! Given an LLC, with an output value in (0,1), what should we do if we are forced to makeapredictionin{+1,−1}? Adefaultansweristopredict+1ifσ(θTx+θ ) > 0.5and 0 −1otherwise. Thevalue0.5issometimescalledapredictionthreshold. In fact, for different problem settings, we might prefer to pick a different prediction threshold. Thefieldofdecisiontheoryconsidershowtomakethischoicefromtheperspec- tive of Bayesian reasoning. For example, if the consequences of"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "predicting +1 when the answer should be −1 are much worse than the consequences of predicting −1 when the answershouldbe+1,thenwemightsetthepredictionthresholdtobegreaterthan0.5. Study Question: Using a prediction threshold of 0.5, for what values of x do each of the LLCs shown in the figure above predict +1? Whend=2,thenourinputsxlieinatwo-dimensionalspacewithaxesx andx ,and 1 2 theoutputoftheLLCisasurface,asshownbelow,forθ=(1,1),θ =2. 0 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 31 Study"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "Question: Convince yourself that the set of points for which σ(θTx+θ ) = 0 0.5, that is, the separator between positive and negative predictions with prediction threshold 0.5 is a line in (x ,x ) space. What particular line is it for the case in the 1 2 figure above? How would the plot change for θ = (1,1), but now with θ = −2? For 0 θ=(−1,−1),θ =2? 0 4 Loss function for logistic classifiers Wehavedefinedaclass,LLC,ofhypotheseswhoseoutputsarein(0,1),butwehavetrain- ing data with y values in"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "{+1,−1}. How can we define a loss function? Intuitively, we wouldliketohavelowlossifweassignalowprobabilitytotheincorrectclass. We’lldefinea lossfunction,callednegativelog-likelihood(NLL),thatdoesjustthis. Inaddition,ithasthe coolpropertythatitextendsnicelytothecasewherewewouldliketoclassifyourinputs intomorethantwoclasses. Inordertosimplifythedescription,wewillassumethat(ortransformsothat)thelabels inthetrainingdataarey {0,1},enablingthemtobeinterpretedasprobabilitiesofbeing ∈"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "amemberoftheclassofinterest. Wewouldliketopicktheparametersofourclassifierto Remembertobesure maximizetheprobabilityassignedbytheLCCtothecorrectyvalues, asspecifiedinthe youryvalueshavethis trainingset. Lettingguessg(i) =σ(θTx(i)+θ ),thatprobabilityis formifyoutrytolearn 0 anLLCusingNLL!| n g(i) ify(i) =1 , (cid:14)1−g(i) otherwise Y i=1 undertheassumptionthatourpredictionsareindependent.Thiscanbecleverlyrewritten, wheny(i) {0,1},as ∈ n g(i)y(i) (1−g(i))1−y(i) . Y i=1 Study Question: Be sure"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "you can see why these two expressions are the same. Now, because products are kind of hard to deal with, and because the log function is monotonic,theθ,θ thatmaximizethelogofthisquantitywillbethesameastheθ,θ that 0 0 maximizetheoriginal,sowecantrytomaximize n y(i)logg(i)+(1−y(i))log(1−g(i)) . X i=1(cid:16) (cid:17) Wecanturnthemaximizationproblemaboveintoaminimizationproblembytakingthe negativeoftheaboveexpression,andwriteintermsofminimizingaloss n L (g(i),y(i)) nll X i=1 whereL"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "isthenegativelog-likelihoodlossfunction: nll L (guess,actual)=−(actual log(guess)+(1−actual) log(1−guess)) . nll · · Thislossfunctionisalsosometimesreferredtoastheloglossorcrossentropy. Youcanuseanybase forthelogarithmand itwon’tmakeanyreal difference. Ifweask youfornumbers,use logbasee. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 32 5 Logistic classification as optimization Wecanfinallyputallthesepiecestogetheranddevelopanobjectivefunctionforoptimiz-"
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "ingregularizednegativelog-likelihoodforalinearlogisticclassifier. Infact,thisprocessis That’salotoffancy usuallycalled“logisticregression,”sowe’llcallourobjectiveJ ,anddefineitas words! lr n 1 J (θ,θ ;D)= L (σ(θTx(i)+θ ),y(i)) +λ θ 2 . lr 0 n nll 0 ! k k X i=1 Study Question: Consider the case of linearly separable data. What will the θ values that optimize this objective be like if λ = 0? What will they be like if λ is very big? Try to work out an example in one dimension with two data points."
  },
  {
    "file": "mit_ocw_ch_5.pdf",
    "chunk": "LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "6 CHAPTER Gradient Descent In the previous chapter, we showed how to describe an interesting objective function for machinelearning,butweneedawaytofindtheoptimalΘ = argmin J(Θ). Thereisan ∗ Θ enormous,fascinating,literatureonthemathematicalandalgorithmicfoundationsofop- timization,butforthisclass,wewillconsideroneofthesimplestmethods,calledgradient Whichyoushouldcon- descent. siderstudyingsome day! Intuitively,inoneortwodimensions,wecaneasilythinkofJ(Θ)asdefiningasurface over Θ; that same idea"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "extends to higher dimensions. Now, our objective is to find the Θ value at the lowest point on that surface. One way to think about gradient descent is that you start at some arbitrary point on the surface, look to see in which direction the “hill”goesdownmoststeeply,takeasmallstepinthatdirection,determinethedirection ofsteepestdescentfromwhereyouare,takeanothersmallstep,etc. Here’saveryold-school humorousdescription ofgradientdescentand 1 One dimension otheroptimizationalgo-"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "rithmsusinganalogies involvingkangaroos: We will start by considering gradient descent in one dimension. Assume Θ R, and ftp://ftp.sas.com/ ∈ that we know both J(Θ) and its first derivative with respect to Θ, J 0 (Θ). Here is pseudo- pub/neural/kangaroos.txt codeforgradientdescentonanarbitraryfunctionf. Alongwithfanditsgradientf ,we 0 have to specify the initial value for parameter Θ, a step-size parameter η, and an accuracy parameter(cid:15): 1D-GRADIENT-DESCENT(Θ init ,η,f,f 0 ,(cid:15)) 1"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Θ(0) = Θ init 2 t = 0 3 repeat 4 t = t+1 5 Θ(t) =Θ(t−1)−ηf (Θ(t−1)) 0 6 until|f(Θ(t))−f(Θ(t−1))|<(cid:15) 7 returnΘ(t) Notethatthisalgorithmterminateswhenthechangeinthefunctionfissufficientlysmall. Therearemanyotherreasonablewaystodecidetoterminate.Theseincludethefollowing. StopafterafixednumberofiterationsT,i.e.whent=T. • 33MIT6.036 Fall2019 34 StopwhenthechangeinthevalueoftheparameterΘissufficientlysmall,i.e.when • Θ(t)−Θ(t−1) <(cid:15). S(cid:12)top when the(cid:12) derivative f at the"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "latest value of Θ is sufficiently small, i.e. when (cid:12) (cid:12) 0 • f (Θ(t)) <(cid:15). 0 (cid:12) (cid:12) Stud (cid:12) y Questi (cid:12) on: In the list of possible stopping criteria for 1D-GRADIENT-DESCENT above, how do the final two potential criteria relate to each other? Theorem1.1. IfJis convex,foranydesiredaccuracy(cid:15),thereissomestepsizeηsuchthatgradient Afunctionisconvex descentwillconvergetowithin(cid:15)oftheoptimalΘ. ifthelinesegmentbe- tweenanytwopoints"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "However,wemustbecarefulwhenchoosingthestepsizetopreventslowconvergence, onthegraphofthe functionliesaboveor oscillationaroundtheminimum,ordivergence. onthegraph. Thefollowingplotillustratesaconvexfunctionf(x)=(x−2)2,startinggradientdescent atθ =4.0withastep-sizeof1/2. Itisverywell-behaved! init y 4 2 x −1 1 2 3 4 5 6 Study Question: What happens in this example with very small η? With very big η? If J is non-convex, where gradient descent converges to depends on θ . When it init"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "reachesavalueofθwheref (θ)=0andf (θ)>0,butitisnotaminimumofthefunction, 0 00 itiscalledalocalminimumorlocaloptimum. Theplotbelowshowstwodifferentθ ,and init twodifferentresultinglocaloptima. 10 y 8 6 4 x −2 −1 1 2 3 4 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 35 2 Multiple dimensions Theextensiontothecaseofmulti-dimensionalΘisstraightforward.Let’sassumeΘ R m, ∈ soJ:R m R. ThegradientofJwithrespecttoΘis → ∂J/∂Θ 1 . J= . Θ  .  ∇ ∂J/∂Θ  m  "
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Thealgorithmremainsthesame,exceptthattheupdatestepinline5becomes Θ(t) =Θ(t−1)−η J(Θ(t−1)) Θ ∇ andwehavetochangetheterminationcriterion.Theeasiestthingistokeepthetestinline 6as f(Θ(t))−f(Θ(t−1)) <(cid:15),whichissensiblenomatterthedimensionalityofΘ. (cid:12) (cid:12) (cid:12) (cid:12) 3 Application to logistic regression objective Wecannowsolvetheoptimizationproblemforourlinearlogisticclassifierasformulated inchapter5. Webeginbystatingtheobjectiveandthegradientnecessaryfordoinggradi- entdescent."
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Inourproblemwhereweareconsideringlinearseparators,theentireparam- etervectorisdescribedbyparametervectorθandscalarθ andsowewillhavetoadjust 0 them both and compute gradients of J with respect to each of them. The objective and gradient (note that we have replaced the constant λ with λ for convenience), are, letting Thefollowingstepre- 2 g(i) =σ(θTx(i)+θ ) quirespassingfamiliar- 0 itywithmatrixderiva- n tives. Afoolproofway 1 λ J (θ,θ )= L (g(i),y(i))+ θ 2 ofcomputingthemisto lr 0 n nll 2k k"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "computepartialderiva- X i=1 tiveofJwithrespectto n 1 eachcomponentθ ofθ. J (θ,θ )= g(i)−y(i) x(i)+λθ i ∇ θ lr 0 n X i=1(cid:16) (cid:17) n ∂J (θ,θ ) 1 lr 0 = g(i)−y(i) . ∂θ n 0 X i=1(cid:16) (cid:17) Notethat Jwillbeofshaped 1and ∂J willbeascalarsincewehaveseparatedθ ∇ θ × ∂θ0 0 fromθhere. Study Question: Convince yourself that the dimensions of all these quantities are correct, under the assumption that θ is d 1. How does d relate to m as discussed × for Θ in the previous section? Study"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Question: Compute θ 2 by finding the vector of partial derivatives θ ∇ k k (∂ θ 2/∂θ ,...,∂ θ 2/∂θ ). What is the shape of θ 2? k k 1 k k d ∇ θ k k Study Question: Compute L (σ(θTx + θ ),y) by finding the vector of partial ∇ θ nll 0 derivatives (∂L (σ(θTx+θ ),y)/∂θ ,...,∂L (σ(θTx+θ ),y)/∂θ ). nll 0 1 nll 0 d Study Question: Use these last two results to verify our derivation above. Puttingeverythingtogether,ourgradientdescentalgorithmforlogisticregressionbe- comes LastUpdated:"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "12/18/1911:56:05MIT6.036 Fall2019 36 LR-GRADIENT-DESCENT(θ init ,θ 0init ,η,(cid:15)) 1 θ(0) = θ init 2 θ(0) = θ 0 0init 3 t = 0 4 repeat 5 t = t+1 6 θ(t) =θ(t−1)−η 1 n σ θ(t−1)T x(i)+θ(t−1) −y(i) x(i)+λθ(t−1) n i=1 0 7 θ(t) =θ(t−1)−η(cid:16)1 P n (cid:16)σ(cid:16)θ(t−1)T x(i)+θ(t−1)(cid:17)−y(i)(cid:17) (cid:17) 0 0 n i=1 0 8 until J lr (θ(t),θ( 0 t))−J(cid:16) lr (θP(t−1),(cid:16)θ( 0 t(cid:16)−1)) <(cid:15) (cid:17) (cid:17)(cid:17) 9 return(cid:12)θ(t),θ(t) (cid:12) (cid:12) 0 (cid:12)"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "(cid:12) (cid:12) Study Question: Is it okay that λ doesn’t appear in line 7? 4 Stochastic Gradient Descent Whentheformofthegradientisasum,ratherthantakeonebig(ish)stepinthedirection of the gradient, we can, instead, randomly select one term of the sum, and take a very Theword“stochastic” smallstepinthatdirection. Thisseemssortofcrazy,butrememberthatallthelittlesteps meansprobabilistic, orrandom;sodoes wouldaverageouttothesamedirectionasthebigstepifyouweretostayinoneplace.Of"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "“aleatoric,”whichisa course,you’renotstayinginthatplace,soyoumove,inexpectation,inthedirectionofthe verycoolword. Look gradient. upaleatoricmusic, Mostobjectivefunctionsinmachinelearningcanendupbeingwrittenasasumover sometime. datapoints,inwhichcase,stochasticgradientdescent(SGD)isimplementedbypickinga datapointrandomlyoutofthedataset,computingthegradientasiftherewereonlythat onepointinthedataset,andtakingasmallstepinthenegativedirection. Let’sassumeourobjectivehastheform n f(Θ)= f (Θ) . i X"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "i=1 Here is pseudocode for applying SGD to an objective f; it assumes we know the form of f foralliin1...n: Θ i ∇ STOCHASTIC-GRADIENT-DESCENT(Θ init ,η,f, ∇ Θ f 1 ,..., ∇ Θ f n ,T) 1 Θ(0) = Θ init 2 fort = 1toT 3 randomlyselecti {1,2,...,n} ∈ 4 Θ(t) =Θ(t−1)−η(t) f (Θ(t−1)) Θ i ∇ 5 returnΘ(t) Note that now instead of a fixed value of η, η is indexed by the iteration of the algo- rithm,t.ChoosingagoodstoppingcriterioncanbealittletrickierforSGDthantraditional gradientdescent."
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Herewe’vejustchosentostopafterafixednumberofiterationsT. ForSGDtoconvergetoalocaloptimumastincreases,thestepsizehastodecreaseasa functionoftime. Thenextresultshowsonestepsizesequencethatworks. Theorem4.1. IfJisconvex,andη(t)isasequencesatisfying η(t)= and η(t)2 < , ∞ ∞ X t=1 X t=1 ∞ ∞ LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 37 thenSGDconvergeswithprobabilityonetotheoptimalΘ. Wehaveleftoutsome gnarlyconditionsinthis One“legal”wayofsettingthestepsizeistomakeη(t)=1/tbutpeopleoftenuserules"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "theorem. Also,youcan thatdecreasemoreslowly,andsodon’tstrictlysatisfythecriteriaforconvergence. learnmoreaboutthe subtledifferencebe- Study Question: If you start a long way from the optimum, would making η(t) de- tween“withprobabil- ityone”and“always” crease more slowly tend to make you move more quickly or more slowly to the opti- bytakinganadvanced mum? probabilitycourse. TherearemultipleintuitionsforwhySGDmightbeabetterchoicealgorithmicallythan regularGD(whichissometimescalledbatchGD(BGD)):"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "Ifyourfisactuallynon-convex,buthasmanyshallowlocaloptimathatmighttrap • BGD, then taking samples from the gradient at some point Θ might “bounce” you aroundthelandscapeandoutofthelocaloptima. Sometimes, optimizing f really well is not what we want to do, because it might • overfit the training set; so, in fact, although SGD might not get lower training error thanBGD,itmightresultinlowertesterror. BGDtypicallyrequirescomputingsomequantityovereverydatapointinadataset. • SGD may perform well after"
  },
  {
    "file": "mit_ocw_ch_6.pdf",
    "chunk": "visiting only some of the data. This behavior can be usefulforverylargedatasets–inruntimeandmemorysavings. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "7 CHAPTER Regression Nowwewillturntoaslightlydifferentformofmachine-learningproblem, calledregres- sion. Itisstillsupervisedlearning,soourdatawillstillhavetheform “Regression,”incom- monparlance,means S = x(1),y(1) ,..., x(n),y(n) . movingbackwards. But n thisisforwardprogress! (cid:10)(cid:16) (cid:17) (cid:16) (cid:17)(cid:11) But now, instead of the y values being discrete, they will be real-valued, and so our hy- potheseswillhavetheform h:R d R . →"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "Thisisagoodframeworkwhenwewanttopredictanumericalquantity,likeheight,stock value,etc.,ratherthantodividetheinputsintocategories. Thefirststepistopickalossfunction,todescribehowtoevaluatethequalityofthepre- dictionsourhypothesisismaking,whencomparedtothe“target”yvaluesinthedataset. Thechoiceoflossfunctionispartofmodelingyourdomain. Intheabsenceofadditional informationaboutaregressionproblem,wetypicallyusesquarederror(SE): Loss(guess,actual)=(guess−actual)2 ."
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "Itpenalizesguessesthataretoohighthesameamountasitpenalizesguessesthataretoo low, and has a good mathematical justification in the case that your data are generated fromanunderlyinglinearhypothesis,butwithGaussian-distributednoiseaddedtothey values. Wewillconsiderthecaseofalinearhypothesisclass, h(x;θ,θ )=θTx+θ , 0 0 remembering that we can get a rich class of hypotheses by performing a non-linear fea- turetransformationbeforedoingtheregression. So,θTx+θ isalinearfunctionofx,but 0 θTϕ(x)+θ"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "isanon-linearfunctionofxifϕisanon-linearfunctionofx. 0 Wewilltreatregressionasanoptimizationproblem, inwhich, givenadatasetD, we wishtofindalinearhypothesisthatminimizesmeansquarederror. Ourobjective, often calledmeansquarederror,istofindvaluesforΘ=(θ,θ )thatminimize 0 n 1 2 J(θ,θ )= θTx(i)+θ −y(i) , 0 n 0 X i=1(cid:16) (cid:17) 38MIT6.036 Fall2019 39 resultinginthesolution: θ ∗,θ ∗0 =argminJ(θ,θ 0 ) . (7.1) θ,θ0 1 Analytical solution: ordinary least squares"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "Oneveryinterestingaspectoftheproblemfindingalinearhypothesisthatminimizesmean squarederror(thisgeneralproblemisoftencalledordinaryleastsquares(OLS))isthatwecan findaclosed-formformulafortheanswer! Whatdoes“closed Everythingiseasiertodealwithifweassumethatthex(i) havebeenaugmentedwith form”mean? Generally, thatitinvolvesdirect an extra input dimension (feature) that always has value 1, so we may ignore θ . (See 0 evaluationofamathe- chapter3,section2forareminderaboutthisstrategy)."
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "maticalexpressionusing Wewillapproachthisjustlikeaminimizationproblemfromcalculushomework: take afixednumberof“typ- thederivativeofJwithrespecttoθ, setittozero, andsolveforθ. Thereisanadditional ical”operations(like arithmeticoperations, steprequired,tocheckthattheresultingθisaminimum(ratherthanamaximumoranin- trigfunctions,powers, flectionpoint)butwewon’tworkthroughthathere.Itispossibletoapproachthisproblem etc.). Soequation7.1is by: notinclosedform,be- causeit’snotatallclear Finding∂J/∂θ"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "forkin 1,...,d, whatoperationsone k • needstoperformtofind Constructingasetofkequationsoftheform∂J/∂θ =0,and thesolution. k • Solvingthesystemforvaluesofθ . Wewillusedherefor k • thetotalnumberoffea- turesineachx(i),in- That works just fine. To get practice for applying techniques like this to more complex cludingtheadded1. problems,wewillworkthroughamorecompact(andcool!) matrixview. Study Question: Work through this and check your answer against ours below."
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "WecanthinkofourtrainingdataintermsofmatricesXandY,whereeachcolumnofX isanexample,andeach“column”ofYisthecorrespondingtargetoutputvalue: x(1) ... x(n) 1 1 X= . . . ... . . .  Y = y(1) ... y(n) . x(1) ... x(n)  (cid:2) (cid:3)  d d    Study Question: What are the dimensions of X and Y? In most textbooks, they think of an individual example x(i) as a row, rather than a column.Sothatwegetananswerthatwillberecognizabletoyou,wearegoingtodefinea newmatrixandvector,W"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "andT,whicharejusttransposesofourXandY,andthenwork withthem: x(1) ... x(1) y(1) 1 d W =XT = . . . ... . . .  T =YT =  . . .  .   x( 1 n) ... x( d n)    y(n)      Study Question: What are the dimensions of W and T? Nowwecanwrite 2 n d 1 1 J(θ)= (Wθ−T)T(Wθ−T)= W θ −T n n  ij j  i  1 n n 1 X i=1 X j=1 × ×    | {z }| {z } LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 40 andusingfactsaboutmatrix/vectorcalculus,weget 2 J= WT (Wθ−T) . ∇ θ n d × n n × 1 Settingto0andsolving,weget:"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "|{z}| {z } 2 WT(Wθ−T)=0 n WTWθ−WTT =0 WTWθ=WTT θ=(WTW)−1WTT Andthedimensionsworkout! θ= WTW −1 WT T d n n 1 (cid:0) d d(cid:1) × × × |{z}|{z} So, given our data, we can directly c|omp{zute t}he linear regression that minimizes mean squarederror. That’sprettyawesome! 2 Regularizing linear regression Well, actually, there are some kinds of trouble we can get into. What if WTW is not invertible? (cid:0) (cid:1) Study Question: Consider, for example, a situation where the data-set is just the same"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "point repeated twice: x(1) = x(2) = (1,2)T. What is W in this case? What is WTW? What is (WTW)−1? Another kind of problem is overfitting: we have formulated an objective that is just about fitting the data as well as possible, but as we discussed in the context of margin maximization, we might also want to regularize to keep the hypothesis from getting too attachedtothedata. Weaddressboththeproblemofnotbeingabletoinvert(WTW)−1 andtheproblemof overfittingusingamechanismcalledridgeregression."
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "Weaddaregularizationterm θ 2to k k theOLSobjective,withtrade-offparameterλ. Study Question: When we add a regularizer of the form θ 2, what is our most k k “preferred” value of θ, in the absence of any data? Hereistheridgeregressionobjectivefunction: n 1 2 J (θ,θ )= θTx(i)+θ −y(i) +λ θ 2 ridge 0 n 0 k k X i=1(cid:16) (cid:17) Larger λ values pressure θ values to be near zero. Note that we don’t penalize θ ; intu- 0 itively, θ is what “floats” the regression surface to the right level for the"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "data you have, 0 andsoyoushouldn’tmakeithardertofitadatasetwheretheyvaluestendtobearound onemillionthanonewheretheytendtobearoundone. Theotherparameterscontrolthe orientationoftheregressionsurface,andwepreferittohaveanot-too-crazyorientation. Thereisananalyticalexpressionfortheθ,θ valuesthatminimizeJ ,butit’salittle 0 ridge bitmorecomplicatedtoderivethanthesolutionforOLSbecauseθ 0 needsspecialtreatment. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 41 Ifwedecidenottotreatθ"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "specially(soweadda1featuretoourinputvectors),thenwe 0 get: 2 J = WT(Wθ−T)+2λθ . ∇ θ ridge n Settingto0andsolving,weget: 2 WT(Wθ−T)+2λθ=0 n 1 1 WTWθ− WTT +λθ=0 n n 1 1 WTWθ+λθ= WTT n n WTWθ+nλθ=WTT (WTW+nλI)θ=WTT θ=(WTW+nλI)−1WTT Whew! So, θ = WTW+nλI −1 WTT ridge whichbecomesinvertiblewhenλ>0.(cid:0) (cid:1) Thisiscalled“ridge” regressionbecausewe Study Question: Derive this version of the ridge regression solution. areaddinga“ridge” ofλvaluesalongthe diagonalofthematrix Talking about"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "regularization In machine learning in general, not just regression, it is beforeinvertingit. usefultodistinguishtwowaysinwhichahypothesish Hmightcontributetoerrorson ∈ testdata. Wehave Structuralerror: Thisiserrorthatarisesbecausethereisnohypothesish H thatwill ∈ performwellonthedata,forexamplebecausethedatawasreallygeneratedbyasin wavebutwearetryingtofititwithaline. Estimation error: This is error that arises because we do not have enough data (or the"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "dataareinsomewayunhelpful)toallowustochooseagoodh H. ∈ Therearetechnicaldefi- Whenweincreaseλ,wetendtoincreasestructuralerrorbutdecreaseestimationerror, nitionsoftheseconcepts thatarestudiedinmore andviceversa. advancedtreatments Study Question: Consider using a polynomial basis of order k as a feature trans- ofmachinelearning. formation φ on your data. Would increasing k tend to increase or decrease structural Structuralerrorisre- ferredtoasbiasand error? What about estimation error?"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "estimationerrorisre- ferredtoasvariance. 3 Optimization via gradient descent Inverting the d d matrix WTW takes O(d3) time, which makes the analytic solution Well,actually,Gauss- × impractical for large d. If we have high-dimensional data, we can fall back on gradient Jordanelimination, (cid:0) (cid:1) apopularalgorithm, descent. takesO(d3)arithmetic Study Question: Why is having large n not as much of a computational problem as operations,butthebit having large d? complexityofthein-"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "termediateresultscan Recalltheridgeobjective growexponentially! Thereareotheralgo- n rithmswithpolynomial 1 2 J (θ,θ )= θTx(i)+θ −y(i) +λ θ 2 bitcomplexity. (Ifthis ridge 0 n 0 k k justmadenosenseto X i=1(cid:16) (cid:17) you,don’tworry.) LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 42 anditsgradientwithrespecttoθ n 2 J= θTx(i)+θ −y(i) x(i)+2λθ ∇ θ n 0 X i=1(cid:16) (cid:17) andpartialderivativewithrespecttoθ 0 n ∂J 2 = θTx(i)+θ −y(i) . ∂θ n 0 0 X i=1(cid:16) (cid:17)"
  },
  {
    "file": "mit_ocw_ch_7.pdf",
    "chunk": "Armedwiththesederivatives,wecandogradientdescent,usingtheregularorstochastic gradientmethodsfromchapter6. Even better, the objective functions for OLS and ridge regression are convex, which means they have only one minimum, which means, with a small enough step size, gra- dientdescentisguaranteedtofindtheoptimum. LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "8 CHAPTER Neural Networks Unlessyouliveunderarockwithnointernetaccess,you’vebeenhearingalotabout“neu- ral networks.” Now that we have several useful machine-learning concepts (hypothesis classes,classification,regression,gradientdescent,regularization,etc.) wearecompletely wellequippedtounderstandneuralnetworksindetail. This, in some sense, the “third wave” of neural nets. The basic idea is founded on the 1943 model of neurons of McCulloch and Pitts and learning ideas of Hebb. There was a great"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "deal of excitement, but not a lot of practical success: there were good train- ingmethods(e.g.,perceptron)forlinearfunctions,andinterestingexamplesofnon-linear functions,butnogoodwaytotrainnon-linearfunctionsfromdata. Interestdiedoutfora while, but was re-kindled in the 1980s when several people came up with a way to train Aswithmanygood neuralnetworkswith“back-propagation,”whichisaparticularstyleofimplementinggra- ideasinscience,the basicideaforhowto dientdescent,whichwewillstudyhere."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Bythemid-90s,theenthusiasmwanedagain,be- trainnon-linearneural cause although we could train non-linear networks, the training tended to be slow and networkswithgradi- was plagued by a problem of getting stuck in local optima. Support vector machines entdescent,wasinde- (SVMs) (regularization of high-dimensional hypotheses by seeking to maximize the mar- pendentlydeveloped bymorethanonere- gin)andkernelmethods(anefficientandbeautifulwayofusingfeaturetransformations searcher."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "tonon-linearlytransformdataintoahigher-dimensionalspace)providedreliablelearning methodswithguaranteedconvergenceandnolocaloptima. However, during the SVM enthusiasm, several groups kept working on neural net- works,andtheirwork,incombinationwithanincreaseinavailabledataandcomputation, has made them rise again. They have become much more reliable and capable, and are nowthemethodofchoiceinmanyapplications. Therearemany,manyvariationsofneu- Thenumberincreases"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "ralnetworks,whichwecan’tevenbegintosurvey. Wewillstudythecore“feed-forward” daily,asmaybeseenon arxiv.org. networks with “back-propagation” training, and then, in later chapters, address some of themajoradvancesbeyondthiscore. Wecanviewneuralnetworksfromseveraldifferentperspectives: View 1: An application of stochastic gradient descent for classification and regression withapotentiallyveryrichhypothesisclass. View 2: A brain-inspired network of neuron-like computing elements that learn dis-"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "tributedrepresentations. View3:Amethodforbuildingapplicationsthatmakepredictionsbasedonhugeamounts 43MIT6.036 Fall2019 44 ofdatainverycomplexdomains. Wewillmostlytakeview1,withtheunderstandingthatthetechniqueswedevelopwill enabletheapplicationsinview3.View2wasamajormotivationfortheearlydevelopment ofneuralnetworks, butthetechniqueswewillstudydonotseemtoactuallyaccountfor Someprominentre- thebiologicallearningprocessesinbrains. searchersare,infact, workinghardtofind analoguesofthese"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "methodsinthebrain 1 Basic element Thebasicelementofaneuralnetworkisa“neuron,”picturedschematicallybelow.Wewill alsosometimesrefertoaneuronasa“unit”or“node.” x pre-activation 1 output w 1 . . z f( ) a . · w x mP m w 0 activationfunction input Itisanon-linearfunctionofaninputvectorx R mtoasingleoutputvaluea R.Itis Sorryforchangingour ∈ ∈ parameterizedbyavectorofweights(w 1 ,...,w m ) R m andanoffsetorthresholdw 0 R. notationhere. Wewere ∈ ∈ usingdasthedimen- Inorderfortheneuron tobenon-linear,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "wealsospecifyanactivationfunctionf : R R, → sionoftheinput,but which can be the identity (f(x) = x), but can also be any other function, though we will wearetryingtobecon- onlybeabletoworkwithitifitisdifferentiable. sistentherewithmany Thefunctionrepresentedbytheneuronisexpressedas: otheraccountsofneural networks. Itisimpossi- bletobeconsistentwith m a=f(z)=f x w +w =f(wTx+w ) . allofthemthough—  j j 0 0 therearemanydiffer- X j=1 entwaysoftellingthis   story. Before thinking about a whole"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "network, we can consider how to train a single unit. GivenalossfunctionL(guess,actual)andadataset{(x(1),y(1)),...,(x(n),y(n))},wecando Thisshouldremindyou (stochastic)gradientdescent,adjustingtheweightsw,w tominimize ofourθandθ forlin- 0 0 earmodels. J(w,w )= L NN(x(i);w,w ),y(i) . 0 0 X i (cid:16) (cid:17) whereNNistheoutputofourneuralnetforagiveninput. Wehavealreadystudiedtwospecialcasesoftheneuron:linearlogisticclassifiers(LLC) with NLL loss and regressors with quadratic loss! The activation"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "function for the LLC is f(x)=σ(x)andforlinearregressionitissimplyf(x)=x. Study Question: Just for a single neuron, imagine for some reason, that we decide to use activation function f(z) = ez and loss function L(g,a) = (g−a)2. Derive a gradient descent update for w and w . 0 2 Networks Now, we’ll put multiple neurons together into a network. A neural network in general takesinaninputx R mandgeneratesanoutputa R n.Itisconstructedoutofmultiple ∈ ∈ LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 45"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "neurons;theinputsofeachneuronmightbeelementsofxand/oroutputsofotherneurons. Theoutputsaregeneratedbynoutputunits. Inthischapter,wewillonlyconsiderfeed-forwardnetworks.Inafeed-forwardnetwork, you can think of the network as defining a function-call graph that is acyclic: that is, the input to a neuron can never depend on that neuron’s output. Data flows, one way, from theinputstotheoutputs,andthefunctioncomputedbythenetworkisjustacomposition ofthefunctionscomputedbytheindividualneurons."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Althoughthegraphstructureofaneuralnetworkcanreallybeanything(aslongasit satisfies the feed-forward constraint), for simplicity in software and analysis, we usually organizethemintolayers. Alayerisagroupofneuronsthatareessentially“inparallel”: theirinputsareoutputsofneuronsinthepreviouslayer,andtheiroutputsaretheinputto theneuronsinthenextlayer. We’llstartbydescribingasinglelayer,andthengoontothe caseofmultiplelayers. 2.1 Singlelayer"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Alayerisasetofunitsthat,aswehavejustdescribed,arenotconnectedtoeachother.The layeriscalledfullyconnectedif,asinthediagrambelow,theinputstoeachunitinthelayer are the same (i.e. x 1 ,x 2 ,...x m in this case). A layer has input x R m and output (also ∈ knownasactivation)a R n. ∈ f a 1 P x 1 f a 2 P x 2 f a 3 . . . P . . . x . . . m . . . W,W 0 f a n P Sinceeachunithasavectorofweightsandasingleoffset,wecanthinkoftheweightsof the whole layer as a matrix, W, and the collection of all the offsets as"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "a vector W . If we 0 haveminputs,nunits,andnoutputs,then Wisanm nmatrix, • × W isann 1columnvector, 0 • × X,theinput,isanm 1columnvector, • × Z=WTX+W ,thepre-activation,isann 1columnvector, 0 • × A,theactivation,isann 1columnvector, • × andtheoutputvectoris A=f(Z)=f(WTX+W ) . 0 Theactivationfunctionfisappliedelement-wisetothepre-activationvaluesZ. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 46 What can we do with a single layer? We have already seen single-layer networks, in the form of"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "linear separators and linear regressors. All we can do with a single layer is makealinearhypothesis. Thewholereasonformovingtoneuralnetworksistomovein Wehaveusedastep thedirectionofnon-linearhypotheses.Todothis,wewillhavetoconsidermultiplelayers, orsigmoidfunctionto transformthelinear wherewecanviewthelastlayerasstillbeingalinearclassifierorregressor,butwherewe outputvalueforclas- interpret the previous layers as learning a non-linear feature transformation φ(x), rather sification,butit’simpor-"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "thanhavingushand-specifyit. tanttobeclearthatthe resultingseparatorisstill linear. 2.2 Manylayers Asingleneuralnetworkgenerallycombinesmultiplelayers,mosttypicallybyfeedingthe outputsofonelayerintotheinputsofanotherlayer. Wehavetostartbyestablishingsomenomenclature.Wewilluseltonamealayer,and letmlbethenumberofinputstothelayerandnlbethenumberofoutputsfromthelayer. Then, Wl and Wl are of shape ml nl and nl 1, respectively. Let fl be the activation 0 × × functionoflayerl."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Then,thepre-activationoutputsarethenl 1vector Itistechnicallypossi- × bletohavedifferent Zl =WlT Al−1+Wl activationfunctions 0 withinthesamelayer, andtheactivationoutputsaresimplythenl 1vector but,again,forconve- × nienceinspecification Al =fl(Zl) . andimplementation, wegenerallyhavethe sameactivationfunction Here’sadiagramofamany-layerednetwork,withtwoblocksforeachlayer,onerep- withinalayer. resenting the linear part of the operation and one representing the non-linear activation function. We"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "will usethis structuraldecompositionto organizeouralgorithmic thinking andimplementation. X=A0 W1 Z1 A1 W2 Z2 A2 AL−1 WL ZL AL f1 f2 fL W1 W2 ··· WL 0 0 0 layer1 layer2 layerL 3 Choices of activation function Therearemanypossiblechoicesfortheactivationfunction.Wewillstartbythinkingabout whetherit’sreallynecessarytohaveanfatall. Whathappensifweletfbetheidentity? Then,inanetworkwithLlayers(we’llleave outW forsimplicity,butkeepingitwouldn’tchangetheformofthisargument), 0 AL =WLT AL−1 =WLT WL−1T"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "W1T X . ··· So,multiplyingouttheweightmatrices,wefindthat AL =WtotalX , whichisalinearfunctionofX! Havingallthoselayersdidnotchangetherepresentational capacityofthenetwork: thenon-linearityoftheactivationfunctioniscrucial. Study Question: Convince yourself that any function representable by any number of linear layers (where f is the identity function) can be represented by a single layer. Nowthatweareconvincedweneedanon-linearactivation,let’sexamineafewcom- monchoices. LastUpdated:"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "12/18/1911:56:05MIT6.036 Fall2019 47 Stepfunction 0 ifz<0 step(z)= (cid:14)1 otherwise Rectifiedlinearunit 0 ifz<0 ReLU(z)= =max(0,z) (cid:14)z otherwise Sigmoid function Also known as a logistic function, can be interpreted as probability, becauseforanyvalueofztheoutputisin[0,1] 1 σ(z)= 1+e−z HyperbolictangentAlwaysintherange[−1,1] ez−e−z tanh(z)= ez+e−z Softmax function Takes a whole vector Z R n and generates as output a vector A [0,1]n with the property that n A ∈ = 1, which means we can"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "interpret it as ∈ a i=1 i probabilitydistributionovernitems: P exp(z )/ exp(z ) 1 i i . softmax(z)= .  P.  exp(z )/ exp(z )  n i i    P 1.5 1.5 step(z) ReLU(z) 1 1 0.5 0.5 z z −2 −1 1 2 −2 −1 1 2 −0.5 −0.5 σ(z) tanh(z) 1 1 0.5 0.5 z z −4 −2 2 4 −4 −2 2 4 −0.5 −0.5 −1 −1 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 48 The original idea for neural networks involved using the step function as an activa- tion,butbecausethederivativeisdiscontinuous,wewon’tbeabletousegradient-descent"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "methodstotunetheweightsinanetworkwithstepfunctions,sowewon’tconsiderthem further. They have been replaced, in a sense, by the sigmoid, relu, and tanh activation functions. Study Question: Consider sigmoid, relu, and tanh activations. Which one is most like a step function? Is there an additional parameter you could add to a sigmoid that would make it be more like a step function? Study Question: What is the derivative of the relu function? Are there some values of the input for which the"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "derivative vanishes? ReLUsareespeciallycommonininternal(“hidden”)layers,andsigmoidactivationsare common forthe output for binaryclassification and softmaxfor multi-class classification (seesection4foranexplanation). 4 Error back-propagation We will train neural networks using gradient descent methods. It’s possible to use batch gradient descent, in which we sum up the gradient over all the points (as in section 2 of chapter6)orstochasticgradientdescent(SGD),inwhichwetakeasmallstepwithrespect"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "tothegradientconsideringasinglepointatatime(asinsection4ofchapter6). Ournotationisgoingtogetprettyhairyprettyquickly. Tokeepitassimpleaswecan, we’llfocusoncomputingthecontributionofonedatapointx(i)tothegradientoftheloss withrespecttotheweights, for SGD; youcansimplysumupthesegradientsoverallthe datapointsifyouwishtodobatchdescent. So,todoSGDforatrainingexample(x,y),weneedtocompute W Loss(NN(x;W),y), ∇ whereWrepresentsallweightsWl,Wlinallthelayersl=(1,...,L).Thisseemsterrifying, 0"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "butisactuallyquiteeasytodousingthechainrule. Rememberthechain Rememberthatwearealwayscomputingthegradientofthelossfunctionwithrespect rule! Ifa = f(b)and b=g(c)(sothat totheweightsforaparticularvalueof(x,y).Thattellsushowmuchwewanttochangethe a=f(g(c))),then weights,inordertoreducethelossincurredonthisparticulartrainingexample. da = da db = First,let’sseehowthelossdependsontheweightsinthefinallayer,WL.Remembering f dc (b)g (c d ) b · = dc 0 0"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "thatouroutputisAL,andusingtheshorthandlosstostandforLoss((NN(x;W),y)which f 0 (g(c))g 0 (c). isequaltoLoss(AL,y),andfinallythatAL = fL(ZL)andZL = WLT AL−1,wecanusethe chainrule: ∂loss ∂loss ∂AL ∂ZL = . ∂WL ∂AL · ∂ZL ·∂WL dependsonlossfunction fL0 AL−1 | {z } |{z} |{z} Itmightreason- Toactuallygetthedimensionstomatch,weneedtowritethisabitmorecarefully,and ablybotheryouthat notethatitistrueforanyl,includingl=L: ∂ZL/∂WL = AL−1. We’resomehowthink- ingaboutthederiva- ∂loss =Al−1 ∂loss T (8.1)"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "tiveofavectorwith ∂Wl ∂Zl respecttoamatrix, ml 1 (cid:18) (cid:19) whichseemslikeit ml × nl |{z × } 1 × nl mightneedtobea Yay!So,inordertofindthegr | a { d z ie } ntofthelo|ssw{zithr}especttotheweightsintheother t t h h r in ee g - . d B im ut en n s o i t o e n t a h l at layersofthenetwork,wejustneedtobeabletofind∂loss/∂Zl. ∂ZL/∂WL isreally (∂WLTAL−1)/∂WL and itseemsokayinatleast aninformalsensethat it’sAL−1. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 49"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Ifwerepeatedlyapplythechainrule,wegetthisexpressionforthegradientoftheloss withrespecttothepre-activationinthefirstlayer: ∂loss ∂loss ∂AL ∂ZL ∂AL−1 ∂A2 ∂Z2 ∂A1 = . (8.2) ∂Z1 ∂AL · ∂ZL · ∂AL−1 · ∂ZL−1 ····· ∂Z2 ·∂A1 ·∂Z1 ∂loss/∂Z2 | ∂{lzoss/∂A1 } This derivation was inf|ormal, to show you {thze general structure o}f the computation. In fact, togetthedimensionstoallworkout, wejusthavetowriteitbackwards! Let’sfirst understandmoreaboutthesequantities: ∂loss/∂ALisnL"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "1anddependsontheparticularlossfunctionyouareusing. • × ∂Zl/∂Al−1isml nlandisjustWl(youcanverifythisbycomputingasingleentry • × ∂Zl/∂Al−1). i j ∂Al/∂Zl isnl nl. It’salittletrickytothinkabout. Eachelemental = fl(zl). This • × i i meansthat∂al/∂zl =0wheneveri=j.So,theoff-diagonalelementsof∂Al/∂Zlare i j 6 all0,andthediagonalelementsare∂al/∂zl =fl0(zl). i j j Now,wecanrewriteequation8.2sothatthequantitiesmatchupas ∂loss ∂Al ∂Al+1 ∂AL−1 ∂AL ∂loss = Wl+1 ...WL−1 WL . (8.3) ∂Zl ∂Zl · · ∂Zl+1 · · ∂ZL−1"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "· · ∂ZL · AL Usingequation8.3tocompute∂loss/∂Zl combinedwithequation8.1,letsusfindthe gradientofthelosswithrespecttoanyoftheweightmatrices. Study Question: Apply the same reasoning to find the gradients of loss with respect to Wl. 0 Thisgeneralprocessiscallederrorback-propagation.Theideaisthatwefirstdoaforward pass to compute all the a and z values at all the layers, and finally the actual loss on this example. Then,wecanworkbackwardandcomputethegradientofthelosswithrespect"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "totheweightsineachlayer,startingatlayerLandgoingbacktolayer1. Iliketothinkofthisas “blamepropagation”. y Youcanthinkofloss ashowmadweare abouttheprediction X=A0 W1 Z1 A1 W2 Z2 A2 AL−1 WL ZL AL f1 f2 fL Loss thatthenetworkjust W 0 1 W 0 2 ··· W 0 L made. Then∂loss/∂AL ishowmuchweblame ∂loss ∂loss ∂loss ∂loss ∂loss ∂loss ∂loss AL fortheloss. Thelast ∂Z1 ∂A1 ∂Z2 ∂A2 ∂AL−1 ∂ZL ∂AL modulehastotakein ∂loss/∂AL andcom- If we view our neural network as a sequential composition of modules (in our work"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "pute∂loss/∂ZL,which so far, it has been an alternation between a linear transformation with a weight matrix, ishowmuchweblame andacomponent-wiseapplicationofanon-linearactivationfunction),thenwecandefine ZL fortheloss. The asimpleAPIforamodulethatwillletuscomputetheforwardandbackwardpasses, as nextmodule(work- ingbackwards)takes wellasdothenecessaryweightupdatesforgradientdescent.Eachmodulehastoprovide in∂loss/∂ZL andcom- thefollowing“methods.”"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Wearealreadyusinglettersa,x,y,zwithparticularmeanings, putes∂loss/∂AL−1. So soherewewilluseuasthevectorinputtothemoduleandvasthevectoroutput: everymoduleisaccept- ingitsblameforthe forward: u v loss,computinghow • → muchofittoallocateto backward: u,v,∂L/∂v ∂L/∂u • → eachofitsinputs,and passingtheblameback weightgrad: u,∂L/∂v ∂L/∂WonlyneededformodulesthathaveweightsW • → tothem. Inhomeworkwewillaskyoutoimplementthesemodulesforneuralnetworkcomponents,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "andthenusethemtoconstructanetworkandtrainitasdescribedinthenextsection. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 50 5 Training Herewego!Here’showtodostochasticgradientdescenttrainingonafeed-forwardneural network. Afterthispseudo-code,wemotivatethechoiceofinitializationinlines2and3. The actual computation of the gradient values (e.g. ∂loss/∂AL) is not directly defined in thiscode,becausewewanttomakethestructureofthecomputationclear. Study Question: What is ∂Zl/∂Wl? Study Question: Which"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "terms in the code below depend on fL? SGD-NEURAL-NET(D n ,T,L,(m1,...,mL),(f1,...,fL)) 1 forl = 1toL 2 Wl ∼Gaussian(0,1/ml) ij 3 Wl ∼Gaussian(0,1) 0j 4 fort = 1toT 5 i = randomsamplefrom{1,...,n} 6 A0 = x(i) 7 //forwardpasstocomputetheoutputAL 8 forl = 1toL 9 Zl = WlTAl−1+Wl 0 10 Al = fl(Zl) 11 loss = Loss(AL,y(i)) 12 forl = Lto1: 13 //errorback-propagation 14 ∂loss/∂Al = ifl<Lthen∂loss/∂Zl+1 ∂Zl+1/∂Alelse∂loss/∂AL · 15 ∂loss/∂Zl = ∂loss/∂Al ∂Al/∂Zl · 16 //computegradientwithrespecttoweights 17"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "∂loss/∂Wl = ∂loss/∂Zl ∂Zl/∂Wl · 18 ∂loss/∂Wl = ∂loss/∂Zl ∂Zl/∂Wl 0 · 0 19 //stochasticgradientdescentupdate 20 Wl =Wl−η(t) ∂loss/∂Wl · 21 Wl =Wl−η(t) ∂loss/∂Wl 0 0 · 0 Initializing W is important; if you do it badly there is a good chance the neural network training won’t work well. First, it is important to initialize the weights to random val- ues. We want different parts of the network to tend to “address” different aspects of the problem; if they all start at the same weights, the symmetry"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "will often keep the values from moving in useful directions. Second, many of our activation functions have (near) zeroslopewhenthepre-activationzvalueshavelargemagnitude,sowegenerallywantto keeptheinitialweightssmallsowewillbeinasituationwherethegradientsarenon-zero, sothatgradientdescentwillhavesomeusefulsignalaboutwhichwaytogo. Onegoodgeneral-purposestrategyistochooseeachweightatrandomfromaGaussian (normal)distributionwithmean0andstandarddeviation(1/m)wheremisthenumber ofinputstotheunit."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Study Question: If the input x to this unit is a vector of 1’s, what would the ex- pected pre-activation z value be with these initial weights? Wewritethischoice(where∼means“isdrawnrandomlyfromthedistribution”) 1 Wl ∼Gaussian 0, . ij ml (cid:18) (cid:19) LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 51 Itwilloftenturnout(especiallyforfancieractivationsandlossfunctions)thatcomput- ing ∂loss ∂ZL iseasierthancomputing ∂loss ∂AL and . ∂AL ∂ZL So, we may instead ask for an implementation of a loss"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "function to provide a backward methodthatcomputes∂loss/∂ZLdirectly. 6 Loss functions and activation functions Differentlossfunctionsmakedifferentassumptionsabouttherangeofinputstheywillget asinputand,aswehaveseen,differentactivationfunctionswillproduceoutputvaluesin differentranges. Whenyouaredesigninganeuralnetwork,it’simportanttomakethese thingsfittogetherwell.Inparticular,wewillthinkaboutmatchinglossfunctionswiththe"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "activationfunctioninthelastlayer,fL.Hereisatableoflossfunctionsandactivationsthat makesenseforthem: Loss fL squared linear hinge linear NLL sigmoid NLLM softmax 6.1 Two-classclassificationandloglikelihood For classification, the natural loss function is 0-1 loss, but we have already discussed the factthatit’sveryinconvenientforgradient-basedlearningbecauseitsderivativeisdiscon- tinuous. Wehavealsoexplorednegativeloglikelihood(NLL)inchapter5. Itisniceandsmooth,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "andextendsnicelytomultipleclassesaswewillseebelow. Hingelossgivesusanotherway,forbinaryclassificationproblems,tomakeasmoother objective,penalizingthemarginsofthelabeledpointsrelativetotheseparator. Thehinge lossisdefinedtobe L (guess,actual)=max(1−guess actual,0) , h · whenactual {+1,−1}. Ithasthepropertythat,ifthesignofguessisthesameasthesign ∈ ofactualandthemagnitudeofguessisgreaterthan1,thenthelossis0. Itistryingtoenforcenotonlythattheguesshavethecorrectsign,butalsothatitshould be some"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "distance away from the separator. Using hinge loss, together with a squared- normregularizer,actuallyforcesthelearningprocesstotrytofindaseparatorthathasthe maximummarginrelativetothedataset.Thisoptimizationset-upiscalledasupportvector machine,andwaspopularbeforetherenaissanceofneuralnetworksandgradientdescent, becauseithasaquadraticformthatmakesitparticularlyeasytooptimize. 6.2 Multi-classclassificationandloglikelihood WecanextendtheideaofNLLdirectlytomulti-classclassificationwithKclasses,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "where T thetraininglabelisrepresentedwiththeone-hotvectory= y ,...,y ,wherey =1if 1 K k theexampleisofclassk. Assumethatournetworkusessoftmaxastheactivationfunction (cid:2) (cid:3) LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 52 T in the last layer, so that the output is a = a ,...,a , which represents a probability 1 K distribution over the K possible classes. Then, the probability that our network predicts (cid:2) (cid:3) thecorrectclassforthisexampleis K ayk"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "andthelogoftheprobabilitythatitiscorrect k=1 k is K y loga ,so k=1 k k Q P K L (guess,actual)=− actual log(guess ) . nllm k · k k X =1 We’llcallthisNLLMfornegativeloglikelihoodmulticlass. Study Question: Show that L for K=2 is the same as L . nllm nll LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 53 7 Optimizing neural network parameters Becauseneuralnetworksarejustparametricfunctions,wecanoptimizelosswithrespectto theparametersusingstandardgradient-descentsoftware,butwecantakeadvantageofthe"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "structureofthelossfunctionandthehypothesisclasstoimproveoptimization.Aswehave seen,themodularfunction-compositionstructureofaneuralnetworkhypothesismakesit easytoorganizethecomputationofthegradient.Aswehavealsoseenearlier,thestructure ofthelossfunctionasasumoverterms,onepertrainingdatapoint,allowsustoconsider stochasticgradientmethods. Inthissectionwe’llconsidersomealternativestrategiesfor organizingtraining,andalsoformakingiteasiertohandlethestep-sizeparameter. 7.1 Batches"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Assumethatwehaveanobjectiveoftheform n J(W)= L(h(x(i);W),y(i)) , X i=1 where h is the function computed by a neural network, and W stands for all the weight matricesandvectorsinthenetwork. Whenweperformbatchgradientdescent,weusetheupdaterule W :=W−η J(W) , W ∇ whichisequivalentto n W :=W−η L(h(x(i);W),y(i)) . W ∇ X i=1 So,wesumupthegradientoflossateachtrainingpoint,withrespecttoW,andthentake astepinthenegativedirectionofthegradient."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Instochasticgradientdescent,werepeatedlypickapoint(x(i),y(i))atrandomfromthe dataset,andexecuteaweightupdateonthatpointalone: W :=W−η L(h(x(i);W),y(i)) . W ∇ As long as we pick points uniformly at random from the data set, and decrease η at an appropriaterate, weare guaranteed, withhighprobability, toconverge toatleasta local optimum. Thesetwomethodshaveoffsettingvirtues. Thebatchmethodtakesstepsintheexact gradientdirectionbutrequiresalotofcomputationbeforeevenasinglestepcanbetaken,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "especiallyifthedatasetislarge.Thestochasticmethodbeginsmovingrightaway,andcan sometimes make very good progress before looking at even a substantial fraction of the wholedataset,butifthereisalotofvariabilityinthedata,itmightrequireaverysmallη toeffectivelyaverageovertheindividualstepsmovingin“competing”directions. Aneffectivestrategyisto“average”betweenbatchandstochasticgradientdescentby using mini-batches. For a mini-batch of size k, we select k distinct data points uniformly at random from"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "the data set and do the update based just on their contributions to the gradient k W :=W−η L(h(x(i);W),y(i)) . W ∇ X i=1 Mostneuralnetworksoftwarepackagesaresetuptodomini-batches. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 54 Study Question: For what value of k is mini-batch gradient descent equivalent to stochastic gradient descent? To batch gradient descent? Picking k unique data points at random from a large data-set is potentially computa- tionallydifficult."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Analternativestrategy,ifyouhaveanefficientprocedureforrandomly shufflingthedataset(orrandomlyshuffflingalistofindicesintothedataset)istooperate inaloop,roughlyasfollows: MINI-BATCH-SGD(NN,data,k) 1 n=length(data) 2 whilenotdone: 3 RANDOM-SHUFFLE(data) 4 fori = 1ton/k 5 BATCH-GRADIENT-UPDATE(NN,data[(i−1)k:ik]) 7.2 Adaptivestep-size Pickingavalueforηisdifficultandtime-consuming. Ifit’stoosmall,thenconvergenceis slowandifit’stoolarge,thenweriskdivergenceorslowconvergenceduetooscillation."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Thisproblemisevenmorepronouncedinstochasticormini-batchmode,becauseweknow weneedtodecreasethestepsizefortheformalguaranteestohold. It’s also true that, within a single neural network, we may well want to have differ- ent step sizes. As our networks become deep (with increasing numbers of layers) we can find that magnitude of the gradient of the loss with respect the weights in the last layer, ∂loss/∂W ,maybesubstantiallydifferentfromthegradientofthelosswithrespecttothe L"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "weightsinthefirstlayer∂loss/∂W . Ifyoulookcarefullyatequation8.3,youcanseethat 1 the output gradient is multiplied by all the weight matrices of the network and is “fed back”throughallthederivativesofalltheactivationfunctions.Thiscanleadtoaproblem ofexplodingorvanishinggradients,inwhichtheback-propagatedgradientismuchtoobig orsmalltobeusedinanupdaterulewiththesamestepsize. So, we’llconsiderhavinganindependentstep-sizeparameterforeachweight, andup-"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "datingitbasedonalocalviewofhowthegradientupdateshavebeengoing. Thissectionisvery stronglyinfluenced bySebastianRuder’s 7.2.1 Runningaverages excellentblogpostson thetopic: ruder.io/ We’llstartbylookingatthenotionofarunningaverage. It’sacomputationalstrategyfor optimizing-gradient-descent estimating a possibly weighted average of a sequence of data. Let our data sequence be a ,a ,...; then we define a sequence of running average values, A ,A ,A ,... using the 1 2 0 1 2 equations A =0 0 A =γ A"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "+(1−γ )a t t t−1 t t whereγ (0,1). Ifγ isaconstant,thenthisisamovingaverage,inwhich t t ∈ A =γA +(1−γ)a T T−1 T =γ(γA +(1−γ)a )+(1−γ)a T−2 T−1 T T = γT−t(1−γ)a t t=0 X So,youcanseethatinputsa closertotheendofthesequencehavemoreeffectonA than t t earlyinputs. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 55 If,instead,wesetγ =(t−1)/t,thenwegettheactualaverage. t Study Question: Prove to yourself that the previous assertion holds. 7.2.2 Momentum Now, we can use methods that are a bit like"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "running averages to describe strategies for computing η. The simplest method is momentum, in which we try to “average” recent gradientupdates,sothatiftheyhavebeenbouncingbackandforthinsomedirection,we takeoutthatcomponentofthemotion. Formomentum,wehave V =0 0 V =γV +η J(W ) t t−1 W t−1 ∇ W =W −V t t−1 t This doesn’t quite look like an adaptive step size. But what we can see is that, if we let η = η (1 − γ), then the rule looks exactly like doing an update with step size η on a 0 0"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "movingaverageofthegradientswithparameterγ: M =0 0 M =γM +(1−γ) J(W ) t t−1 W t−1 ∇ W t =W t−1 −η 0 M t Study Question: Prove to yourself that these formulations are equivalent. WewillfindthatV willbebiggerindimensionsthatconsistentlyhavethesamesign t for andsmallerforthosethatdon’t. Ofcoursewenowhavetwoparameterstoset(η W ∇ and γ), but the hope is that the algorithm will perform better overall, so it will be worth tryingtofindgoodvaluesforthem. Oftenγissettobesomethinglike0.9."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Theredarrowsshowtheupdateafteronestepofmini-batchgradientdescentwith momentum. The blue points show the direction of the gradient with respect to themini-batchateachstep. Momentumsmoothsthepathtakentowardsthelocal minimumandleadstofasterconvergence. Study Question: If you set γ = 0.1, would momentum have more of an effect or less of an effect than if you set it to 0.9? LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 56 7.2.3 Adadelta Anotherusefulideaisthis:"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "wewouldliketotakelargerstepsinpartsofthespacewhere J(W) is nearly flat (because there’s no risk of taking too big a step due to the gradient being large) and smaller steps when it is steep. We’ll apply this idea to each weight in- dependently,andendupwithamethodcalledadadelta,whichisavariantonadagrad(for adaptivegradient). Eventhoughourweightsareindexedbylayer,inputunitandoutput unit, for simplicity here, just let W be any weight in the network (we will do the same j thingforallofthem). g = J(W"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": ") t,j W t−1 j ∇ G =γG +(1−γ)g2 t,j t−1,j t,j η W =W − g t,j t−1,j t,j G +(cid:15) t,j ThesequenceG isamovingaverageofthespquareofthejthcomponentofthegradient. t,j Wesquareitinordertobeinsensitivetothesign—wewanttoknowwhetherthemagni- tudeisbigorsmall. Then,weperformagradientupdatetoweightj,butdividethestep sizeby G +(cid:15),whichislargerwhenthesurfaceissteeperindirectionjatpointW in t,j t−1 weightspace;thismeansthatthestepsizewillbesmallerwhenit’ssteepandlargerwhen p it’sflat. 7.2.4 Adam"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Adamhasbecomethedefaultmethodofmanagingstepsizesneuralnetworks.Itcombines Although,interestingly, theideasofmomentumandadadelta.Westartbywritingmovingaveragesofthegradient itmayactuallyviolate theconvergence andsquaredgradient,whichreflectestimatesofthemeanandvarianceofthegradientfor conditionsof SGD: weightj: arxiv.org/abs/1705.08292 g = J(W ) t,j W t−1 j ∇ m =B m +(1−B )g t,j 1 t−1,j 1 t,j v =B v +(1−B )g2 . t,j 2 t−1,j 2 t,j Aproblemwiththeseestimatesisthat,ifweinitializem =v"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "=0,theywillalwaysbe 0 0 biased(slightlytoosmall). Sowewillcorrectforthatbiasbydefining m mˆ = t,j t,j 1−Bt 1 v vˆ = t,j t,j 1−Bt 2 η W =W − mˆ . t,j t−1,j t,j vˆ +(cid:15) t,j NotethatBt isB raisedtothepowert, and p likewiseforBt. Tojustifythesecorrections, 1 1 2 note that if we were to expand m in terms of m and g ,g ,...,g the coefficients t,j 0,j 0,j 1,j t,j wouldsumto1. However,thecoefficientbehindm isBt andsincem = 0,thesumof 0,j 1 0,j coefficientsofnon-zerotermsis1−Bt,hencethecorrection."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Thesamejustificationholds 1 forv . t,j Now,ourupdateforweightjhasastepsizethattakesthesteepnessintoaccount,asin adadelta, butalsotendstomoveinthesamedirection, asinmomentum. Theauthorsof thismethodproposesettingB = 0.9,B = 0.999,(cid:15) = 10−8. Althoughwenowhaveeven 1 2 moreparameters,Adamisnothighlysensitivetotheirvalues(smallchangesdonothave ahugeeffectontheresult). LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 57 Study Question: Define mˆ directly as a moving average of g . What is the"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "decay j t,j (γ parameter)? Even though we now have a step-size for each weight, and we have to update vari- ous quantities on each iteration of gradient descent, it’s relatively easy to implement by maintainingamatrixforeachquantity(m‘,v‘,g‘,g2‘ )ineachlayerofthenetwork. t t t t 8 Regularization So far, we have only considered optimizing loss on the training data as our objective for neuralnetworktraining. But,aswehavediscussedbefore,thereisariskofoverfittingif wedothis."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Thepragmaticfactisthat,incurrentdeepneuralnetworks,whichtendtobe verylargeandtobetrainedwithalargeamountofdata,overfittingisnotahugeproblem. Thisrunscountertoourcurrenttheoreticalunderstandingandthestudyofthisquestion isahotareaofresearch. Nonetheless,thereareseveralstrategiesforregularizinganeural network,andtheycansometimesbeimportant. 8.1 Methodsrelatedtoridgeregression Onegroupofstrategiescan,interestingly,beshowntohavesimilareffects:earlystopping,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "weightdecay,andaddingnoisetothetrainingdata. Resultisdueto Early stopping is the easiest to implement and is in fairly common use. The idea is Bishop,described inhistextbookand to train on your training set, but at every epoch (pass through the whole training set, or heredoi.org/10.1162/ possibly more frequently), evaluate the loss of the current W on a validation set. It will neco.1995.7.1.108. generally be the case that the loss on the training set goes down fairly consistently with"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "eachiteration,thelossonthevalidationsetwillinitiallydecrease,butthenbegintoincrease again. Once you see that the validation loss is systematically increasing, you can stop trainingandreturntheweightsthathadthelowestvalidationerror. Anothercommonstrategyistosimplypenalizethenormofalltheweights,aswedidin ridgeregression.Thismethodisknownasweightdecay,becausewhenwetakethegradient oftheobjective n J(W)= Loss(NN(x(i)),y(i);W)+λ W 2 k k X i=1 weendupwithanupdateoftheform W =W −η Loss(NN(x(i)),y(i);W"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": ") +λW t t−1 W t−1 t−1 ∇ (cid:16)(cid:16) (cid:17) (cid:17) =W (1−λη)−η Loss(NN(x(i)),y(i);W ) . t−1 W t−1 ∇ (cid:16) (cid:17) This rule has the form of first “decaying” W by a factor of (1−λη) and then taking a t−1 gradientstep. Finally,thesameeffectcanbeachievedbyperturbingthex(i)valuesofthetrainingdata by adding a small amount of zero-mean normally distributed noise before each gradient computation. It makes intuitive sense that it would be more difficult for the network to"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "overfittoparticulartrainingdataiftheyarechangedslightlyoneachtrainingstep. 8.2 Dropout Dropoutisaregularizationmethodthatwasdesignedtoworkwithdeepneuralnetworks. Theideabehinditis,ratherthanperturbingthedataeverytimewetrain,we’llperturbthe network! We’lldothisbyrandomly,oneachtrainingstep,selectingasetofunitsineach LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 58 layer and prohibiting them from participating. Thus, all of the units will have to take a"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "kindof“collective”responsibilityforgettingtheanswerright,andwillnotbeabletorely onanysmallsubsetoftheweightstodoallthenecessarycomputation. Thistendsalsoto makethenetworkmorerobusttodataperturbations. During the training phase, for each training example, for each unit, randomly with probabilityptemporarilyseta‘ := 0. Therewillbenocontributiontotheoutputandno j gradientupdatefortheassociatedunit. Study Question: Be sure you understand why, when using SGD, setting an activa- tion value to 0 will"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "cause that unit’s weights not to be updated on that iteration. Whenwearedonetrainingandwanttousethenetworktomakepredictions,wemul- tiplyallweightsbyptoachievethesameaverageactivationlevels. Implementingdropoutiseasy! Intheforwardpassduringtraining,welet a‘ =f(z‘) d‘ ∗ where denotescomponent-wiseproductandd‘isavectorof0’sand1’sdrawnrandomly ∗ with probability p. The backwards pass depends on a‘, so we do not need to make any furtherchangestothealgorithm. It is common to set p to 0.5, but this is"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "something one might experiment with to get goodresultsonyourproblemanddata. 8.3 BatchNormalization Amoremodernalternativetodropout,whichtendstoachievebetterperformance,isbatch normalization. Itwasoriginallydevelopedtoaddressaproblemofcovariateshift: thatis,if Formoredetailssee youconsiderthesecondlayerofatwo-layerneuralnetwork,thedistributionofitsinput arxiv.org/abs/1502.03167. valuesischangingovertimeasthefirstlayer’sweightschange. Learningwhentheinput"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "distributionischangingisextradifficult:youhavetochangeyourweightstoimproveyour predictions,butalsojusttocompensateforachangeinyourinputs(imagine,forinstance, thatthemagnitudeoftheinputstoyourlayerisincreasingovertime—thenyourweights willhavetodecrease,justtokeepyourpredictionsthesame). So,whentrainingwithmini-batches,theideaistostandardizetheinputvaluesforeach mini-batch,justinthewaythatwediditinsection2.3ofchapter4,subtractingoffthemean"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "anddividingbythestandarddeviationofeachinputdimension.Thismeansthatthescale oftheinputstoeachlayerremainsthesame,nomatterhowtheweightsinpreviouslayers change. However, this somewhat complicates matters, because the computation of the weightupdateswillneedtotakeintoaccountthatweareperformingthistransformation. In the modular view, batch normalization can be seen as a module that is applied to zl, interposedaftertheproductwithWlandbeforeinputtofl."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Batchnormalizationendsuphavingaregularizingeffectforsimilarreasonsthatadding noise and dropout do: each mini-batch of data ends up being mildly perturbed, which preventsthenetworkfromexploitingveryparticularvaluesofthedatapoints. Let’sthinkofthebatch-normlayerastakingzlasinputandproducinganoutputZlas output. Butnow,insteadofthinkingofZl asannl 1vector,wehavetoexplicitlythink × abouthandlingamini-batchofdataofsizeK,allatonce,soZl willbenl K,andsobwill × theoutputZl."
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Ourfirststepwillbetocomputethebatchwisemeanandstandarddeviation. Letµl be thenl 1vbectorwhere × K 1 µl = Zl , i K ij X j=1 LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 59 andletσlbethenl 1vectorwhere × K 1 σl = (Zl −µ )2 . i vK ij i u u X j=1 t Thebasicnormalizedversionofourdatawouldbeamatrix,element(i,j)ofwhichis Zl −µl Z l = ij i , ij σl+(cid:15) i where(cid:15)isaverysmallconstanttoguardagainstdivisionbyzero.However,ifweletthese"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "beourZlvalues,wereallyareforcingsomethingtoostrongonourdata—ourgoalwasto normalizeacrossthedatabatch,butnotnecessarilyforcetheoutputvaluestohaveexactly mean0bandstandarddeviation1. So,wewillgivethelayerthe“opportunity”toshiftand scaletheoutputsbyaddingnewweightstothelayer.TheseweightsareGlandBl,eachof whichisannl 1vector. Usingtheweights,wedefinethefinaloutputtobe × Zl =GlZ l +Bl . ij i ij i That’stheforwardpass. Whew! b Now,forthebackwardpass,wehavetodotwothings: given∂L/∂Zl,"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Compute∂L/∂Zlforback-propagation,and • b Compute∂L/∂Gland∂L/∂Blforgradientupdatesoftheweightsinthislayer. • Schematically ∂L ∂L∂Z = . ∂B ∂Z∂B b It’shardtothinkaboutthesederivativesinmatrixterms,sowe’llseehowitworksforthe components. B contributestoZ foralldatabpointsjinthebatch. So i ij b ∂L ∂L ∂Z ij = ∂B i j ∂Z ij ∂B i X b ∂L = b , ∂Z j ij X Similarly,G contributestoZ foralldatapointbsjinthebatch. So i ij b ∂L ∂L ∂Z ij = ∂G i j ∂Z ij ∂G i X b ∂L = b Z ij . ∂Z j ij X"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "Now,let’sfigureouthowtodobackprop. Wecabnstartschematically: ∂L ∂L∂Z = . ∂Z ∂Z∂Z b Andbecausedependenciesonlyexistacrossthebatch,butnotacrosstheunitoutputs, b K ∂L ∂L ∂Z ik = . ∂Z ij k X =1 ∂Z ik ∂Z b ij b LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 60 Thenextstepistonotethat ∂Z ∂Z ∂Z ik ik ik = ∂Z ij ∂Z ik ∂Z ij b b ∂Z =G ik i∂Z ij Andnowthat ∂Z ∂µ 1 Z −µ ∂σ ik = δ − i − ik i i , ∂Z jk ∂Z σ σ2 ∂Z ij (cid:18) ij(cid:19) i i ij whereδ =1ifj=kandδ"
  },
  {
    "file": "mit_ocw_ch_8.pdf",
    "chunk": "=0otherwise.Gettingclose!Weneedtwomoresmallparts: jk jk ∂µ 1 i = ∂Z K ij ∂σ Z −µ i ij i = ∂Z Kσ ij i Puttingthewholecrazythingtogether,weget K ∂L ∂L 1 (Z −µ )(Z −µ ) = G δ K−1− ik i ij i ∂Z ij k X =1 ∂Z ik iKσ i (cid:18) jk σ2 i (cid:19) b LastUpdated: 12/18/1911:56:05"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "9 CHAPTER Convolutional Neural Networks Sofar,wehavestudiedwhatarecalledfullyconnectedneuralnetworks,inwhichallofthe unitsatonelayerareconnectedtoalloftheunitsinthenextlayer. Thisisagoodarrange- mentwhenwedon’tknowanythingaboutwhatkindofmappingfrominputstooutputs wewillbeaskingthenetworktolearntoapproximate.Butifwedoknowsomethingabout ourproblem,itisbettertobuilditintothestructureofourneuralnetwork. Doingsocan savecomputationtimeandsignificantlydiminishtheamountoftrainingdatarequiredto"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "arriveatasolutionthatgeneralizesrobustly. Oneveryimportantapplicationdomainofneuralnetworks, wherethemethodshave achieved an enormous amount of success in recent years, is signal processing. Signals might be spatial (in two-dimensional camera images or three-dimensional depth or CAT scans)ortemporal(speechormusic).Ifweknowthatweareaddressingasignal-processing problem,wecantakeadvantageofinvariantpropertiesofthatproblem.Inthischapter,we"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "willfocusontwo-dimensionalspatialproblems(images)butuseone-dimensionalonesas asimpleexample. Later,wewilladdresstemporalproblems. Imaginethatyouaregiventheproblemofdesigningandtraininganeuralnetworkthat takesanimageasinput,andoutputsaclassification,whichispositiveiftheimagecontains acatandnegativeifitdoesnot.Animageisdescribedasatwo-dimensionalarrayofpixels, Apixelisa“pictureele- eachofwhichmayberepresentedbythreeintegervalues,encodingintensitylevelsinred, ment.” green,andbluecolorchannels."
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "Therearetwoimportantpiecesofpriorstructuralknowledgewecanbringtobearon thisproblem: Spatiallocality:Thesetofpixelswewillhavetotakeintoconsiderationtofindacat • willbenearoneanotherintheimage. So,forexample,we won’thavetoconsider Translationinvariance: Thepatternofpixelsthatcharacterizesacatisthesameno somecombinationof • matterwhereintheimagethecatoccurs. pixelsinthefourcor- nersoftheimage,in Wewilldesignneuralnetworkstructuresthattakeadvantageoftheseproperties. ordertoseeiftheyen-"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "codecat-ness. Catsdon’tlookdiffer- entifthey’reontheleft ortherightsideofthe image. 61MIT6.036 Fall2019 62 1 Filters Webeginbydiscussingimagefilters.Animagefilterisafunctionthattakesinalocalspatial UnfortunatelyinAI/M- neighborhoodofpixelvaluesanddetectsthepresenceofsomepatterninthatdata. L/CS/Math,theword “filter”getsusedin Let’s consider a very simple case to start, in which we have a 1-dimensional binary manyways: inaddition “image” and a filter F of size two. The filter is a vector of two"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "numbers, which we will totheonewedescribe move along the image, taking the dot product between the filter values and the image here,itcandescribea valuesateachstep,andaggregatingtheoutputstoproduceanewimage. temporalprocess(in fact,ourmovingaver- LetXbetheoriginalimage,ofsized; thenpixeliofthetheoutputimageisspecified agesareakindoffilter) by andevenasomewhat Y =F (X ,X ) . esotericalgebraicstruc- i i−1 i · ture. Toensurethattheoutputimageisalsoofdimensiond,wewillgenerally“pad”theinput image"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "with 0 values if we need to access pixels that are beyond the bounds of the input image. This process of applying the filter to the image to create a new image is called “convolution.” Andfiltersarealso Ifyouarealreadyfamiliarwithwhataconvolutionis, youmightnoticethatthisdef- sometimescalledcon- volutionalkernels. inition corresponds to what is often called a correlation and not to a convolution. In- deed,correlationandconvolutionrefertodifferentoperationsinsignalprocessing. How- ever, in the"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "neural networks literature, most libraries implement the correlation (as de- scribed in this chapter) but call it convolution. The distinction is not significant; in prin- ciple, if convolution is required to solve the problem, the network could learn the nec- essary weights. For a discussion of the difference between convolution and correlation andtheconventionsusedintheliteratureyoucanreadsection9.1inthisexcellentbook: https://www.deeplearningbook.org. Here is a concrete example. Let the"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "filter F = (−1,+1). Then given the first image 1 below,wecanconvolveitwithfilterF toobtainthesecondimage. Youcanthinkofthis 1 filter as a detector for “left edges” in the original image—to see this, look at the places wherethereisa1intheoutputimage, andseewhatpatternexistsatthatpositioninthe inputimage. AnotherinterestingfilterisF = (−1,+1,−1). Thethirdimagebelowshows 2 theresultofconvolvingthefirstimagewithF . 2 Study Question: Convince yourself that filter F can be understood as a detector"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "for 2 isolated positive pixels in the binary image. Image: 0 0 1 1 1 0 1 0 0 0 F 1 : -1 +1 Afterconvolution(w/F 1 ): 0 1 0 0 -1 1 -1 0 0 F 2 -1 +1 -1 Afterconvolution(w/F 2 ): -1 0 -1 0 -2 1 -1 0 Two-dimensional versions of filters like these are thought to be found in the visual cortex of all mammalian brains. Similar patterns arise from statistical analysis of natural LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 63 images. Computervisionpeopleusedtospendalotoftimehand-designingfilterbanks."
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "A filterbankisasetofsetsoffilters,arrangedasshowninthediagrambelow. Image Allofthefiltersinthefirstgroupareappliedtotheoriginalimage;ifthereareksuch filters, then the result is k new images, which are called channels. Now imagine stacking allthesenewimagesupsothatwehaveacubeofdata, indexedbytheoriginalrowand columnindicesoftheimage, aswellasbythechannel. Thenextsetoffiltersinthefilter bankwillgenerallybethree-dimensional:eachonewillbeappliedtoasub-rangeoftherow"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "andcolumnindicesoftheimageandtoallofthechannels. These3Dchunksofdataarecalledtensors. Thealgebraoftensorsisfun,andalotlike Wewilluseapopular matrixalgebra,butwewon’tgointoitinanydetail. pieceofneural-network softwarecalledTensor- Hereisamorecomplexexampleoftwo-dimensionalfiltering.Wehavetwo3 3filters × flowbecauseitmakes in the first layer, f and f . You can think of each one as “looking” for three pixels in a 1 2 operationsontensors row,f 1 verticallyandf 2 horizontally."
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "Assumingourinputimageisn n,thentheresult easy. × of filtering with these two filters is an n n 2 tensor. Now we apply a tensor filter × × (hard to draw!) that “looks for” a combination of two horizontal and two vertical bars (nowrepresentedbyindividualpixelsinthetwochannels),resultinginasinglefinaln n × image. Whenwehaveacolor imageasinput,wetreat itashaving3channels, andhenceasann n 3 × × tensor. f 2 tensor filter f 1 We are going to design neural networks that have this structure. Each “bank”"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "of the filter bank will correspond to a neural-network layer. The numbers in the individual fil- terswillbethe“weights”(plusasingleadditivebiasoroffsetvalueforeachfilter)ofthe network, which we will train using gradient descent. What makes this interesting and powerful(andsomewhatconfusingatfirst)isthatthesameweightsareusedmanymany times in the computation of each layer. This weight sharing means that we can express a transformationonalargeimagewithrelativelyfewparameters; italsomeanswe’llhave"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "totakecareinfiguringoutexactlyhowtotrainit! LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 64 Wewilldefineafilterlayerlformallywith: Forsimplicity,weare assumingthatallim- numberoffiltersml; agesandfiltersare • square(havingthesame sizeofonefilteriskl kl ml−1plus1biasvalue(forthisonefilter); numberofrowsand • × × columns). Thatisinno strideslisthespacingatwhichweapplythefiltertotheimage;inallofourexamples waynecessary,butis • usuallyfineanddef-"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "sofar,wehaveusedastrideof1,butifwewereto“skip”andapplythefilteronlyat initelysimplifiesour odd-numberedindicesoftheimage,thenitwouldhaveastrideoftwo(andproduce notation. aresultingimageofhalfthesize); inputtensorsizenl−1 nl−1 ml−1 • × × padding: pl is how many extra pixels – typically with value 0 – we add around the • edgesoftheinput. Foraninputofsizenl−1 nl−1 ml−1,ourneweffectiveinput × × sizewithpaddingbecomes(nl−1+2 pl) (nl−1+2 pl) ml−1. ∗ × ∗ × Thislayerwillproduceanoutputtensorofsizenl nl"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "ml,wherenl = (nl−1+2 pl− × × d ∗ (kl−1))/sl .1 Theweightsarethevaluesdefiningthefilter:therewillbemldifferentkl e × kl ml−1tensorsofweightvalues;pluseachfiltermayhaveabiasterm,whichmeansthere × isonemoreweightvalueperfilter.Afilterwithabiasoperatesjustlikethefilterexamples above,exceptweaddthebiastotheoutput. Forinstance,ifweincorporatedabiasterm of 0.5 into the filter F above, the output would be (−0.5,0.5,−0.5,0.5,−1.5,1.5,−0.5,0.5) 2 insteadof(−1,0,−1,0,−2,1,−1,0). This may seem complicated,"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "but we get a rich class of mappings that exploit image structureandhavemanyfewerweightsthanafullyconnectedlayerwould. Study Question: How many weights are in a convolutional layer specified as above? Study Question: If we used a fully-connected layer with the same size inputs and outputs, how many weights would it have? 2 Max Pooling Itistypicaltostructurefilterbanksintoapyramid,inwhichtheimagesizesgetsmallerin Bothinengineeringand successive layers of processing. The idea is that we find local"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "patterns, like bits of edges innature in the early layers, and then look for patterns in those patterns, etc. This means that, ef- fectively,wearelookingforpatternsinlargerpiecesoftheimageasweapplysuccessive filters.Havingastridegreaterthanonemakestheimagessmaller,butdoesnotnecessarily aggregateinformationoverthatspatialrange. Another common layer type, which accomplishes this aggregation, is max pooling. A max pooling layer operates like a filter, but has no weights. You can think of it as a"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "pure functionallayer,likeaReLUlayerinafullyconnectednetwork. Ithasafiltersize,asinafilter layer, butsimplyreturnsthemaximumvalueinitsfield. Usually, weapplymaxpooling Wesometimesusethe withthefollowingtraits: termreceptivefieldor justfieldtomeanthe stride>1,sothattheresultingimageissmallerthantheinputimage;and areaofaninputimage • thatafilterisbeingap- k>stride,sothatthewholeimageiscovered. pliedto. • 1Here, isknownastheceilingfunction;itreturnsthesmallestintegergreaterthanorequaltoitsinput."
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "d·e E.g., 2.5 =3and 3 =3. d e d e LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 65 Asaresultofapplyingamaxpoolinglayer,wedon’tkeeptrackofthepreciselocationofa pattern. Thishelpsourfilterstolearntorecognizepatternsindependentoftheirlocation. Consideramaxpoolinglayerofstride = k = 2. Thiswouldmapa64 64 3image × × toa32 32 3image. Notethatmaxpoolinglayersdonothaveadditionalbiasoroffset × × values. Study Question: Maximilian Poole thinks it would be a good idea to add two max pooling layers of"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "size k, one right after the other, to their network. What single layer would be equivalent? 3 Typical architecture Hereistheformofatypicalconvolutionalnetwork: Source: https://www.mathworks.com/solutions/deep-learning/convolutional-neural- network.html After each filter layer there is generally a ReLU layer; there maybe be multiple fil- ter/ReLUlayers,thenamaxpoolinglayer,thensomemorefilter/ReLUlayers,thenmax pooling. Oncetheoutputisdowntoarelativelysmallsize, thereistypicallyalastfully-"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "connectedlayer,leadingintoanactivationfunctionsuchassoftmaxthatproducesthefinal output. Theexactdesignofthesestructuresisanart—thereisnotcurrentlyanyclearthe- oretical(orevensystematicempirical)understandingofhowthesevariousdesignchoices affectoverallperformanceofthenetwork. Thecriticalpointforusisthatthisisalljustabigneuralnetwork,whichtakesaninput andcomputesanoutput. Themappingisadifferentiablefunctionoftheweights, which Well,thederivativeis"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "meanswecanadjusttheweightstodecreasethelossbyperforminggradientdescent,and notcontinuous,bothbe- causeoftheReLUand wecancomputetherelevantgradientsusingback-propagation! themaxpoolingoper- Let’sworkthroughaverysimpleexampleofhowback-propagationcanworkonacon- ations,butweignore volutionalnetwork.Thearchitectureisshownbelow. Assumewehaveaone-dimensional thatfact. single-channel image, of size n 1 1 and a single k 1 1 filter (where we omit the × × × × filter bias) in the first convolutional layer."
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "Then we pass it through a ReLU layer and a fully-connectedlayerwithnoadditionalactivationfunctionontheoutput. LastUpdated: 12/18/1911:56:05MIT6.036 Fall2019 66 0 conv ReLU fc Z2 =A2 W1 padwith0’s (togetoutput 0 Z1 A1 ofsameshape) X=A0 Forsimplicityassumekisodd,lettheinputimageX = A0,andassumeweareusing squaredloss. Thenwecandescribetheforwardpassasfollows: Z1 =W1T A0 i · [i− b k/2 c :i+ b k/2 c ] A1 =ReLU(Z1) A2 =W2T A1 L(A2,y)=(A2−y)2 Study Question: For a filter of size k, how much padding do"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "we need to add to the top and bottom of the image? HowdoweupdatetheweightsinfilterW1? ∂loss ∂Z1 ∂A1 ∂loss = ∂W1 ∂W1 · ∂Z1 · ∂A1 ∂Z1/∂W1isthek nmatrixsuchthat∂Z1/∂W1 =X .So,forexample,ifi= • × i j i− b k/2 c +j−1 10,whichcorrespondstocolumn10inthismatrix,whichillustratesthedependence of pixel 10 of the output image on the weights, and if k = 5, then the elements in column10willbeX ,X ,X ,X ,X . 8 9 10 11 12 ∂A1/∂Z1isthen ndiagonalmatrixsuchthat • × 1 ifZ1 >0 ∂A1/∂Z1 = i i i (cid:14)0 otherwise"
  },
  {
    "file": "mit_ocw_ch_9.pdf",
    "chunk": "∂loss/∂A1 =∂loss/∂A2 ∂A2/∂A1 =2(A2−y)W2,ann 1vector • · × Multiplyingthesecomponentsyieldsthedesiredgradient,ofshapek 1. × LastUpdated: 12/18/1911:56:05"
  }
]