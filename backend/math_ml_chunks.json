{
  "ml_math_lect_1.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 1 Scribe:Philippe Rigollet Sep. 9, 2015 1. WHAT IS MACHINE LEARNING (IN THIS COURSE) ? This course focuses on statistical learning theory , which roughly means understanding the amount of data required to achieve a certain prediction accuracy. To better understand what this means, we ﬁrst focus on stating some diﬀerences between statistics andmachine learning since the two ﬁelds share common goals. Indeed,bothseemtotrytousedatatoimprovedecisions. Whiletheseﬁeldshaveevolved in the same direction and currently share a lot of aspects, they were at the beginning quite diﬀerent. Statistics was around much before machine learning and statistics was already a fully developed scientiﬁc discipline by 1920, most notably thanks to the contributions of R. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for statistical inference. However, MLErequiresessentially",
    "knowingtheprobabilitydistribution fromwhichthedataisdraw,uptosomeunknownparameterofinterest. Often, theunknown parameter has a physical meaning and its estimation is key in better understanding some phenomena. Enabling MLE thus requires knowing a lot about the data generating process: this is known as modeling . Modeling can be driven by physics or prior knowledge of the problem. In any case, it requires quite a bit of domain knowledge. More recently (examples go back to the 1960’s) new types of datasets (demographics, social, medical,...) have become available. However, modeling the data that they contain is much more hazardous since we do not understand very well the input/output process thus requiring a distribution free approach. A typical example is image classiﬁcation where the goal is to label an image simply from a digitalization of this image. Understanding what makes an image a cat or a dog for example is a very complicated process. However, for the classiﬁcation task, one",
    "does not need to understand the labelling process but rather to replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1). inputX outputYblackbox y=f(x)+εinputX outputY Figure 1: The machine learning blackbox (left) where the goal is to replicate input/output pairs from past observations, versus the statistical approach that opens the blackbox and models the relationship. These diﬀerences between statistics and machine learning have receded over the last couple of decades. Indeed, on the one hand, statistics is more and more concerned with ﬁnite sample analysis, model misspeciﬁcation and computational considerations. On the other hand, probabilistic modeling is now inherent to machine learning. At the intersection of the two ﬁelds, lies statistical learning theory , a ﬁeld which is primarily concerned with sample complexity questions, some of which will be the focus of this class. 12. STATISTICAL LEARNING THEORY 2.1 Binary classiﬁcation A large partof",
    "this class will bedevoted tooneof thesimplest problemof statistical learning theory: binary classiﬁcation (aka pattern recognition [DGL96]). In this problem, weobserve (X1,Y1),...,(Xn,Yn) that are nindependent random copies of ( X,Y)∈ X×{0,1}. Denote byPX,Ythe joint distribution of ( X,Y). The so-called featureXlives in some abstract spaceX(think IRd) andY∈ {0,1}is called label. For example, Xcan be a collection of gene expression levels measured on a patient and Yindicates if this person suﬀers from obesity. The goal of binary classiﬁcation is to build a rule to predict YgivenXusing only the data at hand. Such a rule is a function h:X → {0,1}called a classiﬁer . Some classiﬁers are better than others and we will favor ones that have low classiﬁcation error R(h) = IP(h(X) =Y). Let us make some important remarks. Fist of all, since Y∈ {0,1}thenYhas a Bernoulli distribution: so much for distribution free assumptions! However, we will not make assumptions on the marginal distribution of",
    "Xor, what matters for prediction, the conditional distribution of YgivenX. We write, Y|X∼Ber(η(X)), where η(X) = IP(Y= 1|X) = IE[Y|X] is called the regression function ofYontoX. Next, note that we did not write Y=η(X). Actually we have Y=η(X) +ε, where ε=Y−η(X) is a“noise” randomvariablethat satisﬁes IE[ ε|X] = 0. Inparticular, this noise accounts for the fact that Xmay not contain enough information to predict Yperfectly. This is clearly the case in our genomic example above: it not whether there is even any information about obesity contained in a patient’s genotype. The noise vanishes if and only ifη(x)∈ {0,1}for allx∈ X. Figure 2.1 illustrates the case where there is no noise and the the more realistic case where there is noise. When η(x) is close to .5, there is essentially no information about YinXas theYis determined essentially by a toss up. In this case, it is clear that even with an inﬁnite amount of data to learn from, we cannot predict Ywell since there is nothing to",
    "learn. We will see what the eﬀect of the noise also appears in the sample complexity./ne}ationslash Figure 2: The thick black curve corresponds to the noiseless case where Y=η(X)∈ {0,1} and the thin red curve corresponds to the more realistic case where η∈[0,1]. In the latter case, even full knowledge of ηdoes not guarantee a perfect prediction of Y. In the presence of noise, since we cannot predict Yaccurately, we cannot drive the classiﬁcation error R(h) to zero, regardless of what classiﬁer hwe use. What is the smallest value that can beachieved? As a thought experiment, assume to begin with that we have all 2 xη(x) 1 .5the information that we may ever hope to get, namely we know the regression function η(·). For a given Xto classify, if η(X) = 1/2 we may just toss a coin to decide our prediction and discard Xsince it contains no information about Y. However, if η(X) = 1/2, we have an edge over random guessing: if η(X)>1/2, it means that IP( Y= 1|X)>IP(Y= 0|X) or, in words, that 1",
    "is more likely to be the correct label. We will see that the classiﬁer h∗(X) = 1I(η(X)>1/2) (called Bayes classiﬁer ) is actually the best possible classiﬁer in the sense that R(h∗) = infR(h), h(·) where the inﬁmum is taken over all classiﬁers, i.e. functions from Xto{0,1}. Note that unlessη(x)∈ {0,1}for allx∈ X(noiseless case), we have R(h∗) = 0. However, we can always look at the excess risk E(h) of a classiﬁer hdeﬁned by E(h) =R(h)−R(h∗)≥0. In particular, we can hope to drive the excess risk to zero with enough observations by mimicking h∗accurately. 2.2 Empirical risk The Bayes classiﬁer h∗, while optimal, presents a major drawback: we cannot compute it because we do not know the regression function η. Instead, we have access to the data (X1,Y1),...,(Xn,Yn), which contains some (but not all) information about ηand thus h∗. In order to mimic the properties of h∗recall that it minimizes R(h) over all h. But the function R(·) is unknown since it depends on the unknown distribution",
    "PX,Yof (X,Y). We ˆ estimate it by the empirical classiﬁcation error, or simply empirical risk Rn(·) deﬁned for any classiﬁer hby n1ˆRn(h) =/summationdisplay 1I(h(Xi) =Yi).ni=1 ˆ ˆ Since IE[1I( h(Xi) =Yi)] = IP(h(Xi) =Yi) =R(h), we have IE[ Rn(h)] =R(h) soRn(h) is anunbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have ˆ ˆ Rn(h)→R(h) asn→ ∞, almost surely. This indicates that if nis large enough, Rn(h) should be close to R(h). As a result, in order to mimic the performance of h∗, let us use the empirical risk ˆ ˆ minimizer (ERM) hdeﬁned to minimize Rn(h) over all classiﬁers h. This is an easy enough ˆ ˆ task: deﬁne hsuchh(Xi) =Yifor alli= 1,...,nandh(x) = 0 ifx∈/{X1,...,X n}. We ˆˆ haveRn(h) = 0, which is clearly minimal. The problem with this classiﬁer is obvious: it does not generalize outside the data. Rather, it predicts the label 0 for any xthat is not in ˆˆ the data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0. ˆ",
    "In particular it is unlikely that IE[ R(h)] will be small. 3/ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslashImportant Remark :Recall that R(h) = IP(h(X)/ne}ationslash=Y). ˆ ˆ ˆ Ifh(·) =h({(X1,Y1),...,(Xn,Yn)};·) is constructed from the data, R(h) denotes theconditional probability ˆ ˆR(h) = IP(h(X)/ne}ationslash=Y|(X1,Y1),.. .,(Xn,Yn)). ˆ ˆ ratherthanIP( h(X)/ne}ationslash=Y). As aresult R(h)is arandomvariablesinceit dependsonthe randomness of the data ( X1,Y1),...,(Xn,Yn). One way to view this is to observe that ˆ we compute the deterministic function R(·) and then plug in the random classiﬁer h. This problem is inherent to any method if we are not willing to make any assumption on the distribution of ( X,Y) (again, so much for distribution freeness!). This can actually be formalized in theorems, known as no-free-lunch theorems. Theorem: ˆ For any integer n≥1, any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and anyε >0, there exists a distribution PX,Yfor",
    "(X,Y) such that R(h∗) = 0 and ˆIER(hn)≥1/2−ε. To be fair, note that here the distribution of the pair ( X,Y) is allowed to depend on nwhich is cheating a bit but there are weaker versions of the no-free-lunch theorem that essentially imply that it is impossible to learn without further assumptions. One such theorem is the following. Theorem: ˆ For any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and any sequence {an}n>0 that converges to 0, there exists a distribution PX,Yfor (X,Y) such that R(h∗) = 0 and ˆIER(hn)≥an,for alln≥1 In the above theorem, the distribution of ( X,Y) is allowed to depend on the whole sequence {an}n>0 but not on a speciﬁc n. The above result implies that the convergence to zero of the classiﬁcation error may be arbitrarily slow. 2.3 Generative vs discriminative approaches Both theorems above imply that we need to restrict the distribution PX,Yof (X,Y). But isn’t that exactly what statistical modeling is? The is answer is not so clear depending on how we perform",
    "this restriction. There are essentially two schools: generative which is the statistical modeling approach and discriminative which is the machine learning approach. Generative: This approach consists in restricting the set of candidate distributions PX,Y. This is what is done in discriminant analysis1where it is assumed that the condition dis- 1Amusingly, the generative approach is called discriminant analysis but don’t let the terminology fool you. 4tributions of XgivenY(there are only two of them: one for Y= 0 and one for Y= 1) are Gaussians on X= IRd(see for example [HTF09] for an overview of this approach). Discriminative: This is the machine learning approach. Rather than making assumptions directly on the distribution, one makes assumptions on what classiﬁers are likely to perform correctly. In turn, this allows to eliminate classiﬁers such as the one described above and that does not generalize well. While it is important to understand both, we will focus on the discriminative",
    "approach in this class. Speciﬁcally we are going to assume that we are given a class Hof classiﬁers such that R(h) is small for some h∈ H. 2.4 Estimation vs. approximation Assumethat we aregiven a class Hin which weexpect to ﬁnda classiﬁer that performswell. Thisclassmaybeconstructedfromdomainknowledgeorsimplycomputational convenience. ˆ We will see some examples in the class. For any candidate classiﬁer hnbuilt from the data, we can decompose its excess risk as follows: ˆ ˆ ˆ E(hn) =R(hn)−R(h∗) =R(hn)−infR(h)+ infR(h)−R(h∗). h∈H h∈H/bracehtipupleft estimat/bracehtipdownright io/bracehtipdownleft n error/bracehtipupright /bracehtipupleft approxim/bracehtipdownright a/bracehtipdownleft tion error/bracehtipupright On the one hand, estimation error accounts for the fact that we only have a ﬁnite amount of observations and thus a partial knowledge of the distribution PX,Y. Hopefully we can drive this error to zero as n→ ∞. But we already know from the no-free-lunch theorem that this will",
    "not happen if His the set of all classiﬁers. Therefore, we need to takeHsmall enough. On the other hand, if His too small, it is unlikely that we will ﬁnd classiﬁer with performance close to that of h∗. A tradeoﬀ between estimation and approximation can be made by letting H=Hngrow (but not too fast) with n. For now, assume that His ﬁxed. The goal of statistical learning theory is to understand how the estimation error drops to zero as a function not only of nbut also of H. For the ﬁrst argument, we will use concentration inequalities such as Hoeﬀding’s and Bernstein’s inequalities that allow us to control how close the empirical risk is to the classiﬁcation error by bounding the random variable /vextendsinglen1/vextendsingle/summationdisplay 1I(h(X (h /vextendsingle i) =Yi)−IP (X) =Y)/vextendsingle n/vextendsingle i=1/vextendsingle with high probability. More generally we will be interested in results that allow to quantify how close the average of independent and identically",
    "distributed (i.i.d) random variables is to their common expected value. ˆˆˆ Indeed, since by deﬁnition, we have Rn(h)≤Rn(h) for allh∈ H, the estimation error ¯ can be controlled as follows. Deﬁne h∈ Hto be any classiﬁer that minimizes R(·) overH (assuming that such a classiﬁer exist). ˆ ˆ ¯ R(hn)−infR(h) =R(hn)−R(h) h∈H ˆˆˆ¯ ˆ ˆ ¯ ¯ =Rn(hn)−Rn(h)+R(hn)−Rˆn(h)+Rˆn n(h)−R(h)/bracehtipupleft ≤/bracehtipdownright/bracehtipdownleft 0/bracehtipupright ≤/vextendsingle/vextendsingleˆˆ ˆ ˆ¯ ¯ Rn(hn)−R(hn)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle. 5/ne}ationslash /ne}ationslash¯ ˆ¯ ¯ Sincehis deterministic, we can use a concentration inequality to control/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle. However, n1ˆˆ ˆ Rn(hn) =/summationdisplay 1I(hn(Xi) =Yi)ni=1 isnot ˆ the average of independent random variables since hndepends in a complicated manner on all of the pairs ( Xi,Yi),i= 1,...,n. To overcome this limitation,",
    "we often use ˆ a blunt, but surprisingly accurate tool: we “sup out” hn, /vextendsingle/vextendsingleˆˆ ˆ ˆˆ ˆ Rn(hn)−R(hn)/vextendsingle/vextendsingle≤sup/vextendsingle h∈/vextendsingleRn(hn)−R(hn)/vextendsingle H/vextendsingle. Controlling this supremum falls in the scope of suprema of empirical processes that we will study in quite a bit of detail. Clearly the supremum is smaller as His smaller but Hshould be kept large enough to have good approximation properties. This is the tradeoﬀ between approximation and estimation. It is also know in statistics as the bias-variance tradeoﬀ. References [DGL96] L. Devroye, L. Gy¨ orﬁ, and G. Lugosi, A probabilistic theory of pattern recognition , Applications of Mathematics (New York), vol. 31, Springer-Verlag, New York, 1996. MR MR1383093 (97d:68196) [HTF09] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statis- tical learning , second ed., Springer Series in Statistics, Springer, New York, 2009, Data mining,",
    "inference, and prediction. MR 2722294 (2012d:62081) 6/ne}ationslashMIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_10.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 10 Scribe:Aden Forrow Oct. 13, 2015 Recall the following deﬁnitions from last time: Deﬁnition: A function K:X ×X /ma√sto→ Ris called a positive symmetric deﬁnite kernel (PSD kernel) if 1.∀x,x′∈ X,K(x,x′) =K(x′,x) 2.∀n∈Z+,∀x1,x2,...,x n, then×nmatrix with entries K(xi,xj) is positive deﬁ- nite. Equivalently, ∀a R 1,a2,...,a n∈, n/summationdisplay aiajK(xi,xj) i,j=1≥0 Deﬁnition: LetWbe a Hilbert space of functions X /ma√sto→R. A symmetric kernel K(·,·) is called a reproducing kernel ofWif 1.∀x∈ X, the function K(x,·)∈W. 2.∀x∈ X,∀f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x). If such a K(x,·) exists, Wis called a reproducing kernel Hilbert space (RKHS). As before, /an}bracketle{t·,·/an}bracketri}htWand/bardbl·/bardblWrespectively denote the inner product and norm of W. The subscript Wwill occasionally be omitted. We can think of the elements of Was inﬁnite linear combinations of functions of the form",
    "K(x,·). Also note that /an}bracketle{tK(x,·),K(y,·)/an}bracketri}htW=K(x,y) Since so many of our tools rely on functions being bounded, we’d like to be able to bound the functions in W. We can do this uniformly over x∈ Xif the diagonal K(x,x) is bounded. Proposition: LetWbe a RKHS with PSD Ksuch that supx∈XK(x,x) =kmaxis ﬁnite. Then ∀f∈W, sup|f(x)| ≤ /bardblf/bardblW/radicalbig kmax x∈X . Proof.We rewrite f(x) as an inner product and apply Cauchy-Schwartz. f(x) =/an}bracketle{tf,K(x,·)/an}bracketri}htW≤ /bardblf/bardblW/bardblK(x,·)/bardblW Now/bardblK(x,·)/bardbl2 W=/an}bracketle{tK(x,·),K(x,·)/an}bracketri}htW=K(x,x)≤kmax. The result follows immediately. 11.5.2 Risk Bounds for SVM We now analyze support vector machines (SVM) the same way we analyzed boosting. The general idea is to choose a linear classiﬁer that maximizes the margin (distance to classiﬁers) while minimizing empirical risk. Classes that are not linearly separable can be embedded in a higher dimensional space so that",
    "they are linearly separable. We won’t go into that, however; we’ll just consider the abstract optimization over a RKHS W. Explicitly, we minimize the empirical ϕ-risk over a ball in Wwith radius λ: ˆ ˆ f= min Rn,ϕ(f) f∈W,/bardblf/bardblW≤λ ˆ ˆ ˆ The soft classiﬁer fis then turned into a hard classiﬁer h= sign(f). Typically in SVM ϕ is the hinge loss, though all our convex surrogates behave similarly. To choose W(the only other free parameter), we choose a PSD K(x1,x2) that measures the similarity between two pointsx1andx2. As written, this is an intractable minimum over an inﬁnite dimensional ball {f,/bardblf/bardblW≤ λ}. The minimizers, however, will all be contained in a ﬁnite dimensional subset. Theorem: Representer Theorem. LetWbe a RKHS with PSD Kand letG: nR/ma√sto→Rbe any function. Then minG(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn)) f∈W,/bardblf/bardbl≤λ ∈¯f Wn,/bardblf/bardbl≤λ = min G(gα(x1),...,gα(xn)), α∈Rn,α⊤IKα≤λ2 wheren ¯Wn={f∈W|f(·) =gα(·) =/summationdisplay αiK(xi,",
    "i=1·)} and IK ij=K(xi,xj). Proof. ¯SinceWnis a linear subspace of W, we can decompose any f Wuniquely as ¯⊥¯∈¯ ⊥∈ f=f+fwithf W nandf∈¯W⊥ n. The Pythagorean theorem then gives /bardblf/bardbl2 W=/bardbl¯f/bardbl2 W+/bardblf⊥/bardbl2 W ¯ Moreover, since K(xi,·)∈Wn, f⊥(xi) =/an}bracketle{tf⊥,K(xi,·)/an}bracketri}htW= 0 ¯ Sof(xi) =f(xi) and G(f(x1¯ ¯ ),...,f(xn)) =G(f(x1),...,f(xn)). Because f⊥does not contribute to G, we can remove it from the constraint: ¯ ¯ min G(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn)).¯f∈W,/bardblf/bardbl2+/bardblf⊥/bardbl2≤λ2 f∈ /bardbl¯W, f/bardbl2≤λ2 2¯ Restricting to f∈Wnnow does not change the minimum, which gives us the ﬁrst equality. For the second, we need to show that /bardblgα/bardblW≤λis equivalent to α⊤IKα≤λ2. /bardblgα/bardbl2=/an}bracketle{tgα,gα n/an}bracketri}ht n =/an}bracketle{t/summationdisplay αiK(xi, i=1·), n/summationdisplay αjK(xj, =1·) j/an}bracketri}ht =/summationdisplay αiαj/an}bracketle{tK(xi,( ,j=1·),K xj, i·)/an}bracketri}ht n",
    "=/summationdisplay αiαjK(xi,xj) i,j=1 =α⊤IKα We’ve reduced the inﬁnite dimensional problem to a minimization over α∈nR. This works because we’re only interested in Gevaluated at a ﬁnite set of points. The matrix IK here is a Gram matrix, though we will not not use that. IK should be a measure of the similarity of the points xi. For example, we could have W={/an}bracketle{tx,·/an}bracketri}htRd,x∈dR}withK(x,y the usual inner product K(x,y) =/an}bracketle{tx,y/an}bracketri}htRd. ˆ ˆ We’ve shown that fonly depends on Kthrough IK, but does Rn,ϕdepend on K(x,y) forx,y∈/{xi}? It turns out not to: n n n1ˆRn,ϕ=/summationdisplay 1ϕ(−Yigα(xi)) =/summationdisplay ϕ(−Yi/summationdisplay αjK(xj,xi)).n ni=1 i=1 j=1 The last expression only involves IK. This makes it easy to encode all the knowledge about our problem that we need. The hard classiﬁer is n ˆ ˆ h(x) = sign( f(x)) = sign( gαˆ(x)) = sign(/summationdisplay αˆjK(xj,x)) j=1 If we are given a new point xn+1, we need to compute a new column",
    "for IK. Note that xn+1must be in some way comparable or similar to the previous {xi}for the whole idea of extrapolating from data to make sense. The expensive part of SVMs is calculating the n×nmatrix IK. In some applications, IK may be sparse; this is faster, but still not as fast as deep learning. The minimization over the ellipsoid α⊤IKαrequires quadratic programming, which is also relatively slow. In practice, it’s easier to solve the Lagrangian form of the problem n1αˆ = argmin/summationdisplay ϕ(−Yigα(x′ ⊤i))+λ αIKα α∈Rnni=1 This formulation is equivalent to the constrained one. Note that λandλ′are diﬀerent. SVMs have few tuning parameters and so have less ﬂexibility than other methods. We now turn to analyzing the performance of SVM. 3Theorem: Excess Risk for SVM. Letϕbe anL-Lipschitz convex surrogate and ˆ ˆ Wa RKHS with PSD Ksuch that max x|K(x,x)|=kmax<∞. Lethn,ϕ= signfn,ϕ, ˆwherefn,ϕis the empirical ϕ-risk minimizer over F={f ˆˆˆ∈W./bardblf/bardblW≤λ}(that is,",
    "Rn,ϕ(fn,ϕ)≤Rn,ϕ(f)∀f∈ F). Suppose λ√kmax≤1. Then ˆR(hn,ϕ)−R∗≤2c/parenleftbigg γ γγkmax 2log(2/δ)inf (Rϕ(f)−R∗ ϕ)/parenrightbigg +2c/parenleftBigg 8Lλ + f∈/radicalbigg 2L F n/parenrightBigg 2c/parenleftBigg/radicalbigg n/parenrightBigg with probability 1 −δ. The constants candγare those from Zhang’s lemma. For the hinge loss, c=1 2andγ= 1. Proof.The ﬁrst term comes from optimizing over a restricted set Finstead of all classiﬁers. The third term comes from applying the bounded diﬀerence inequality. These arise in exactly the same way as they do for boosting, so we will omit the proof for those parts. For the middle term, we need to show that Rn,ϕ(F)≤λkmax n. First,|f(x)| ≤ /bardblf/bardblW√kmax≤λ√kmax≤1 for all/radicalBig f∈ F, so we can use the contraction inequality to replace Rn,ϕ(F) withRn(F). Next we’ll expand f(xi) inside the Rademacher complexity and bound inner products using Cauchy-Schwartz. n1Rn(F) = sup Esup σif(xi) x1,...,xn/bracketleftBigg f∈F/vextendsingle/vextendsingle",
    "/vextendsingle ni=1/vextendsingle/vextendsingle/bracketrightBigg/summationdisplay /vextendsingle/vextendsingle /vextendsingle n1=supE/bracketleftBigg sup/vextendsingle/vextendsingle /vextendsingle/summationdisplay σ/vextendsingle iK(x/vextendsingle /an}bracketle{ti,·),fnx1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg n1/vextendsingle =supE/vextendsingle/vextendsingle /bracketleftBigg sup/vextendsingle/vextendsingle/vextendsingle/an}bracketle{t/summationdisplay σiK(xi,·),f/vextendsingle n/vextendsingle x1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg /vextendsingle λ/vextendsingle/vextendsingle /vextendsingle ≤sup/radicaltp /radicalvertex/radicalvertex /radicalbtE/bracketleftBiggn /bardbl/summationdisplay σiK(x2i,/vextendsingle nx1,...,xni=1·)/bardblW/bracketrightBigg Now, n n n 2E/bracketleftBigg /bardbl/summationdisplay σiK(xi,·)/bardblW/bracketrightBigg =E /an}bracketle{t/summationdisplay σiK(xi,·),/summationdisplay σjK(xj,",
    "i=1 i=1 j=1·)/an}bracketri}htW n =/summationdisplay /an}bracketle{tK(x E i,·),K(xj,·)/an}bracketri}ht[σiσj] i,j=1 n = i/summationdisplay K(xi,xj)δij ,j=1 ≤nkmax SoRn(F)≤λkmax nandwearedonewiththenewpartsoftheproof. Theremainderfollows as with boosti/radicalBig ng, using symmetrization, contraction, the bounded diﬀerence inequality, and Zhang’s lemma. 4MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_11.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 11 Scribe: Kevin Li Oct. 14, 2015 2. CONVEX OPTIMIZATION FOR MACHINE LEARNING In this lecture, we will cover the basics of convex optimization as it applies to machine learning. There is much more to this topic than will be covered in this class so you may beinterested in the following books. Convex Optimization by Boyd and Vandenberghe Lecture notes on Convex Optimization by Nesterov Convex Optimization: Algorithms and Complexity by Bubeck Online Convex Optimization by Hazan The last two are drafts and can be obtained online. 2.1 Convex Problems A convex problem is an optimization problem of the form min f(x) wherefand are x2CC convex. First, we will debunk the idea that convex problems are easy by showing that virtually all optimization problems can be written as a convex problem. We can rewrite anoptimization problem as follows. minf(x), mint, mint X2X t\u0015f(x);x2X (x;t)2epi(f ) where the epigraph of a",
    "function is de ned by epi(f)=f(x;t )2X\u0002 IR :t\u0015f(x)g , , f 2 \u0002 \u0015 g Figure 1: An example of an epigraph. Source: https://en.wikipedia.org/wiki/Epigraph_(mathematics) 1Now we observe that for linear functions, minc>x= min c>x x2D x2conv (D) where the convex hull is de ned N N conv(D) =fy:9N2Z+;x1;:::;xN2D; i\u00150;X i= 1;y = i=1X ixi i=1g To prove this, we know that the left side is a least as big as the right side since D\u001aconv(D). For the other direction, we have N minc>x= min min min c> ixix2conv (D) N x 1;:::;x N2D 1;:::; N NX i=1 = min min min ic>ximinc>x N x 1;:::;x N2D 1;:::; NX =1\u0015 x2Di N \u0015min min min iminc>x N x 1;:::;x N2D 1;:::; NX xi=12D = minc>x x2D Therefore we have minf(x) min x2X, t (x;t)2conv (epi(f )) which is a convex problem. Why do we want convexity? As we will show, convexity allows us to infer global infor- mation from local information. First, we must de ne the notion of subgradient . De nition (Subgradient): LetC\u001a I Rd,f:C! I R. A vector g2I Rdis called a subgradient",
    "offatx2Cif f(x)\u0000f(y)\u0014g>(x\u0000y)8y2C: The set of such vectors gis denoted by @f(x). Subgradients essentially correspond to gradients but unlike gradients, they always ex- ist for convex functions, even when they are not di erentiable as illustrated by the next theorem. Theorem: Iff:C ! I R is convex, then for all x,@f(x) =;. In addition, if fis di erentiable at x, then@f(x) =frf(x)g. Proof. Omitted. Requires separating hyperplanes for convex sets. 26f 9 2 2 \u0015 g \u001a \u0015 \u0015 , \u001a! 2 2 \u0000 \u0014 \u0000 2 ! 6=; fr g8 6Theorem: Letf;Cbe convex. If xis a local minimum of fonC, then it is also global minimum. Furthermore this happens if and only if 0 2@f(x). Proof. 02@f(x) if and only if f(x)−f(y)\u00140 for ally2C. This is clearly equivalent to xbeing a global minimizer. Next assume\u0010xis a local minimum. Then for all y2Cthere exists \"small enough such thatf(x)\u0014f(1−\")x+\"y\u0011 \u0014(1−\")f(x)+\"f(y)=)f(x)\u0014f(y) for ally2C. Not only do we know that local minimums are global minimums, looking at the subgra- dient also tells us",
    "where the minimum can be. If g>(x−y)<0 thenf(x)<f(y). This meansf(y) cannot possibly be a minimum so we can narrow our search to ys such that g>(x−y). In one dimension, this corresponds to the half line fy2IR :y\u0014xgifg>0 and the half linefy2IR :y\u0015xgifg<0 . This concept leads to the idea of gradient descent. 2.2 Gradient Descent y\u0019xandfdi erentiable the rst order Taylor expansion of fatxyieldsf(y)\u0019f(x)+ g>(y−x). This means that minf(x+\"\u0016^)\u0019minf(x)+g>(\"\u0016^) j\u0016^j2=1 gwhich is minimized at \u0016^=− . Therefore to minimizes the linear approximation of fatjgj2x, one should move in direction opposite to the gradient. Gradient descent is an algorithm that produces a sequence of points fxjgj\u00151such that (hopefully) f(xj+1)<f(xj). 2 2 \u0014 2 2 \u0014 \u0014 ) \u0014 2 f 2 \u0014 g f 2 \u0015 g \u0019 \u0019 \u0019 f g Figure 2: Example where the subgradient of x1is a singleton and and the subgradient of x2contains multiple elements. Source: https://optimization.mccormick.northwestern.edu/index.php/ Subgradient_optimization 3Algorithm 1",
    "Gradient Descent algorithm Input:x12C, positive sequence f\u0011sgs\u00151 fors= 1 tok\u00001do xs+1=xs\u0000\u0011sgs; gs2@f(xs) end for k1return Eitherx\u0016 =kX xsorx\u000e s=12argminf(x) x2fx 1;:::;x kg Theorem: Letfbe a convex L-Lipschitz function on I Rdsuch thatx\u00032argminI Rdf(x) exists. Assume that jx1\u0000x\u0003j2\u0014R. Then if\u0011Rs=\u0011=Lpfor allsk\u00151, then k1 LRf(kX xs) s=1\u0000f(x\u0003)\u0014p k andLRminf(xs) 1s k\u0000f(x\u0003)\u0014p \u0014\u0014 k Proof. Using the fact that gs=1(x2s+1 +\u0011\u0000xs) and the equality 2a>b=kak kbk2\u0000ka\u0000bk2, 1f(xs)\u0000f(x\u0003)\u0014gs>(xs\u0000x\u0003) = (xs\u0011\u0000xs+1)>(xs\u0000x\u0003) 1=x2sxs+1 +x x\u00032x2s s+1x\u0003 2\u0011h k \u0000 k k \u0000 k \u0000k \u0000 k \u0011 1i =2kg2sk+ (\u000e2 2\u0011s\u0000\u000e2 s+1) where we have de ned \u000es=kxs\u0000x\u0003k. Using the Lipschitz condition \u0011 1f(xs)\u0000f(x\u0003)\u0014L2+ (\u000e2 2 2\u0011s\u0000\u000e2 s+1) Taking the average from 1, to kwe get k1X \u0011 \u0011 1 R2 f(xs)f(x\u0003)\u0014L2 1 \u0011\u0000 + (\u000e2 k1\u0000\u000e2 ks)\u0011\u0014L2 +1 +\u000e2 2 2 2k\u00111\u0014L2+2 2 2k\u0011s=1 Taking\u0011=R Lpto minimize the expression, we obtaink k1 kX LRf(xs) s=1\u0000f(x\u0003)\u0014p k k Noticing that the left-hand side of the inequality is larger than both f(Pxs)\u0000f(x\u0003) by s=1 Jensen's inequality",
    "and min f(xs) 1\u0014s\u0014k\u0000f(x\u0003) respectively, completes the proof. 42 f g \u0000 \u0000 2 2 2 j \u0000 j \u0014 \u0015 \u0000 \u0014 \u0000 \u0014p \u0000 k k kk k k \u0000 \u0014 \u0000 \u0000 \u0000 k \u0000 k k \u0000 k \u0000k \u0000 k k k \u0000 k \u0000 k \u0000 \u0014 \u0000 \u0000 \u0014 \u0000 \u0014 \u0014 \u0000 \u0014p \u0000 \u0000p \u0000 \u0000One aw with this theorem is that the step size depends on k. We would rather have step sizes\u0011sthat does not depend on kso the inequalities hold for all k. With the new step sizes, XkXk k k\u00112 s21[X R2 \u0011sf(x)\u0000f x\u0003)]\u0014L \u000e2 L s ( + (\u000e2 s\u0000s+1)\u00112+2 2s2 2s=1 s=1 s=1\u0014\u0010X s=1\u0011 After dividing byPk Ps=1\u0011s, wePwould like the right-hand side to approach 0. For this to \u00112happen we need Ps!0 and\u0011s!1. One candidate for the step size is \u0011s=G \u0011spsinces k thenP k \u00112 s\u0014c1G2log(k ) and\u0011sc2Gp k. So we get s=1 sP =1\u0015 \u0010Xkc1\u0011s\u0011k\u00001X GLlogk R2 \u0011s[f(xs)f(x\u0003)] 2c2p + k 2c2Gp ks=1 s=1\u0000 \u0014 ChoosingGappropriately, the right-hand side approaches 0 at the rate of LRlogk. Noticepk that we get an extra factor of log k. However, if we look at the sum from k=q 2 tokinstead k of 1 tok,P k \u00112 s\u0014c0 1G2andP\u0011s\u0015c0 2Gp k. Now we have =k s=1 s2 k k\u00001",
    "cLRminf(xs)f(x\u0003) minf(xs)f(x\u0003)\u0011s\u0011s[f(xs)f(x\u0003)] 1\u0014s\u0014k\u0000 \u0014 ks k2\u0000 \u0014\u0010X k\u0011X k\u0000 \u0014p \u0014\u0014 ks= s=2 2 which is the same rate as in the theorem and the step sizes are independent of k. Important Remark: Note this rate only holds if we can ensure that jxk=2\u0000x\u0003j2\u0014R since we have replaced x1byxk=2in the telescoping sum. In general, this is not true for gradient descent, but it will be true for projected gradient descent in the next lecture. One nal remark is that the dimension ddoes not appear anywhere in the proof. How- ever, the dimension does have an e ect because for larger dimensions, the conditions fis L-Lipschitz andjx1\u0000x\u0003j2\u0014Rare stronger conditions in higher dimensions. 5\u0000 \u0014 \u0000 \u0014 !!1 \u0014 \u0015p \u0000 \u0014pp p \u0014 \u0015p \u0000 \u0014 \u0000 \u0014 \u0000 \u0014p j \u0000 j \u0014 j j \u0014 \u0000MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_12.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 12 Scribe:Michael Traub Oct. 19, 2015 2.3 Projected Gradient Descent In the original gradient descent formulation, we hope to optimize min xf(x) where nC Ca d ∈ fare convex, but we did not constrain the intermediate xk. Projected gradient descent will incorporate this condition. 2.3.1 Projection onto Closed Convex Set First we must establish that it is possible to always be able to keep xkin the convex set C. One approach is to take the closest point π(xk)∈ C. Deﬁnition: LetCbe a closed convex subset of IRd. Then∀x∈IRd, letπ(x)∈ Cbe the minimizer of /ba∇dblx−π(x)/ba∇dbl= minx z z∈C/ba∇dbl − /ba∇dbl where/ba∇dbl·/ba∇dbldenotes the Euclidean norm. Then π(x) is unique and, /an}b∇acketle{tπ(x)−x,π(x)−z/an}b∇acket∇i}ht ≤0∀z∈ C (2.1) Proof.From the deﬁnition of π:=π(x), we have /ba∇dblx−π/ba∇dbl2≤ /ba∇dblx−v/ba∇dbl2for anyv∈ C. Fix w∈ Cand deﬁne v= (1−t)π+twfort∈(0,1]. Observe that since Cis convex we have v∈ Cso",
    "that /ba∇dbl − /ba∇dbl2≤ /ba∇dbl − /ba∇dbl2 2x π x v =/ba∇dblx−π−t(w−π)/ba∇dbl Expanding the right-hand side yields /ba∇dbl2 2 2x−π/ba∇dbl ≤ /ba∇dblx−π/ba∇dbl −2t/an}b∇acketle{tx−π,w−π/an}b∇acket∇i}ht+t2/ba∇dblw−π/ba∇dbl This is equivalent to /an}b∇acketle{tx−π,w− /an}b∇acket∇i}ht ≤2π t/ba∇dblw−π/ba∇dbl Since this is valid for all t∈(0,1), letting t→0 yields (2.1). Proof of Uniqueness. Assumeπ1,π2∈ Csatisfy /an}b∇acketle{tπ1−x,π1−z/an}b∇acket∇i}ht ≤0∀z∈C /an}b∇acketle{tπ2−x,π2−z/an}b∇acket∇i}ht ≤0∀z∈C Takingz=π2in the ﬁrst inequality and z=π1in the second, we get /an}b∇acketle{tπ1−x,π1−π2/an}b∇acket∇i}ht ≤0 /an}b∇acketle{tx−π2,π1−π2/an}b∇acket∇i}ht ≤0 Adding these two inequalities yields /ba∇dblπ1−π2/ba∇dbl2≤0 so that π1=π2. 12.3.2 Projected Gradient Descent Algorithm 1 Projected Gradient Descent algorithm Input:x1∈ C, positive sequence {ηs}s≥1 fors= 1 tok−1do ys+1=xs−ηsgs, gs∈∂f(xs) xs+1=π(ys+1) end for k1return Eitherx¯ =/summationdisplay xsorx◦∈argmin f(x)kxs={x1,...,x1∈ k}",
    "Theorem: LetCbe a closed, nonempty convex subset of IRdsuch that diam( C)≤R. Letfbe a convex L-Lipschitz function on RCsuch that x∗∈argminxf(x) exists.∈C Then ifηs≡η=L√thenk LR LRf(x¯)−f(x∗)≤√andf(x¯◦)−f(x∗) k≤√ k Moreover, if ηs=R√, then∃c >0 such thatL s LR LRf(x¯)−f(x∗)≤c√andf(x¯◦)f(x∗) k− ≤ c√ k Proof.Again we will use the identity that 2 a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbl2a−b/ba∇dbl. By convexity, we have f(xs)−f(x∗)≤gs⊤(xs−x∗) 1= (xs−ys+1)⊤(xsη−x∗) 1=2η/bracketleftBig /ba∇dbl2xs−ys+1/ba∇dbl+/ba∇dblxs−x∗/ba∇dbl2−/ba∇dblys+1−x∗/ba∇dbl2/bracketrightBig Next, /ba∇dblys+1−x∗/ba∇dbl2=/ba∇dbl2ys+1−xs+1/ba∇dbl+/ba∇dblxs+1−x∗/ba∇dbl2+2/an}b∇acketle{tys+1 2−xs+1,xs+1−x∗/an}b∇acket∇i}ht =/ba∇dblys+1−xs+1/ba∇dbl+/ba∇dbl2xs+1−x∗/ba∇dbl+2/an}b∇acketle{tys+1−π(ys+1),π(ys+1)−x∗/an}b∇acket∇i}ht ≥ /ba∇dblxs+1−x∗/ba∇dbl2 where we used that /an}b∇acketle{tx−π(x),π(x)−z/an}b∇acket∇i}ht ≥0∀z∈ C, andx∗ 2 2∈ C. Also notice that /ba∇dblxs−2y2 2s+1/ba∇dbl=η/ba∇dblgs/ba∇dbl ≤η",
    "LsincefisL-Lipschitz with respect to /ba∇dbl·/ba∇dbl. Using this we ﬁnd k k1 k/summationdisplay 1 1( )−(∗)≤/summationdisplay2 2+∗2 ∗2f xsf x η L x sx x s+1xk2ηs=1 s=1/bracketleftBig /ba∇dbl − /ba∇dbl −/ba∇dbl − /ba∇dbl/bracketrightBig ηL212ηL2R2 ≤+x2k/ba∇dbl1−x∗ 2η/ba∇dbl ≤ +2 2ηk 22 2Minimizing over ηwe getL=R 2=⇒η=R√, completing the proof22η k L k RLf(x¯)−f(x∗)≤√ k 2 Moreover, the proof of the bound for f(/summationtextk kxs)−f(x∗) is identical because xkxs=/vextenddouble/vextenddouble 2−∗ 2/vextenddouble/vextenddouble≤ R2as well./vextenddouble /vextenddouble 2.3.3 Examples Support Vector Machines The SVM minimization as we have shown before is n1minn/summationdisplay max(0,1Yifα(Xi)) αR ⊤∈In ≤2i=1− αIKα C wherefα(Xi) =α⊤IKei=/summationtextn =1αjK(X ,j jXi). Forconvenience, call gi(α) = max(0 ,1−Yifα(Xi)). In this case executing the projection onto the ellipsoid {α:α⊤IKα≤C2}is not too hard, but we do not know about C,R, orL. We must determine these we can know that our bound is not",
    "exponential with respect to n. First we ﬁnd Land start with the gradient of gi(α): ∇gi(α) = 1I(1−Yifα(Xi)≥0)YiIKei ˆ With this we bound the gradient of the ϕ-riskRn,ϕ(fα) =1 n n n/summationtextn =1gi(αi). /vextenddouble/vextenddouble∂ 1 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble/vextenddouble/summationdisplay/vextenddouble=/vextenddouble/vextenddouble/vextenddouble∇gi(α)∂α/vextenddouble/vextenddoublen/vextenddouble /vextenddoublei=1/vextenddouble/vextenddouble≤/summationdisplay IKe/vextenddouble /vextenddouble in2 i=1/ba∇dbl /ba∇dbl by the triangle inequality and the fact that that 1I(1/vextenddouble −Yifα(Xi)≥0)Yi≤1. We can now use the properties of our kernel K. Notice that /ba∇dblIKei 1 2/ba∇dblis theℓ2norm of the ithcolumn so /ba∇dblIKei/ba∇dbln 2=/parenleftBig/summationtext j=1K(Xj,Xi)2/parenrightBig . We also know that K(Xj,Xi)2=/an}b∇acketle{tK(X2j,·),K(Xi,·)/an}b∇acket∇i}ht ≤ /ba∇dblK(Xj,·)/ba∇dblKH/ba∇dbl(Xi,·)/ba∇dblH≤kmax Combining all of these we get 1/vextenddouble n",
    "n2/vextenddouble∂ 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble ≤max /summationdisplay /summationdisplay/vextenddouble/vextenddoublek2=kmax√n=L/vextenddouble∂α/vextenddoubleni=1j=1 To ﬁndRwe try to evaluate diam {α⊤IKα≤C2}= 2 max√ α α⊤⊤α. We can use the IKα≤C2 condition to put bounds on the diameter C2 2C≥α⊤IKα≥λmin(IK)α⊤α=⇒diam{α⊤IKα≤C2} ≤/radicalbig λmin(IK) We need to understand how small λmincan get. While it is true that these exist random samples selected by an adversary that make λmin= 0, we will consider a random sample of 3i.i.dX1,...,X n∼ N(0,Id). This we can write these d-dimensional samples as a d×nmatrix X. We can rewrite the matrix IK with entries IK ij=K(Xi,Xj) =/an}b∇acketle{tXi,Xj/an}b∇acket∇i}htIRdas a Wishart matrix IK = X⊤X(in particular,1Xd⊤Xis Wishart). Using results from random matrix theory, if we take n,d→ ∞but holdnas a constant γ, thenλ(IK 2min) (d→1√−γ) . Takingd an approximation since we cannot take n,dto inﬁnity, we get λmin(IK)≃d/parenleftbiggn",
    "d1−2/radicalbigg d/parenrightbigg ≥2 using the fact that d≫n. This means that λminbecoming too small is not a problem when we model our samples as coming from multivariate Gaussians. Now we turn our focus to the number of iterations k. Looking at our bound on the excess risk nRˆn,ϕ(f ˆα◦ R)≤minRn,ϕ(fα)+C/radicalbigg kmax α⊤IKα≤C2 kλmin(IK) we notice that our all of the constants in our stochastic term can be computed given the number of points and the kernel. Since statistical error is often √1, to be generous we wantn to have precision up to1 nto allow for fast rates in special cases. This gives us n3k2C2 k≥max λmin(IK) which is not bad since nis often not very big. In [Bub15], the rates for many a wide rage of problems with various assumptions are available. For example, if we assume strong convexity and Lipschitz we can get an exponen- tial rate so k∼logn. If gradient is Lipschitz, then we get get1 kinstead of √1in the bound.k However, often times we are not optimizing over",
    "functions with these nice properties. Boosting We already know that ϕisL-Lipschitz for boosting because we required it before. Remember that our optimization problem is n1min/summationdisplay ϕ(−Yifα(Xi)) αRNn |α∈I |1≤i=11 wherefα=N j=1αjfjandfjis thejthweak classiﬁer. Remember before we had some rate like/radicalBig logNc/summationtext nand we would hope to get some other rate that grows with log NsinceNcan be very large. Taking the gradient of the ϕ-loss in this case we ﬁnd N1∇Rˆn,ϕ(fα) =/summationdisplay ϕ′(−Yifα(Xi))(−Yi)F(Xi)ni=1 whereF(x) is the column vector [ f1(x),...,fN(x)]⊤. Since|Yi| ≤1 andϕ′≤L, we can bound the ℓ2norm of the gradient as /vextenddouble nL /vextenddouble∇Rˆ/vextenddoublen,ϕ(fα)/vextenddouble/vextenddouble/vextenddouble 2≤n/vextenddouble /vextenddouble/vextenddouble/vextenddouble/summationdisplay F(X/vextenddouble i)/vextenddoublei=1/vextenddouble n/vextenddouble L/vextenddouble/vextenddouble ≤n/summationdisplay ) i=/ba∇dblF(Xi 1/ba∇dbl ≤L√ N 4using triangle",
    "inequality and the fact that F(Xi) is aN-dimensional vector with each component bounded in absolute value by 1. Using the fact th√at the diameter of the ℓ1ball is 2, R= 2 and the Lipschitz associated with our ϕ-risk isL NwhereLis the Lipschitz constant for ϕ. Our stochastic termR√L k becomes 2 L/radicalBig N k. Imposing the same1 nerror as before we ﬁnd that k∼N2n, which is very bad especially since we want log N. 2.4 Mirror Descent Boosting is an example of when we want to do gradient descent on a non-Euclidean space, in particular a ℓ1space. While the dual of the ℓ2-norm is itself, the dual of the ℓ1norm is theℓor sup norm. We want this appear if we have an ℓ1constraint. The reason for this ∞ is not intuitive because we are taking about measures on the same space IRd, but when we consider optimizations on other spaces we want a procedure that does is not indiﬀerent to the measure we use. Mirror descent accomplishes this. 2.4.1 Bregman Projections Deﬁnition: If/ba∇dbl·/ba∇dblis some",
    "norm on IRd, then/ba∇dbl·/ba∇dblis its dual norm.∗ Example: If dual norm of the ℓpnorm/ba∇dbl·/ba∇dblpis theℓqnorm/ba∇dbl·/ba∇dblq, then1 p+1 q= 1. This is the limiting case of H¨ older’s inequality. In general we can also reﬁne our bounds on inner products in IRdtox⊤y≤ /ba∇dblx/ba∇dbl/ba∇dbly/ba∇dblif∗ we consider xto be the primal and yto be the dual. Thinking like this, gradients live in the dual space, e.g. in gs⊤(x−x∗),x−x∗is in the primal space, so gsis in the dual. The transpose of the vectors suggest that these vectors come from spaces with diﬀerent measure, even though all the vectors are in IRd. Deﬁnition: Convex function Φ on a convex set Dis said to be (i) L-Lipschitz with respect to /ba∇dbl·/ba∇dblif/ba∇dblg/ba∇dbl∗≤L∀g∈∂Φ(x)∀x∈D (ii)α-strongly convex with respect to /ba∇dbl·/ba∇dblif αΦ(y)≥Φ(x)+g⊤(y−x)+2/ba∇dbly−x/ba∇dbl2 for allx,y∈Dand forg∈∂f(x) Example: If Φ is twice diﬀerentiable with Hessian Hand/ba∇dbl·/ba∇dblis theℓ2norm, then all eig(H)≥α. Deﬁnition (Bregman",
    "divergence): For a given convex function Φ on a convex set Dwithx,y∈ D, the Bregman divergence of yfromxis deﬁned as DΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)⊤(y−x) 5This divergence is the error of the function Φ( y) from the linear approximation at x. Also note that this quantity is not symmetric with respect to xandy. If Φ is convex then DΦ(y,x)≥0 because the Hessian is positive semi-deﬁnite. If Φ is α-strongly convex then DΦ(y,x)≥α 2/ba∇dbly−x/ba∇dbl2and if the quadratic approximation is good then this approximately holds in equality and this divergence behaves like Euclidean norm. Proposition: Given convex function Φ on Dwithx,y,z∈ D (∇Φ(x)−∇Φ(y))⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y) Proof.Looking at the right hand side = Φ(x)−Φ(y)−∇Φ(y)⊤(x−y)+Φ(z)−Φ(x)−∇Φ(x)⊤(z−x) −/bracketleftBig Φ(z)−Φ(y)−∇Φ(y)⊤(z−y)/bracketrightBig =∇Φ(y)⊤(y−x+z−y)−∇Φ(x)⊤(z−x) = (∇Φ(x)−∇Φ(y))⊤(x−z) Deﬁnition (Bregman projection): Givenx∈IRd, Φaconvex diﬀerentiablefunction onD ⊂ D¯ IRdand convex C⊂, the Bregman projection of xwith",
    "respect to Φ is πΦ(x)∈argminDφ(x,z) z∈C References [Bub15] S´ ebastien Bubeck, Convex optimization: algorithms and complexity , Now Publish- ers Inc., 2015. 6MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_13.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 13 Scribe:Mina Karzand Oct. 21, 2015 Previously, we analyzed the convergence of the projected gradient descent algorithm. We proved that optimizing the convex L-Lipschitz function fon a closed, convex set Cwith diam(C)≤Rwith step sizes ηs=R√would give us accuracy of f(x)≤f(x∗)+LR L k√afterk kiterations. Although it might seem that projected gradient descent algorithm provides dimension- free convergence rate, it is not always true. Reviewing the proof of convergence rate, we realize that dimension-free convergence is possible when the objective function fand the constraint set Care well-behaved in Euclidean norm (i.e., for all x∈ Candg∈∂f(x), we have that |x|2and|g|2are independent of the ambient dimension). We provide an examples of the cases that these assumptions are not satisﬁed. •Consider the diﬀerentiable, convex function fon the Euclidean ball B2,nsuch that /ba∇dbl∇f(x)/ba∇dbl ≤1,∀x∈B2,n. This implies",
    "that f(x)√nand the projected ∞ |∇ | 2≤ gradient descent converges to the minimum of finBn2,/radicalignat ratek. Using the log(n)method of mirror descent we can get convergence rate of/radicalbig k To get better rates of convergence in the optimization problem, we can use the Mirror Descent algorithm. The idea is to change the Euclidean geometry to a more pertinent geometry to a problem at hand. We will deﬁne a new geometry by using a function which is sometimes called potential function Φ( x). We will use Bregman projection based on Bregman divergence to deﬁne this geometry. The geometric intuition behind the mirror Descent algorithm is the following: The projected gradient described in previous lecture works in any arbitrary Hilbert space Hso that thenormof vectors isassociated withaninnerproduct. Now, supposeweareinterested in optimization in a Banach space D. In other words, the norm (or the measure of distance) that we use does not derive from an inner product. In this case, the",
    "gradient descent does not even make sense since the gradient ∇f(x) are elements of dual space. Thus, the term x−η∇f(x) cannot be performed. (Note that in Hilbert space used in projected gradient descent, the dual space of His isometric to H. Thus, we didn’t have any such problems.) The geometric insight of the Mirror Descent algorithm is that to perform the optimiza- tion in the primal space D, one can ﬁrst map the point x∈ Din primal space to the dual spaceD∗, then perform the gradient update in the dual space and ﬁnally map the optimal point back to the primal space. Note that at each update step, the new point in the primal spaceDmight be outside of the constraint set C ⊂ D, in which case it should be projected into the constraint set C. The projection associate with the Mirror Descent algorithm is Bergman Projection deﬁned based on the notion of Bergman divergence. Deﬁnition (Bregman Divergence): Forgivendiﬀerentiable, α-stronglyconvexfunc- tion Φ(x) :D →R, we deﬁne the Bregman",
    "divergence associated with Φ to be: DΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)T(y−x) 1We will usetheconvex openset D ⊂nRwhoseclosure contains theconstraint set C ⊂ D. Bregman divergence is the error term of the ﬁrst order Taylor expansion of the function Φ inD. Also, note that the function Φ( x) is said to be α-strongly convex w.r.t. a norm /ba∇dbl./ba∇dblif Φ(y)−Φ(x)−∇Φ(x)T α(y−x)≥2/ba∇dbly−x/ba∇dbl2. We used the following property of the Euclidean norm: 2a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbla−b/ba∇dbl2 in the proof of convergence of projected gradient descent, where we chose a=xs−ys+1and b=xs−x∗. To prove the convergence of the Mirror descent algorithm, we use the following property oftheBregmandivergence inasimilarfashion. Thispropositionshowsthat theBregmandi- vergenceessentially behaves astheEuclideannormsquaredintermsof projections: Proposition: Givenα-strongly diﬀerentiable convex function Φ : D →R, for all x,y,z∈ D, [∇Φ(x)−∇Φ(y)]⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y). As described previously,",
    "the Bregman divergence is used in each step of the Mirror descent algorithm to project the updated value into the constraint set. Deﬁnition (Bregman Projection): Givenα-strongly diﬀerentiable convex function Φ :D →Rand for all x∈ Dand closed convex set C ⊂D ΠΦ(x) = argmin DC Φ(z,x) z∈C∩D 2.4.2 Mirror Descent Algorithm Algorithm 1 Mirror Descent algorithm Input:x1∈argmin Φ( x),ζ:d dR R such that ζ(x) = Φ(x)C∩D → ∇ fors= 1,···,kdo ζ(ys+1) =ζ(xs)−ηgsforgs∈∂f(xs) xs+1= ΠΦ(yCs+1) end for return Eitherx=1 k/summationtextk s=1xsorx◦∈argminx x1, ,xkf(x)∈{ ··· } Proposition: Letz∈ C ∩D, then∀y∈ D, (∇Φ(π(y)−∇Φ(y))⊤(π(y)−z)≤0 2Moreover, DΦ(z,π(y))≤DΦ(z,y). Proof.Deﬁneπ= ΠΦ(y) andh(t) =DΦ(π+t(z−π),y).Sinceh(t) is minimized at t= 0C (due to the deﬁnition of projection), we have h′(0) =∇xDΦ(x,y)|x=π(z−π)≥0 where suing the deﬁnition of Bregman divergence, ∇xDΦ(x,y) =∇Φ(x)−∇Φ(y) Thus, (∇Φ(π)−∇Φ(y))⊤(π−z)≤0. Using proposition 1, we know that (∇Φ(π)−∇Φ(y))⊤(π−z) =DΦ(π,y)+DΦ(z,π)−DΦ(z,y)≤0, and since",
    "DΦ(π,y)≥0, we would have DΦ(z,π)≤DΦ(z,y). Theorem: Assume that fis convex and L-Lipschitz w.r.t. /ba∇dbl./ba∇dbl. Assume that Φ is α-strongly convex on C ∩ Dw.r.t./ba∇dbl./ba∇dbland R2= sup Φ( x) m x−in Φ(x) ∈C∩D x∈C∩D ta/radicaligkex1= argminxΦ(x) (assume that it exists). Then, Mirror Descent with η=∈C∩D R2α L Rgives, 2 2f(x)−f(x∗)≤RL/radicalbigg andf(x◦)αk−f(x∗)≤RL/radicalbigg ,αk Proof.Takex♯∈ C ∩D. Similar to the proof of the projected gradient descent, we have: (i) f(xs)−f(x♯)≤gs⊤(xs−x♯) (ii)1=(ζ(x♯s)−ζ(ys+1))⊤(xs)η−x (iii)1=( Φ(xs) Φ(ys+1))⊤(xsx♯)η∇ −∇ − (iv)1=/bracketleftig D♯Φ(xs,ys+1)+D♯Φ(x ,xs)η−DΦ(x ,ys+1)/bracketrightig (v)1≤/bracketleftig DΦ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x♯,xs+1)η/bracketrightig (vi)ηL21≤+/bracketleftig DΦ(x♯,xs)2α2η−DΦ(x♯,xs+1)/bracketrightig Where (i) is due to convexity of the function f. 3Equations (ii) and (iii) are direct results of Mirror descent algorithm. Equation (iv) is the result of applying proposition 1. Inequality (v) is a result of the fact",
    "that x= ΠΦ ♯s+1 (yCs+1), thus for x ♯ ♯∈ C ∩ D, we have DΦ(x ,ys+1)≥DΦ(x ,xs+1). We will justify the following derivations to prove inequality (vi): (a)DΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1) (b) α≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl (c) α≤η/ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbl−2/ba∇dblys+1−xs/ba∇dbl2 (d)η2L2 ≤.2α Equation (a) is the deﬁnition of Bregman divergence. To show inequality (b), we used the fact that Φ is α-strongly convex which implies that Φ(ys+1)−Φ(xs)≥ ∇Φ(xs)T(ys+1−xs)α 2/ba∇dbly2s+1−xs/ba∇dbl. According to the Mirror descent algorithm, ∇Φ(xs)− ∇Φ(ys+1) =ηgs. We use H¨ older’s inequality to show that gs⊤(xs−ys+1)≤ /ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbland derive inequality (c). Lookingatthequadraticterm ax−bx2fora,b >0,itisnothardtoshowthatmax ax a−bx2= 2. We use this statement with x=/ba∇dblys+1−xs/ba∇dbl,a=η gb/ba∇dbls/ba∇dblL4 ∗≤andb=αto derive2 inequality (d). Again, we use telescopic sum to get k1/summationdisplay ηL2D(x♯Φ,x1)[f(xs)f(x♯)] +",
    ". (2.1)k 2α kηs=1− ≤ We use the deﬁnition of Bregman divergence to get DΦ(x♯,x1) = Φ(x♯)−Φ(x1)−∇Φ(x1)(x♯−x1) ≤Φ(x♯)−Φ(x1) ≤sup Φ(x) min Φ( x) x x− ∈C∩D ∈C∩D ≤R2. Where we used the fact x1∈argmin Φ( x) in the description of the Mirror Descent ♯C∩D algorithm to prove ∇Φ(x1)(x−x1)≥0. We optimize the right hand side of equation (2.1) forηto get k1/summationdisplay (x♯ 2[f(xs)−f)]ks=≤RL 1/radicalbigg .αk To conclude the proof, let x♯→x∗∈ C. Note that with the right geometry, we can get projected gradient descent as an instance the Mirror descent algorithm. 42.4.3 Remarks The Mirror Descent is sometimes called Mirror Prox. We can write xs+1as xs+1= argmin DΦ(x,ys+1) x∈C∩D = argminΦ( x) x−∇Φ⊤(ys+1)x ∈C∩D = argminΦ( x) xs x−[∇Φ( )−ηgs]⊤x ∈C∩D = argmin η(gs⊤x)+Φ(x) x−∇Φ⊤(xs)x ∈C∩D = argmin η(gs⊤x)+DΦ(x,xs) x∈C∩D Thus, we have xs+1= argmin η(gs⊤x)+DΦ(x,xs). x∈C∩D To getxs+1, in the ﬁrst term on the right hand side we look at linear approximations close toxsin the direction determined by the",
    "subgradient gs. If the function is linear, we would just look at the linear approximation term. But if the function is not linear, the linear approximation is only valid in a small neighborhood around xs. Thus, we penalized by adding the term DΦ(x,xs). We can penalized by the square norm when we choose DΦ(x,xs) =/ba∇dblx−xs/ba∇dbl2. In this case we get back the projected gradient descent algorithm as an instance of Mirror descent algorithm. But if we choose a diﬀerent divergence DΦ(x,xs), we are changing the geometry and we can penalize diﬀerently in diﬀerent directions depending on the geometry. Thus, using the Mirror descent algorithm, we could replace the 2-norm in projected gradient descent algorithm by another norm, hoping to get less constraining Lipschitz con- stant. On the other hand, the norm is a lower bound on the strong convexity parameter. Thus, there is trade oﬀ in improvement of rate of convergence. 2.4.4 Examples Euclidean Setup: Φ(x) =1x2, =dR, Φ(x) =ζ(x) =x. Thus,",
    "the updates will be similar to2/ba∇dbl /ba∇dbl D ∇ the gradient descent. 1DΦ(y,x) =/ba∇dbly/ba∇dbl21− /ba∇dblx2 2/ba∇dbl2x2−⊤y+/ba∇dblx/ba∇dbl 1=/ba∇dblx−y/ba∇dbl2.2 Thus, Bregman projection with this potential function Φ( x) is the same as the usual Eu- clidean projection and the Mirror descent algorithm is exactly the same as the projected descent algorithm since it has the same update and same projection operator. Note that α= 1 since D1Φ(y,x)≥2/ba∇dblx−y/ba∇dbl2. ℓ1Setup: We look at D=dR+\\{0}. 5Deﬁne Φ( x) to be the negative entropy so that: d Φ(x) =/summationdisplay xilog(xi), ζ(x) =∇Φ(x) ={1+log(xdi) i=1}i=1 (s+1)∇(s)−(s+1)Thus, looking at the update function y= Φ(x)ηgs, we get log( yi) = (s)log(xi)−(s) (s+1) ( s) (s)ηgiand for all i= 1,···,d, we have yi=xiexp(−ηgi). Thus, y(s)=x(s)exp(−ηg(s)). We call this setup exponential Gradient Descent or Mirror Descent with multiplicative weights. The Bregman divergence of this mirror map is given by DΦ(y,x) = Φ(y)−Φ(x)−∇Φ⊤(x)(y−x)",
    "/summationdisplayd /summationdisplayd d =yilog(yi)−xilog(xi)(1+log( xi))(yixi) i 1/summationdisplay i− =1 i− = =1 /summationdisplaydyi=yilog()+xii=1/summationdisplayd (yi i=1−xi) Note that/summationtextd i=1yilog(yi i) is call the Kullback-Leibler divergence (KL-div) between yx andx. We show that the projection with respect to this Bregman divergence on the simplex ∆d={x∈dR:d i=1xi= 1,xi≥0}amounts to a simple renormalization y/ma√sto→y/|y|1. To prove so, we prov/summationtext ide the Lagrangian: /summationdisplayd /summationdisplayd dyLi=yilog()+(xixii=1 i=−yi)+λ( 1/summationdisplay xi i=1−1). To ﬁnd the Bregman projection, for all i= 1,···,dwe write ∂ yL −i= +1+ λ= 0∂xixi Thus, for all i, we have xi=γyi. We know that/summationtextd i=1xi= 1. Thus, γ=1/summationtextyi. Thus, we have ΠΦ y ∆d(y) = 1. The Mirror Descent algorithm with this update and|y| projection would be: ys+1=xsexp(−ηgs) yxs+1=.|y|1 To analyze the rate of convergence, we want to study the ℓ1norm on ∆ d. Thus, we have",
    "to show that for some α, Φ isα-strongly convex w.r.t |·|1on ∆d. 6DΦ(y,x) =KL(y,x)+/summationdisplay (xi i−yi) =KL(y,x) 1≥2|x−y|2 1 Where we used the fact that x,y∈∆dto showi(xi−yi) = 0 and used Pinsker inequality show the result. Thu/summationtexts, Φ is 1-strongly conve/summationtext x w.r.t.|·|1on ∆d. Remembering that Φ( x) =d i=1xilog(xi) was deﬁned to be negative entropy, we know that−log(d)≤Φ(x)≤0 forx∈∆d. Thus, R2= maxΦ( x) x∈∆d−min Φ(x) = log(d). x∈∆d Corollary: Letfbe a convex function on ∆ dsuch that /ba∇dblg/ba∇dbl∞≤L,∀g∈∂f(x),∀x∈∆d. 2log(d)Then, Mirror descent with η=1/radicalig givesL k 2log(d) 2log(d)f(xk)−f(x∗)≤L/radicalbigg , f(x◦ k)−f(x∗)k≤L/radicalbigg k Boosting: For weak classiﬁers f1(x),···,fN(x) andα∈∆n, we deﬁne N f1(x) .fα= /summationdisplay αjfjandF(x) = j=1.. fN(x) so thatfα(x) is the weighted majority vote classiﬁer. Note that |F|∞≤1. As shown before, in boosting, we have: n g=∇R/hatwide1 n,φ(fα) =/summationdisplay φ′(−yifα(xi))(−yi)F(xi),ni=1 Since|F|",
    "≤1 and|y| ≤1, then|g| ≤LwhereLis the Lipschitz constant of φ ∞ ∞ ∞ (e.g., a constant like eor 2). /radicalbigg /hatwide /hatwide2log(N)Rn,φ(fα◦ k)−minRn,φ(fα)L α∈∆n≤k We need the number of iterations k≈n2log(N). The functions fj’s could hit all the vertices. Thus, if we want to ﬁt them in a ball, the /radicaligball has to be radius√ N. This is why the projected gradient descent would give the rate of N k. But by looking at the gradient we can determine the right geometry. In this case, the gradient is bounded by sup-norm which is usually the most constraining norm in projected 7gradient descent. Thus, using Mirror descent would be most beneﬁcial. Other Potential Functions: There are other potential functions which are strongly convex w.r.t ℓ1norm. In partic- ular, for 1Φ(x) =|x|p 1 p, p= 1+p log(d) then Φ is c/radicalbig log(d)-strongly convex w.r.t ℓ1norm. 8MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these",
    "materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_14.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 14 Scribe:Sylvain Carpentier Oct. 26, 2015 In this lecture we will wrap up the study of optimization techniques with stochastic optimization. The tools that we are going to develop will turn out to be very eﬃcient in minimizing the ϕ-risk when we can bound the noise on the gradient. 3. STOCHASTIC OPTIMIZATION 3.1 Stochastic convex optimization Weareconsideringrandomfunctions x/ma√sto→ℓ(x,Z)wherexistheoptimization parameterand Za random variable. Let PZbe the distribution of Zand let us assume that x/ma√sto→ℓ(x,Z) is convexPZa.s. In particular, IE[ ℓ(x,Z)] will also be convex. The goal of stochastic convex optimization is to approach min xIE[ℓ(x,Z)] when is convex. For our purposes, will ∈C C C be a deterministic convex set. However, stochastic convex optimization can be deﬁned more broadly. The constraint can be itself stochastic : C={x,IE[g(x,Z)]≤0}, gconvexPZa.s. C={x,IP[g(x,Z)≤0]≥1−ε},“chance constraint”",
    "The second constraint is not convex a priori but remedies are possible (see [NS06, Nem12]). In the following, we will stick to the case where Xis deterministic. A few optimization problems we tackled can be interpreted in this new framework. 3.1.1 Examples Boosting. Recall that the goal in Boosting is to minimize the ϕ-risk: minIE[ϕ(f ∈Λ−Yα(X))], α where Λ is the simplex of IRd. Deﬁne Z= (X,Y) and the random function ℓ(α,Z) = ϕ(−Yfα(X)), convex PZa.s. Linear regression. Here the goal is the minimize the ℓ2risk: min IE[(Y−f2α(X)) ]. α∈IRd DeﬁneZ= (X,Y) and the random function ℓ(α,Z) = (Y−fα(X))2, convex PZa.s. Maximum likelihood. We consider samples Z1,...,Z niid with density pθ,θ∈Θ. For instance, ZN(θ,1). The likelihood functions associated to this set of samples is θ ∼ /ma√sto→/producttextn =1pθ(Zi). Letp∗(Z) denote the true density of Zi (it does not have to be of the form pθ forsomeθ∈Θ. Then n1/productdisplay/integraldisplayp∗(z)IE[log pθ(Zi)] =−log( ) p∗(z)dz+C=n",
    "pθ(z)i=1−KL(p∗,pθ)+C 1whereCis a constant in θ. Hence maximizing the expected log-likelihood is equivalent to minimizing the expected Kullback-Leibler divergence: n maxIE[log/productdisplay pθ(Zi)] θi=1⇐⇒KL(p∗,pθ) External randomization. Assume that we want to minimize a function of the form 1f(x) =n/summationdisplay fi(x),ni=1 where the functions f1,...,fnare convex. As we have seen, this arises a lot in empirical risk minimization. In this case, we treat this problem as deterministic problem but inject artiﬁcial randomness as follows. Let Ibe a random variable uniformly distributed on [n] =:{1,...,n}. We have the representation f(x) = IE[fI(x)], which falls into the context of stochastic convex optimization with Z=Iandℓ(x,I) =fI(x). Important Remark :There is a key diﬀerence between the case where we assume that wearegivenindependentrandomvariablesandthecasewherewegenerateartiﬁcialran- domness. LetusillustratethisdiﬀerenceforBoosting. Wearegiven( X1,Y1),...,(Xn,Yn) i.i.d from some",
    "unknown distribution. In the ﬁrst example, our aim is to minimize IE[ϕ(−Yfα(X))] based on these nobservations and we will that the stochastic gradient allows to do that by take one pair ( Xi,Yi) in each iteration. In particular, we can use each pair at most once. We say that we do one pass on the data. We could also leverage our statistical analysis of the empirical risk minimizer from previous lectures and try to minimize the empirical ϕ-risk 1Rˆn,ϕ(fα) =n/summationdisplay ϕ(α i=1−Yif(Xi))n by generating kindependent random variables I1,...,Ikuniform over [ n] and run the stochasticgradientdescenttousonerandomvariable Ijineachiteration. Thediﬀerence hereisthat kcanbearbitrarylarge, regardlessofthenumber nofobservations(wemake multiple passes onthedata). However, minimizingIE I[ϕ(−YIfα(XI))|X1,Y1,...,X n,Yn] will perform no better than the empirical risk minimizer whose statistical performance is limited by the number nof observations. 3.2 Stochastic gradient descent If the",
    "distribution of Zwas known, then the function x/ma√sto→IE[ℓ(x,Z)] would be known and we could apply gradient descent, projected gradient descent or any other optimization tool seen before in the deterministic setup. However this is not the case in reality where the true distribution PZis unknown and we are only given the samples Z1,...,Z nand the random function ℓ(x,Z). In what follows, we denote by ∂ℓ(x,Z) the set of subgradients of the function y/ma√sto→ℓ(y,Z) at point x. 2Algorithm 1 Stochastic Gradient Descent algorithm Input:x1∈ C, positive sequence {ηs}s1, independent random variables Z ,...,Z ≥ 1 k with distribution PZ. fors= 1 tok−1do ys+1=xs−ηsg˜s, g˜s∈∂ℓ(xs,Zs) xs+1=π(yCs+1) end for 1return x¯k=k k/summationdisplay xs s=1 Note the diﬀerence here with the deterministic gradient descent which returns either x¯korx◦ k= argmin f(x). In the stochastic framework, the function f(x) = IE[ℓ(x,ξ)] is x1,...,xn typically unknown and x˚kcannot be computed. Theorem: LetCbea closed convex",
    "subset of IRdsuch that diam( C)≤R. Assume that he convex function f(x) = IE[ℓ(x,Z)] attains its minimum on Catx∗∈IRd. Assume thatℓ(x,Z) is convex PZa.s. and that IE /ba∇dblg˜/ba∇dbl2≤L2for allg˜∈∂ℓ(x,Z) for allx. Then ifηs≡η=R L√,k LRIE[f(x¯k)]−f(x∗)≤√ k Proof. f xsf x g sxsx = IE[g˜s⊤(xs−x∗)|xs] 1= IE[(ys+1xs)⊤(xsx∗)xs]η− − | 1=IE[/ba∇dblx2 2s−y2s+1/ba∇dbl+η/ba∇dblxs−x∗ 2/ba∇dbl −/ba∇dblys+1−x∗/ba∇dbl |xs] 1≤(η2IE[/ba∇dblg˜s/ba∇dbl2|xs]+IE[/ba∇dblx2s−x∗/ba∇dbl |xs]−IE[/ba∇dblxs+1−x∗xη/ba∇dbl2 2|s] Taking expectations and summing over swe get k1/summationdisplay ηL2R2 f(xs) ( s=1−f x∗)k≤+.2 2ηk Using Jensen’s inequality and chosing η=R L√, we getk LRIE[f(x¯k)]−f(x∗)≤√ k 3( )−(∗)≤⊤(−∗)3.3 Stochastic Mirror Descent We can also extend the Mirror Descent to a stochastic version as follows. Algorithm 2 Mirror Descent algorithm Input: x1∈argmin Φ( x),ζ:dR→dRsuch that ζ(x) =∇Φ(x), independentC∩D random variables Z1,...,Z kwith distribution PZ. fors= 1,···,kdo ζ(ys+1)",
    "=ζ(xs)−ηg˜sforg˜s∈∂ℓ(xs,Zs) xΦs+1= Π (yCs+1) end for return x=1k k/summationtext s=1xs Theorem: Assume that Φ is α-strongly convex on C ∩ Dw.r.t./ba∇dbl·/ba∇dbland R2= sup Φ( x) Φ x) x−min ( ∈C∩D x∈C∩D takex1= argminxΦ(x) (assume that it exists). Then, Stochastic Mirror Descent∈C∩D withη=R L/radicalBig 2αxRoutputs ¯ k, such that IE[f(x¯k)]−f(x∗)≤RL/radicalbigg 2.αk Proof.We essentially reproduce the proof for the Mirror Descent algorithm. Takex♯∈ C ∩D. We have f(xs ss IE[g˜s⊤(xs−x∗)|xs] 1=IE[(ζ(xs)η−ζ(ys+1))⊤(xs−x♯)|xs] 1=IE[(∇Φ(xs)−∇Φ(ys+1))⊤(xsη−x♯)|xs] 1=IE/bracketleftBig D♯Φ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x ,ys+1)η/vextendsingle/vextendsinglexs/bracketrightBig 1≤IE/bracketleftBig DΦ(x♯s,ys+1)+DΦ(x ,xs)−DΦ(x♯,xs+1)xη/vextendsingle/vextendsingles/bracketrightBig η≤2IE[/ba∇dblg˜s/ba∇dbl21|xs]+ IE2α∗η/bracketleftBig DΦ(x♯,xs)−DΦ(x♯,xs+1)/vextendsingle/vextendsinglexs/bracketrightBig)−f(x♯)≤g⊤(x−x♯) 4where the last inequality comes from DΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1)",
    "α≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl α≤η/ba∇dblg˜s/ba∇dblx y y x2∗/ba∇dbls−s+1/ba∇dbl−2/ba∇dbls+1−s/ba∇dbl η2/ba∇dblg˜≤s/ba∇dbl2 ∗.2α Summing and taking expectations, we get k1/summationdisplay ηL2DΦ(x♯,x1)[f(x♯s) ] s=1≤+k−f(x) . (3.1)2α kη We conclude as in the previous lecture. 3.4 Stochastic coordinate descent Letfbe a convex L-Lipschitz and diﬀerentiable function on IRd. Let us denote by ∇ifthe partial derivative of fin the direction ei. One drawback of the Gradient Descent Algorithm is that at each step one has to update every coordinate ∇ifof the gradient. The idea of the stochastic coordinate descent is to pick at each step a direction ejuniformly and to choose that ejto be the direction of the descent at that step. More precisely, of Iis drawn uniformly on [ d], then IE[ d∇If(x)eI] =∇f(x). Therefore, the vector d∇If(x)eIthat has only one nonzero coordinate is an unbiased estimate of the gradient ∇f(x). We can use this estimate to perform stochastic gradient",
    "descent. Algorithm 3 Stochastic Coordinate Descent algorithm Input: x1∈ C, positive sequence {ηs}s1, independent random variables I ,...,I ≥ 1 k uniform over [ d]. fors= 1 tok−1do ys+1=xs−ηsd∇If(x)eI, g˜s∈∂ℓ(xs,Zs) xs+1=π(ys+1) C end for k1return x¯k=k/summationdisplay xs s=1 If we apply Stochastic Gradient Descent to this problem for η=R/radicalBig 2, we directlyL dk obtain 2dIE[f(x¯k)]−f(x∗)≤RL/radicalbigg k We are in a trade-oﬀ situation where the updates are much easier to implement but where we need more steps to reach the same precision as the gradient descent alogrithm. 5References [Nem12] Arkadi Nemirovski, On safe tractable approximations of chance constraints , Euro- pean J. Oper. Res. 219(2012), no. 3, 707–718. MR 2898951 (2012m:90133) [NS06] Arkadi Nemirovski and Alexander Shapiro, Convex approximations of chance constrained programs , SIAM J. Optim. 17(2006), no. 4, 969–996. MR 2274500 (2007k:90077) 6MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine",
    "Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_15.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 15 Scribe:Zach Izzo Oct. 27, 2015 Part III Online Learning It is often the case that we will be asked to make a sequence of predictions, rather than just one prediction given a large number of data points. In particular, this situa- tion will arise whenever we need to perform online classiﬁcation: at time t, we have (X1,Y1),...,(Xt−1,Yt−1) iid random variables, and given Xt, we are asked to predict Yt∈ {0,1}. Consider the following examples. Online Shortest Path: We have a graph G= (V,E) with two distinguished vertices sandt, and we wish to ﬁnd the shortest path from stot. However, the edge weights E1,...,E tchange with time t. Our observations after time tmay be all of the edge weights E1,...,E t; or our observations may only be the weights of edges through which our path traverses; orourobservationmayonlybethesumoftheweightsoftheedgeswe’vetraversed. Dynamic Pricing: We have a sequence of customers, each of",
    "which places a value vt on some product. Our goal is to set a price ptfor thetth customer, and our reward for doing so is ptifpt≤vt(in which case the customer buys the product at our price) or 0 otherwise (in which case the customer chooses not to buy the product). Our observations after time tmay bev1,...,vt; or, perhaps more realistically, our observations may only be 1I(p1< v1),...,1I(pt< vt). (In this case, we only know whether or not the customer bought the product.) Sequential Investment: GivenNassets, a portfolio is ω∈∆N={x∈IRn:xi≥ 0,/summationtextNxi=1i= 1}. (ωtells what percentage of our funds to invest in each stock. We could also allow for negative weights, which would correspond to shorting a stock.) At each time t, we wish to create a portfolio ωt∈∆Nto maximize ωT tzt, wherezt∈IRNis a random variable which speciﬁes the return of each asset at time t. There are two general modelling approaches we can take: statistical or adversarial. Statistical methods typically require",
    "that the observations are iid, and that we can learn somethingaboutfuturepointsfrompastdata. Forexample, inthedynamicpricingexample, we could assume vt∼N(v,1). Another example is the Markowitz model for the sequential investment example, in which we assume that log( zt)∼ N(µ,Σ). In this lecture, we will focus on adversarial models. We assume that ztcan be any bounded sequence of numbers, and we will compare our predictions to the performance of some benchmark. In these types of models, one can imagine that we are playing a game against an opponent, and we are trying to minimize our losses regardless of the moves he plays. In this setting, we will frequently use optimization techniques such as mirror descent, as well as approaches from game theory and information theory. 11. PREDICTION WITH EXPERT ADVICE 1.1 Cumulative Regret LetAbe a convex set of actions we can take. For example, in the sequential investment example, A= ∆N. If our options are discrete–for instance, choosing edges in",
    "a graph–then think ofAas the convex hull of these options, and we can play one of the choices randomly according to some distribution. We will denote our adversary’s moves by Z. At time t, we simultaneously reveal at∈ Aandzt∈ Z. Denote by ℓ(at,zt) the loss associated to the player/decision maker taking action atand his adversary playing zt. Inthegeneral case,/summationtextn tℓ=1(at,zt)canbearbitrarilylarge. Therefore, ratherthanlooking at theabsoluteloss foraseriesof nsteps, wewill compareourloss totheloss ofabenchmark called an expert. An expert is simply some vector b∈ An,b= (b1,...,bt,...,bTn) . If we chooseKexpertsb(1),...,b(K), then our benchmark value will be the minimum cumulative loss amongst of all the experts: n (j)benchmark = min ≤j≤K/summationdisplay ℓ(bt,zt). 1t=1 Thecumulative regret is then deﬁned as n n Rn=/summationdisplay(j)ℓ(at,zt)−min ℓ(bt,zt). 1≤j≤Kt=1/summationdisplay t=1 At timet, we have access to the following information: 1. All of our previous moves, i.e.",
    "a1,...,at−1, 2. all of our adversary’s previous moves, i.e. z1,...,zt−1, and 3. All of the experts’ strategies, i.e. b(1),...,b(K). Naively, one might try a strategy which chooses a=b∗ ∗tt, where bis the expert which has incurred minimal total loss for times 1 ,...,t−1. Unfortunately, this strategy is easily exploitable by the adversary: he can simply choose an action which maximizes the loss for that move at each step. To modify our approach, we will instead take a convex combination of the experts’ suggested moves, weighting each according to the performanceof that expert thus far. To that end, we will replace ℓ(at,zt) byℓ(p,(bt,zt)), where p∈∆Kdenotes a (1) ( K)convex combination, bt= (bt,...,bt)T∈ AKis the vector of the experts’ moves at time t, andzt∈ Zis our adversary’s move. Then n n Rn=/summationdisplay ℓ(pt,zt)−min ℓ(ej,zt) 1≤j≤Kt=1/summationdisplay t=1 whereejis the vector whose jth entry is 1 and the rest of the entries are 0. Since we are restricting ourselves to convex",
    "combinations of the experts’ moves, we can write A= ∆K. We can now reduce our goal to an optimization problem: K n min ∈∆K/summationdisplay θj θj=1/summationdisplay ℓ(ej,zt). t=1 2From here, one option would be to use a projected gradient descent type algorithm: we deﬁne qt+1=pt−η(ℓ(eT1,zt),...,ℓ(eK,zT)) Kand then p∆t+1=π(pt) to be the projection of qt+1onto the simplex. 1.2 Exponential Weights Suppose we instead use stochastic mirror descent with Φ = negative entropy. Then qtqt+1,j=pt+1,jexp(−ηℓ(ej,zt)), pt+1,j= ,/summationtextK lq=1t+1,l where we have deﬁned K/parenleftBigg wt,jpt= ej, expKwj=1 l=1,l/parenrightBigg wt,j= t/parenleftBiggt−1 /summationdisplay −η/summationdisplay ℓ(ej,zs) s=1/parenrightBigg . This process looks at the los/summationtext s from each expert and downweights it exponentially according to the fraction of total loss incurred. For this reason, this method is called an exponential weighting (EW) strategy . Recall the deﬁnition of the cumulative regret Rn: n n",
    "Rn=/summationdisplay ℓ(pt,zt)−min 1≤j≤Kt=1/summationdisplay ℓ(ej,zt). t=1 Then we have the following theorem. Theorem: Assume ℓ(·,z) is convex for all z∈ Zand that ℓ(p,z)∈[0,1] for all p∈ ∆K,z∈ Z. Then the EW strategy has regret logKηnRn≤ +.η2 In particular, for η=/radicalBig 2logK,n Rn≤/radicalbig 2nlogK. Proof.We will recycle much of the mirror descent proof. Deﬁne K ft(p) =/summationdisplay pjℓ(ej,zt). j=1 Denote/bardbl·/bardbl:=|·|1. Then n n1/summationdisplay η1/bardblg/bardbl2tlogKft( (∗pt)−f∗t)≤n/summationtext t=1p +,n 2 ηnt=1 3wheregt∈∂ft(pt) and/bardbl · /bardbl∗is the dual norm (in this case /bardbl · /bardbl∗=| · |∞). The 2 in the denominator of the ﬁrst term of this sum comes from setting α= 1 in the mirror descent proof. Now, gt∈∂ft(pt)⇒gt= (ℓ(e1,zt),...,ℓ(eTK,zt)). Furthermore, since ℓ(p,z)∈[0,1], we have /bardblgt/bardbl∗=|gt|∞≤1 for all t. Thus nη1 n/summationtext t=1/bardblgt/bardbl2 ∗logK η logK+ ≤+.2 nη2ηn Substituting for ftyields nK K",
    "n/summationdisplay/summationdisplay ηnlogKpt,jℓ(ej,zt)−min/summationdisplay pjℓ(ej,zt)≤+. p∈∆K 2ηt=1j=1 j=1/summationdisplay t=1 Note that the boxed term is actually min 1≤j≤K/summationtextnℓt=1(ej,zt). Furthermore, applying Jensen’s to the unboxed term gives n K n /summationdisplay/summationdisplay pt,jℓ(ej,zt)≥/summationdisplay ℓ(pt,zt). t=1j=1 t=1 Substituting these expressions then yields ηnlogKRn≤+.2η We optimize over ηto reach the desired conclusion. We now oﬀer a diﬀerent proof of the same theorem which will give us the optimal constant in the error bound. Deﬁne /parenleftBiggt−1/parenrightBiggK K/summationdisplay /summationdisplay/summationtextwt,jej=1 jwt,j= exp−ηℓ(ej,zs), Wt=wt,j, pt= .Wts=1 j=1 Fort= 1, we initialize w1,j= 1, soW1=K. It should be noted that the starting values for w1,jare uniform, so we’re starting at the correct point (i.e. maximal entropy) for mirrored descent. Now we have /parenleftbigg Kexp−t−1η ℓ ,zW j=1 s=1(ej s) exp(−ηℓ(ej,zt))t+1log",
    "gW/summationtext  K tt/parenrightbigg = lo/parenleftBig · /summationtext−1 l ℓ e=e/summationtext xp/parenleftBig −η1/summationtext j,z=1/parenrightBig (ls) = log(IE J∼pt[exp(−ηℓ(eJ,zt))])/parenrightBig 12Hoeﬀding’s lemma ⇒ ≤log/parenleftBig ηIEe e−ηJ J8ℓ(e ,zt) η2/parenrightBig =−ηIEJℓ(eJ,zt)8 η2η2 Jensen’s ⇒ ≤ − ηℓ(IEJeJ,zt) =−ηℓ(pt,zt)8 8 4since IE Jej=/summationtextKpj=1t,jej. If we sum over t, the sum telescopes. Since W1=K, we are left withnnη2 log(Wn+1)−log(K)≤−η/summationdisplay ℓ(pt,zt).8t=1 We have K n log(Wn+1) = log /summationdisplay exp/parenleftBigg −η/summationdisplay ℓ(ej,zs) j=1 s=1/parenrightBigg , so settingnj∗= argmin1≤j≤K/summationtextℓ(e  t=1j,zt), we obtain n n log(Wn+1)≥log/parenleftBigg exp/parenleftBigg −η/parenrightBigg/parenrightBigg/summationdisplay ℓ(ej∗,zs) =−η/summationdisplay ℓ(ej∗,zt). s=1 t=1 Rearranging, we have n n /summationdisplay ηnlogKℓ(pt,zt)−ℓ t=/summationdisplay (ej∗,zt)≤+.8η1 t=1 Finally, we optimize over ηto arrive at η=/radicalbigg",
    "8logK oRn≤n/radicalbigg nl gK⇒ .2 The improved constant comes from the assumption that our loss lies in an interval of size 1 (namely [0 ,1]) rather than in an interval of size 2 (namely [ −1,1]). 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_16.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 16 Scribe:Haihao (Sean) Lu Nov. 2, 2015 Recall that in last lecture, we talked about prediction with expert advice. Remember thatl(ej,zt) means the loss of expert jat timet, whereztis one adversary’s move. In this lecture, for simplexity we replace the notation ztand denote by ztthe loss associated to all experts at time t: zt= ℓ(e1,zt) ... , ℓ(eK,zt) whereby for p∈∆K,p ⊤zt=Kp ej=1jℓ(j,zt). This gives an alternative deﬁnition of ft(p) in last lecture. Actually it is easy to check ft(p) =p⊤zt, thus we can rewrite the theorem for exponential weighting(EW/summationtext ) strategy as n n Rn≤/summationdisplay p⊤ tzt−minp pt=1∈∆k/summationdisplay⊤zt2nlogK, t=1≤/radicalbig where the ﬁrst inequality is Jensen inequality: n n/summationdisplay p⊤ zzt t=1≥/summationdisplay ℓ(pz,zt). t=1 We consider EW strategy for bounded convex losses. Without loss of generality, we assumeℓ(p,z)∈[0,1], for all ( p,z)∈∆K×Z, thus in",
    "notation here, we expect pt∈∆K andzK ¯t∈[0,1] . Indeed if ℓ(p,z)∈[m,M] then one can work with a rescaled loss ℓ(a,z) = ℓ(a,z)−m. Note that now we have bounded gradient on pt, sinceztis bounded.M−m 2. FOLLOW THE PERTURBED LEADER (FPL) In this section, we consider a diﬀerent strategy, called Follow the Perturbed Leader. At ﬁrst, we introduce Follow the Leader strategy, and give an example to show that Follow the Leader can be hazardous sometimes. At time t, assume that choose t−1 pt= argmin p∈∆K/summationdisplay p⊤zs. s=1 Notethat thefunctiontobeoptimized islinear in p, wherebytheoptimal solutionshould be a vertex of the simplex. This method can be viewed as a greedy algorithm, however, it might not be a good strategy. Consider the following example. Let K= 2,z1= (0,ε)⊤,z2= (0,1)⊤,z3= (1,0)⊤, z4= (0,1)⊤and so on (alternatively having (0 ,1)⊤and (1,0)⊤whent≥2), where εis small enough. Then with Following the Leader Strategy, we have that p1is arbitrary and in the best case p1= (1,0)⊤,",
    "andp2= (1,0)⊤,p3= (0,1)⊤,p4= (1,0)⊤and so on (alternatively having (0 ,1)⊤and (1,0)⊤whent≥2). 1In the above example, we have n n/summationdisplay nnp⊤ tztminp⊤ztn1 1, p∆k 2 2t=1− ≤ − − ≤ ∈/summationdisplay t=1− which gives raise to linear regret. Now let’s consider FPL. FPL regularizes FL by adding a small amount of noise, which can guarantee square root regret under oblivious adversary situation. Algorithm 1 Follow the Perturbed Leader (FPL) Input:Letξbe a random variables uniformly drawn on [0 ,1]K.η fort= 1 tondo t−1 pt= argmin p∈∆K/summationdisplay s=1/parenleftbig p⊤zs+ξ/parenrightbig . end for We analyze this strategy in oblivious adversaries, which means the sequence ztis chosen ahead of time, rather than adaptively given. The following theorem gives a bound for regret of FPL: Theorem: FPL with η=√1yields expected regret:kn IEξ[Rn]≤2√ 2nK . Before proving the theorem, we introduce the so-called Be-The-Leader Lemma at ﬁrst. Lemma: (Be-The-Leader) For all loss function ℓ(p,z),",
    "let t p∗ t= arg min ℓ(p,zs), p∈∆K/summationdisplay s=1 then we haven n /summationdisplay ℓ(p∗ t,zt) t=1≤/summationdisplay ℓ(pn∗,zt) t=1 Proof.The proof goes by induction on n. Forn= 1, it is clearly true. From nton+1, it 2follows from: n+1 n /summationdisplay ℓ(pt∗,zt) =/summationdisplay ℓ(p∗ t,zt)+ℓ(pn∗ +1,zn+1) t=1 i=1 n ≤/summationdisplay ℓ(p∗ n,zt)+ℓ(p∗ n+1,zn+1) i=1 n ≤/summationdisplay ℓ(p∗ n+1,zt)+ℓ(p∗ n+1,zn+1), i=1 wheretheﬁrstinequality usesinductionandthesecondinequality followsfromthedeﬁnition ofp∗ n. Proof of Theorem . Deﬁne t qt= argmin p⊤(ξ+ p∈∆K/summationdisplay zs). s=1 Using the Be-The-Leader Lemma with /braceleftbiggpT(ξ+z1)if t= 1ℓ(p,zt) =pTzt if t >1, we haven n q1⊤ξ+/summationdisplay qt⊤zt≤minq⊤(ξ+ qt=∈∆K 1/summationdisplay zt), t=1 whereby for any q∈∆K, n/summationdisplay/parenleftig2qt⊤zt−q⊤zt/parenrightig ≤/parenleftig q⊤−q1⊤/parenrightig ξ≤ /bardblq 1 i−q1 =1/bardbl /bardblξ/bardbl∞≤,η where the second inequality uses H¨ older’s inequality and the third",
    "inequality is from the fact that qandq1are on the simplex and ξis in the box. Now let t qt= arg min p⊤/parenleftigg ξ+zt+/summationdisplay zs p∈∆K s=1/parenrightigg and t pt= arg min p⊤+ ∈∆/parenleftigg ξ+0 pK/summationdisplay zs s=1/parenrightigg . Therefore, n n IE[Rn]≤/summationdisplay p⊤ tzt i−minp⊤zt p∈∆k =1/summationdisplay i=1 n/parenleftig /parenrightign ≤/summationdisplay qt⊤zt−p∗Tzt+/summationdisplay IE[(pt t i=1 =1−q)⊤zt] i n2≤+/summationdisplay IE[(pt−qt)⊤zt], (2.1)ηi=1 3wherep∗= argminp∈∆K/summationtextnp zt=1⊤t. Now let t−1 h(ξ) =zt⊤/parenleftigg arg min p⊤[ξ+ p∈∆K/summationdisplay zs], s=1/parenrightigg then we have a easy observation that IE[zt⊤(pt−qt)] = IE[h(ξ)]−IE[h(ξ+zt)]. Hence, IE[zt⊤(pt−qK Kt)] =η/integraldisplay h(ξ)dξ−η/integraldisplay h(ξ)dξ ξ∈[0,1]K ξ∈z+[0,1]Ktη η ≤ηK/integraldisplay h(ξ)dξ ξ∈[0,1]K η\\/braceleftBig z ,1]Kt+[0η/bracerightBig ≤ηK/integraldisplay 1dξ ξ∈[0,1]K\\/braceleftBig zt+[0,1]K η η/bracerightBig = IP(∃i∈[K],ξ(i)≤zt(i)) K",
    "≤/summationdisplay IP/parenleftbigg Unif/parenleftbigg1[0,]/parenrightbigg ≤zt(i)ηi=1/parenrightbigg ≤ηKzt(i)≤ηK , (2.2) where the ﬁrst inequality is from the fact that h(ξ)≥0, the second inequality uses h(ξ)≤1, the second equation is just geometry and the last inequality is due to zt(i)≤1. Combining (2.1) and (2.2) together, we have 2IE[Rn]≤+ηKn .η In particular, with η=/radicalig 2, we haveKn IE[Rn]≤2√ 2Kn , which completes the proof. 4MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_17.pdf": [
    "online learning with structured experts–a biased survey G´ abor Lugosi ICREA and Pompeu Fabra University, Barcelona 1on-line prediction A repeated game between forecaster and environment. At each round t, the forecaster chooses an action t2{1,..., }; (actions are often called experts ) the environment chooses losses `t(1),...,` t(N)2[0,1]; the forecaster su ↵ers loss`t(It). The goal is to minimize the regret Rn= Xn n `t(It)\u0000min itN=1X `t(i) t=1! . 7I N 7simplest example Is it possible to make (1/n)Rn!0forall loss assignments? LetN=2 and deﬁne, for all t=1,..., n, `t(1) =⇢0ifIt=2 1ifIt=1 and`t(2) = 1 \u0000`t(1). Then Xn nn`t(It)=n and min` i=12t=1X t(i) , t=12 so1 1Rnn\u0000.2 11randomized prediction Key to solution: randomization . At time t, the forecaster chooses a probability distribution pt\u00001=(p1,t\u00001,..., pN,t\u00001) and chooses action iwith probability pi,t\u00001. Simplest model: all losses `s(i),i=1,..., N,s<t,a r e observed: full information . 1212Hannan and Blackwell Hannan (1957)",
    "andBlackwell (1956) showed that the forecaster has a strategy such that 1 Xn n `t(It)n\u0000min itN=1X `t(i) t=1! !0 almost surely for all strategies of the environment. 1313basic ideas expected loss of the forecaster: N `t(pt\u00001)=X pi,t (\u00001`ti)=E t`t(It) i=1 By martingale convergence, Xn n1``1 2t(It)\u0000X/t(pnt\u00001) = OP(n\u0000) t=1 t=1 so it su\u0000 ces to study 1 n Xn n `t(pt)min`(i)\u00001\u0000 N=1X tit t=1!! 16weighted average prediction Idea: assign a higher probability to better-performing actions. Vovk (1990), Littlestone and Warmuth (1989) . A popular choice is exp⇣ \u0000⌘Pt\u00001 =1`(s si) pi,t =\u00001⌘ P =1N =1exp⇣ \u0000⌘P i ,...,N .t\u00001 =1`s( )k sk where⌘> 0.Then⌘ 1Xn n n `t(pt\u00001)\u0000min it=1NX `t(i) t=1! =r lnN 2n with⌘=p 8l nN/n. 19proof tLetLi,t=P s=1`s(i)and N Wt=X XN wi,t= e\u0000⌘Li,t i=1 i=1 fort\u00001,a n dW 0=N. First observe that Wnln eWXN =ln, 0 \u0000⌘Li n i=1! \u0000lnN \u0000ln✓ max e\u0000⌘Li,n ln i=1,...,N◆ \u0000N =\u0000⌘ min Li,ni=1,...,N\u0000lnN. 21proof On the other hand, for each t=1,...,n WtPN i=1wi,t1e\u0000⌘`t(i) ln =ln\u0000 Wt\u00001PN Pj=1wj,t\u00001",
    "N =1w Pi,t\u00001`(\u0000i ti)⌘2 ⌘ +N8j=1wj,t\u00001 ⌘2 =\u0000⌘` t(pt\u00001)+8 by Hoe↵ding’s inequality. Hoe↵ding (1963) :i fX2[0,1], lnEe\u0000⌘⌘2 X\u0000⌘EX+8 23proof for each t=1,..., n W2t ⌘ln \u0000⌘`t(p )+Wt t\u00001\u000018 Summing over t=1,..., n, Wn n ⌘ln\u0000⌘X 2 `t(pt1)+ n.W0\u00008t=1 Combining these, we get XnlnN⌘`t(pt1) min Li,n+ + n\u0000i=1,...,N ⌘ 8t=1 24lower bound The upper bound is optimal: for all predictors, Pn =1`t(Itp\u0000n)mint iN`tsup=1 t(i)1. n,N,`t(i) (n/2) ln NP \u0000 Idea: choose`t(i)to be i.i.d. symmetric Bernoulli coin ﬂips. n sup Xn `t(It) t(i)\u0000min ` it=1NX `t(i) \"t=1! n \u0000EX `t(It)\u0000min it=1N nXn `t(i) t=1# =2\u0000minBiiN Where B1,..., BNare independent Binomial (n,1/2) . Use the central limit theorem. 27follow the perturbed leader t\u00001 It=arg minX `s(i)+Zi,t i=1,...,Ns=1 where the Zi,tare random noise variables. The original forecaster of Hannan (1957) is based on this idea. 28follow the perturbed leader If the Zi,tare i.i.d. uniform [0,p nN],t h e n 1 NRnn2r +Op(n\u00001/2).n If the Zzi,tare i.i.d. with density",
    "(⌘/2)e\u0000⌘| |,t h e nf o r ⌘⇡p logN/n, 1 log NR1/2ncr +Op(n\u0000).n n Kalai and Vempala (2003) . 30combinatorial experts Often the class of experts is very large but has some combinatorial structure. Can the structure be exploited? path planning. At each time instance, the forecaster chooses apath in a graph between twoﬁxed nodes. Each edge has anassociated loss. Loss of a path isthe sum of the losses over theedges in the path. Nis huge!!! 32© Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.assignments: learning permutations Given a complete bipartite graphK m,m,t h e forecaster chooses aperfect matching.The loss is the sumof the losses overthe edges. Helmbold and Warmuth (2007): full information case. 33This image has been removed due to copyright restrictions. Please see the image athttp://38.media.tumblr.com/tumblr_m0ol5tggjZ1qir7tc.gifspanning trees The forecaster chooses a spanning tree in the",
    "completegraph K m. The cost is the sum of the losses over the edges. 34combinatorial experts dFormally, the class of experts is a set S⇢{ 0,1} of cardinality |S|=N. t 2RdAt each time , a loss is assigned to each component: `t . Loss of expert v2S is`t(v)=`> tv. Forecaster chooses It2S. The goal is to control the regret XnXn `t(It)\u0000 min`t(k). k=1,...,Nt=1 t=1 35computing the exponentially weighted average forecaster One needs to draw a random element of Swith distribution proportional to wt(v) = exp\u0000t \u0000⌘Lt(v)\u0000 = exp \u00001 \u0000⌘X `> tv. s=1! t exp jYd = =1 \u00001 \u0000⌘X `t,jvj s=1! . 36computing the exponentially weighted average forecaster path planning: Sampling may be done by dynamic programming. assignments: Sum of weights (partition function) is the permanent of a non-negative matrix. Sampling may be done by a FPAS of Jerrum, Sinclair, and Vigoda (2004) . spanning trees: Propp and Wilson (1998) deﬁne an exact sampling algorithm. Expected running time is the average hitting time ofthe Markov",
    "chain deﬁned by the edge weights w t(v). 39computing the follow-the-perturbed leader forecaster In general, much easier. One only needs to solve a linear optimization problem over S.T h i sm a yb eh a r db u ti ti sw e l l understood. In our examples it becomes either a shortest path problem, or an assignment problem, or a minimum spanning tree problem. 40follow the leader: random walk perturbation Suppose Nexperts, no structure. Deﬁne t It=arg min (`i,s1+Xs) i=1,...,NX \u0000 s=1 where the Xsare either i.i.d. normal or ±1coinﬂips. This is like follow-the-perturbed-leader but with random walk tperturbation:s=1Xt. Advantage: foP recaster rarely changes actions! 42follow the leader: random walk perturbation IfRnis the regret and Cnis the number of times It6=It\u00001,t h e n ERn2ECn8p 2nlogN+ 16 log n+ 16 . Devroye, Lugosi, and Neu (2015). Key tool: number of leader changes in Nindependent random walks with drift. 43follow the leader: random walk perturbation This also works in the",
    "“combinatorial” setting: just add an independent N(0,d)at each time to every component. ERn=Oe(B3/2p nlogd) and ECn=O(Bpnlogd), where B= max v2Skvk1. 44why exponentially weighted averages? May be adapted to many di ↵erent variants of the problem, including bandits, tracking, etc. 45multi-armed bandits The forecaster only observes `t(It)but not`t(i)fori6=It. Herbert Robbins (1952). 46 This image has been removed due to copyright restrictions. Please see the image at https://en.wikipedia.org/wiki/Herbert_Robbins#/media/File:1966-HerbertRobbins.jpgmulti-armed bandits Trick: estimate`t(i)by `e`t(It) t(iI=i)={t} pIt,t\u00001 This is an unbiased estimate: eXN`t(j)Et`t(i)= pj,t1{j=i}=`t(i) \u0000 jj,=1pt\u00001 Use the estimated losses to deﬁne exponential weights and mix with uniform ( Auer, Cesa-Bianchi, Freund, and Schapire, 2002): ⇣ \u0000Ptexp⌘\u00001`( ) P⇣s=1pi,t 1\u0000\u0000esi 1=( ) \u0000N k=1exp⌘ \u0000 \u0000⌘P +t\u00001 s=1`s(k)⌘ N exploration e | {z } exploitation|{z} 49multi-armed bandits Xn1E `t(pt1)\u0000minXn `t(i)! =O r NlnN!",
    ",n\u0000it=1Nt=1n 50multi-armed bandits Lower bound: n1 NsupE Xn `t(pt1)\u0000minX `t(i) , `t(i)n\u0000i N=1=1! \u0000Cr nt t Dependence on Nis not logarithmic anymore! Audibert and Bubeck (2009) constructed a forecaster with n n1 NmaxE`t(pti\u00001)`t(i) = O , Nn \u0000 t=1 t=1! r n!X X 52calibration Sequential probability assignment. Ab i n a r ys e q u e n c ex 1,x2,... is revealed one by one. After observing x1,...,x t\u00001, the forecaster issues prediction It2{0,1,..., N}. Meaning: “chance of rain is It/N”. Forecast is calibrated if \u0000\u0000P \u0000n \u0000t=1xt \u0000 {It=i}Pn t=1 {It=i}\u0000i N\u0000\u0000\u0000\u0000\u00001 2N+o(1) whenever lim supn(1/n)Pn t=1 0 {It=i>. } Is there a forecaster that is calibrated for all possible sequences? NO. (Dawid, 1985). 55randomized calibration However, if the forecaster is allowed to randomize then it is possible! (Foster and Vohra, 1997). This can be achieved by a simple modiﬁcation of any regret minimization procedure. Set of actions (experts): {0,1,...,N }. At time t, assign loss`t(i)=( xt\u0000i/N)2to action i. One",
    "can now deﬁne a forecaster. Minimizing regret is not su\u0000cient. 56internal regret Recall that the (expected) regret is Xn `t(ptXn n 1)\u0000min`t(i) = max\u0000i it=1 t=1X pj,t(`t(j)\u0000`t(i)) t=1X j The internal regret is deﬁned by n maxX pj,t(`t(j)\u0000`t(i)) i,jt=1 pj,t(`t(j)\u0000`t(i)) =Et ` {It=j(}t(j)\u0000`t(i)) is the expected regret of having taken action jinstead of action i. 59internal regret and calibration By guaranteeing small internal regret, one obtains a calibrated forecaster. This can be achieved by an exponentially weighted average forecaster deﬁned over N2actions. Can be extended even for calibration with checking rules. 60prediction with partial monitoring For each round t=1,..., n, the environment chooses the next outcome Jt2{1,...,M } without revealing it; the forecaster chooses a probability distribution ptand\u00001 draws an action It2{1,...,N }according to pt\u00001; the forecaster incurs loss `(It,Jt)and each action iincurs loss `(i,Jt). None of these values is revealed to the forecaster; the",
    "feedback h(It,Jt)is revealed to the forecaster. H=[h(i,j)]N⇥Mis the feedback matrix. L=[`(i,j)]N⇥Mis the loss matrix. 61examples Dynamic pricing. HereM=N,a n dL =[`(i,j)]N⇥Nwhere (j )i j\u0000i{ij+c(,)=} {i >j`}.N andh(i,j)= {i>jor} h(i,j)=a i j +b i>j,i,j=1,..., N. {} { } Multi-armed bandit problem. The only information the forecaster receives is his own loss: H=L. 63examples Apple tasting. = =2. L=01 10\u0000 H=aabc\u0000 . The predictor only receives feedback when he chooses the secondaction.Label e\u0000 cient prediction. N=3,M=2. L=211 401103 5 H=2ab cc3 .N M 4 cc5 65ag e n e r a lp r e d i c t o r A forecaster ﬁrst proposed by Piccolboni and Schindelhauer (2001). Crucial assumption: Hcan be encoded such that there exists an N⇥Nmatrix K=[k(i,j)]N⇥Nsuch that L=K·H. Thus, `(i,j)=XN k(i,l)h(l,j). l=1 Then we may estimate the losses by `ek(i,It)h(It,Jt)(i,Jt)=pIt,t. 66ag e n e r a lp r e d i c t o r Observe eXNk(i,k)h(k,Jt)Et`(i,Jt) = pk,t\u00001 k=1pk,t\u00001 k kXN = ( i,k)h(k,Jt)=` (i,Jt), =1 `e(i,Jt)is an",
    "unbiased estimate of`(i,Jt). Let e\u0000⌘Li,t\u00001\u0000pi,t\u00001=( 1\u0000\u0000)e P +N =1e\u0000⌘Lek,t\u00001 Nk where Li,t=Pt =1`(i,Jt). ese 67performance bound With probability at least 1\u0000\u0000, 1Xn1`(It,Jt)n\u0000 min i=1,...,Nt=1 \u0000Xn `(i,Jt)nt=1 Cn1/3N2/3p ln(N/\u0000). where Cdepends on K.(Cesa-Bianchi, Lugosi, Stoltz (2006) ) Hannan consistency is achieved with rate O(n\u00001/3)whenever L=K·H. This solves the dynamic pricing problem. Bart´ ok, P´ al, and Szepesv´ ari (2010) :i fM=2, only possible rates aren\u00001/2,n\u00001/3,1 70imperfect monitoring: a general framework Sis a ﬁnite set of signals. Feedback matrix: H:{1,...,N }⇥{ 1,...,M }!P (S). For each round t=1,2...,n , the environment chooses the next outcome Jt2{1,...,M } without revealing it; the forecaster chooses pt\u00001and draws an action It2{1,...,N }according to it; the forecaster receives loss `(It,Jt)and each action isu↵ers loss`(i,Jt), none of these values is revealed to the forecaster; af e e d b a c ks tdrawn at random according to H(It,Jt)is revealed to the forecaster.",
    "71target Deﬁne `(p,q)=X piqj`(i,j) i,j H(·,q)=( H(1,q),...,H (N,q)) where H(i,q)=jqjH(i,j). Denote by Fthe setP of those \u0000that can be written as H(·,q)for some q. Fis the set of “observable” vectors of signal distributions \u0000. The key quantity is ⇢(p,\u0000) = max `(p,q) q:H(·,q)=\u0000 ⇢is convex in pand concave in \u0000. 72rustichini’s theorem The value of the base one-shot game is minmax`(p,q)=m i n max⇢(p,\u0000) p q p\u00002F Ifqnis the empirical distribution of J1,...,J n,e v e nw i t ht h e knowledge of H(·,qn)we cannot hope to do better than min p⇢(p,H(·,qn)). Rustichini (1999) proved that there exists a strategy such that for all strategies of the opponent, almost surely, lim sup0 @1X `(It,Jt)\u0000min⇢(p,H( n n !1 p·,qn)) t=1,...,n1A0 73rustichini’s theorem Rustichini’s proof relies on an approachability theorem for a continuum of types ( Mertens, Sorin, and Zamir, 1994). It is non-constructive. It does not imply any convergence rate.Lugosi, Mannor, and Stoltz (2008) construct e\u0000 ciently computable",
    "strategies that guarantee fast rates of convergence. 74combinatorial bandits The class of actions is a set S⇢{ 0d,1} of cardinality |S|=N. At each time td, a loss is assigned to each component: `t2R. Loss of expert v2S is`t(v)=`> tv. Forecaster chooses It2S. The goal is to control the regret Xn n `t(It)\u0000 min k=1,...,Nt=1X `t(k). t=1 75combinatorial bandits Three models. (Full information.) Alldcomponents of the loss vector are observed.(Semi-bandit.) Only the components corresponding to the chosen object are observed.(Bandit.) Only the total loss of the chosen object is observed. Challenge: IsO(n \u00001/2poly(d ))regret achievable for the semi-bandit and bandit problems? 77combinatorial prediction game Adversary Player 78combinatorial prediction game Adversary Player 79© Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.combinatorial prediction game Adversary Player 80© Google. All rights reserved.",
    "This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.combinatorial prediction game Adversary Player`2`6 `d\u00001`1`4 `5 `9`d\u00002 `d `3 `8`7 81 © Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.combinatorial prediction game Adversary Player`2`6 `d\u00001`1`4 `5 `9`d\u00002 `d `3 `8`7 loss su ↵ered:`2+`7+...+`d 82© Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.combinatorial prediction game Adversary Player`2`6 `d\u00001`1`4 `5 `9`d\u00002 `d `3 `8`7 <8 Full Info:`1,`2,...,` d Feedback:: loss su ↵ered:`2+`7+...+`d 83© Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.combinatorial prediction game Adversary Player`2`6 `d\u00001`14 `5 `9`d\u00002 `d `3 `8`7` < n d b ck:8 Full I fo: `1,`2,...,`",
    "Feed a:Semi-Bandit: `2,`7,...,` d Bandit: `2+`7+...+`d loss su ↵ered:`2+`7+...+`d 85© Google. All rights reserved. This content is excluded from our Creative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.notation `2`6 `d\u00001`1`4 `5 `9`d\u00002 `d `3 `8`7 `2`6 `d\u00001`1`4 `5 `9`d\u00002 `d `3 `8`7S⇢{ 0,1}d `t2Rd + Vt2ST, loss su ↵ered:`tVt regret: n R EXn TV\u0000 EXTn=`ttmin`tu ut=12St=1 Tloss assumption: `tv 1for all v andt=1,...,n . || 2S 86weighted average forecaster At time tassign a weight wt,ito each i=1,...,d . The weight of each vk2S is wt(k)= wt,i. i:vkY (i)=1 Letqt\u00001(k)=w t\u00001(kN)/k=1wt\u00001(k). At each time t,d r a wK tfromP the distribution pt\u00001(k)=( 1\u0000\u0000)qtk\u00001()+\u0000µ(k) where µis a ﬁxed distribution on Sand\u0000> 0.H e r e wt,i= exp\u0000⌘Lt,i where Lt,i=`1,i+ +`t,iand\u0000 `t,iis a\u0000 n estimated loss.e ee ···e e 89loss estimates Dani, Hayes, and Kakade (2008) . Deﬁne the scaled incidence vector Xt=`t(Kt)VKt where Ktis distributed according to pt\u00001. LetPt\u00001=E⇥ VKtV>be theKtd⇥dcorrelation",
    "matrix. Hence Pt1⇤ \u0000(i,j)= k:vk(iX pt\u00001(k). )=v k(j)=1 Similarly, let Qt\u00001andMbe the correlation matrices of EVV> when Vhas law, qt\u00001andµ.T h e n⇥ ⇤ Pt\u00001(i,j)=( 1\u0000\u0000)Qt\u00001(i,j)+\u0000M(i,j). The vector of loss estimates is deﬁned by e`t=P+ t\u00001Xt where P+ t\u00001is the pseudo-inverse of Pt\u00001. 92key properties MM+v=vfor all v2S. Qt\u00001is positive semideﬁnite for every t. Pt1P+r at1v=vfo ll t v \u0000 and\u00002S. By deﬁnition, EtXt=Pt\u00001`t and therefore Ete`t=P+ t\u00001EtXt=`t An unbiased estimate! 93performance bound The regret of the forecaster satisﬁes 1✓lnELb2B2 d N n\u0000 min Ln(k) 2 +1 .n k=1,...,N◆ s✓ d\u0000min(M)◆ n where T\u0000min(M)= m i n xMx>0 x2span (S):kxk=1 is the smallest “relevant” eigenvalue of M.(Cesa-Bianchi and Lugosi, 2009.) Large\u0000min(M)is needed to make sure no `t,iis too large.|e| 94performance bound Other bounds: Bp dlnN/n(Dani, Hayes, and Kakade ). No condition on S. Sampling is over a barycentric spanner . dp (✓lnn)/n(Abernethy, Hazan, and Rakhlin). Computationally e\u0000cient. 95eigenvalue bounds",
    "\u0000min(M)= m i n E(V,x)2. x2span (S):kxk=1 where Vhas distribution µoverS. In many cases it su\u0000ces to take µuniform. 96multitask bandit problem The decision maker acts in mgames in parallel. In each game, the decision maker selects one of Rpossible actions. After selecting the mactions, the sum of the losses is observed. 1\u0000min=R maxELn\u0000Ln(k) lnR. k2mp 3nR The price of only obsh b erving the sui m of losses is a factor of m. Generating a random joint action can be done in polynomial time. 97assignments Perfect matchings of Km,m. At each time one of the N=m!perfect matchings of Km,mis selected. 1\u0000min(M)=m\u00001 maxELn\u0000Ln(k)2m 3nln(m !). k Only a factor of mh wb orse than ni aive full-ip nformation bound. 98spanning trees In a network of mnodes, the cost of communication between two nodes joined by edge eis`t(e)at time t.A te a c ht i m eam i n i m a l connected subnetwork (a spanning tree) is selected. The goal is to minimize the total cost. N=mm\u00002. 1\u0000min(M)=1Om\u0000✓ m2◆ . The entries of Mare",
    "2P{V i=1}=m P\u0000 3Vi=1,Vj=1 = ifm2i⇠j 4PVi=1,Vj=1 = if .2i6⇠jm\u0000 99stars At each time a central node of a network of mnodes is selected. Cost is the total cost of the edges adjacent to the node. 1\u0000min\u00001\u0000O✓ m◆ . 100cut sets Ab a l a n c e dc u ti nK 2mis the collection of all edges between a set ofmvertices and its comp\u0000lement. Each balanced cut has m2 2medges and there are N=m\u0000 balanced cuts. 1 1\u0000min(M)=\u0000O .4✓ m2◆ Choosing from the exponentially weighted average distribution is equivalent to sampling from ferromagnetic Ising model. FPAS byRandall and Wilson (1999) . 101hamiltonian cycles A Hamiltonian cycle in Kmis a cycle that visits each vertex exactly once and returns to the starting vertex. N=(m\u00001)! 2\u0000min\u0000m E\u0000cient computation is hopeless. 102sampling paths In all these examples µis uniform over S. Forpath planning it does not always work. What is the optimal choice of µ? What is the optimal way of exploration? 104minimax regret Rn=i n f max sup Rnstrategy S⇢{ 0,1}dadversary Theorem",
    "Letn\u0000d2.I nt h e full information and semi-bandit games, we have 0.008 dpnRndp 2n, and in the bandit game, 0.01d3/2pnRn2d5/2p 2n. 106proof upper bounds: D=[ 0,+1)d,F(x)=1 dog⌘ i=1xilxiworks for full information but it is only optiP mal up to a logarithmic factor in the semi-bandit case. in the bandit case it does not work at all! Exponentially weightedaverage forecaster is used. lower bounds:careful construction of randomly chosen set Sin each case. 108ag e n e r a ls t r a t e g y LetDdbe a convex subset of Rwith nonempty interior int(D). A function F:D! RisLegendre if •Fisstrictly convex and admits continuous ﬁrst partial derivatives on int(D), •Foru2@D,a n dv2int(D),w eh a v e lim (u\u0000vT)rF\u0000 (1\u0000s)u+sv=+ s!0,s >01. The Bregman divergence DF:D⇥ int(\u0000 D)associated to a Legendre function Fis DF(uT,v)=F (u)\u0000F(v)\u0000(u\u0000v)rF(v). 111CLEB (Combinatorial LEarning with Bregman divergences) Parameter: FLegendre on D\u0000 Conv (S) Conv (S)D \u0000(S )ptwtw0 t+1 wt+1 pt+1(1)wt0 +12D : rF(w0 +1)=rF (",
    ")twt\u0000`˜t (2)wt+12arg min DF(w,wt0 +1) w2Conv (S) (3)pt+12\u0000(S ):w t+1=EV⇠pt+1V 117General regret bound for CLEB Theorem IfFadmits a Hessian r2Falways invertible then, n Rn/diam ˜T 2\u00001˜DF(S)+EX `t =1⇣ rF(wt) t⌘ `t. 118Di↵erent instances of CLEB: LinExp (Entropy Function) D 1Pd=[ 0,+ )d,F(x)=1 ⌘ i=1xilogxi 8 >><Full Info :Exponentially weighted average >>:Semi-Bandit=Bandit :Exp3 Auer et al. [2002] 8 >>>Full Info :Component Hedge >>>>Koolen, Warmuth and Kivinen [2010] >< >>Semi-Bandit: MW >>Kale, Reyzin and Schapire [2010] Bandit: new algorithm >>>>: 124Di↵erent instances of CLEB: LinINF (Exchangeable Hessian) D=[ 0,+1)d,F(x)=Pd i=1Rxi 1( )0\u0000s ds INF, Audibert and Bubeck [2009] ⇢ (x) = exp(⌘ x):LinExp (x)=(\u0000⌘x)\u0000q,q>1:LinPoly 128Di↵erent instances of CLEB: Follow the regularized leader D=Conv (S),t h e n wt+12arg min Xt ˜T`sw+F(w) w2Ds=1! Particularly interesting choice: Fself-concordant barrier function , Abernethy, Hazan and Rakhlin [2008] 130MIT OpenCourseWare http://ocw.mit.edu 18.657",
    "Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_18.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 1 Scribe:Haihao (Sean) Lu Nov. 2, 2015 3. STOCHASTIC BANDITS 3.1 Setup The stochastic multi-armed bandit is a classical model for decision making and is deﬁned as follows: There are Karms(diﬀerent actions). Iteratively, a decision maker chooses an arm k∈ {1,...,K}, yielding a sequence XK,1,...,X K,t,..., which are i.i.d random variables with meanµk. Deﬁne µ∗= max jµjor∗ ∈argmax. A policy πis a sequence {πt}t≥1, which indicates which arm to be pulled at time t.πt∈ {1,...,K}and it depends only on the observations strictly interior to t. The regret is then deﬁned as: n n Rn= maxIE[ XK,t]−IE[Xπt,t] k/summationdisplay t=1/summationdisplay t=1 n =nµ∗−IE[/summationdisplay Xπt,t] t=1 n =nµ∗−IE[IE[ K/summationdisplay Xπt,t|πt]] t=1 =/summationdisplay ∆kIE[Tk(n)], k=1 nwhere ∆ k=µ∗−µkandTk(n) =/summationtext t=11I(πt=k) is the number of time when arm kwas pulled. 3.2 Warm Up: Full Info Case X1,t .Assume in this",
    "subsection that K= 2 and we observe the full information .. at XK,t timetafter choosing πt. So in each iteration, a normal idea is to choose the arm with highest average return so far. That is ¯ πt= argmax Xk,t k=1,2 where 1¯Xk,t=tt/summationdisplay s=1 Assume from now on that all random variable Xk,tare subGaussian with variance proxy 2 2 σ2, which means IE[ euxu σ]≤e2for allu∈IR. For example, N(0,σ2) is subGaussian withXk,s 18variance proxy σ2and any boundedrandomvariable X∈[a,b] is subGaussianwith variance proxy (b−a)2/4 by Hoeﬀding’s Lemma. Therefore, Rn= ∆IE[T2(n)], (3.1) where ∆ = µ1−µ2. Besides, n ¯ ¯ T2(n) = 1+/summationdisplay 1I(X2,t> X1,t) t=2 n = 1+/summationdisplay¯ ¯1I(X2,t−X1,t−(µ2−µ1)≥∆). t=2 ¯ ¯ It is easy to check that ( X2,t−X1,t)−(µ2−µ1) is centered subGaussian with variance proxy 2σ2, whereby 2¯ ¯−t∆IE[1I(X22,t> X1,t)]≤e4σ by a simple Chernoﬀ Bound. Therefore, ∞2 Rn≤∆(1+/summationdisplay 4σ2−t∆e24σ)≤∆+ , (3.2)∆t=0 whereby the benchmark is 4σ2 Rn≤∆+ .∆ 3.3",
    "Upper Conﬁdence Bound (UCB) Without loss of generality, from now on we assume σ= 1. A trivial idea is that after s pulls on arm k, we use µˆk,s=1/summationtext j∈{pulls ofk}XK,jand choose the one with largest µˆk,s.s The problem of this trivial policy is that for some arm, we might try it for only limited times, which give a bad average and then we never try it again. In order to overcome this limitation, a good idea is to choose the arm with highest upperbound estimate on the mean of each arm at some probability lever. Note that the arm with less tries would have a large deviations from its mean. This is called Upper Conﬁdence Bound policy. 2Algorithm 1 Upper Conﬁdence Bound (UCB) fort= 1 toKdo πt=t end for fort=K+1 tondo t−1 Tk(t) =/summationdisplay 1I(πt=k) s=1 (number of time we have pull arm kbefore time t) 1µˆk,t=/summationdisplay XK,t∧sTk(t)s=1 logt)πt∈argmax/braceleftigg 2 (µˆk,t+2 k∈[K]/radicaligg Tk(t)/bracerightigg , end for Theorem: The UCB policy has regret K",
    "/summationdisplaylogn π2 Rn≤8 +(1+ )∆k 3k,∆k>0/summationdisplay ∆k k=1 Proof.From now on we ﬁx ksuch that ∆ k>0. Then n IE[Tk(n)] = 1+ t=/summationdisplay IP(πt=k). K+1 Note that for t > K, 2logt 2logt{πt=k} ⊆ {µˆk,t+2/radicaligg ≤µˆ∗,t+2 }Tk(t)/radicaligg T∗(t) /braceleftigg /radicaligg 2logt/radicaligg /uniondisplay 2logt 2logt⊆ {µk≥µˆk,t+2 } {µ∗≥µˆ∗,t+2 }/uniondisplay {µ∗≤µk+2Tk(t) T∗(t)/radicaligg ,πt=k}Tk(t)/bracerightigg And from a union bound, we have /radicaligg 2logt 2logtIP(µˆk,t−µk<−2 ) = IP( µˆk,t−µk<2Tk(t)/radicaligg )Tk(t) t−s8logt ≤/summationdisplay exp(s)2s=1 1=t3 3t−12logt 2logtThus IP( µk> µˆk t+ 2 k)≤1, 3and similarly we have IP( µ∗> µˆ∗,t+ 2 ) ≤1,T(t)t T3∗(t)t whereby/radicalig /radicalig n n n /summationdisplay 1 2logtIP(πt=k)≤2 µk+2/radicaligg /summationdisplay +/summationdisplay IP(µ∗≤ ,πt=k)t3 Tk(t)t=K+1 t=1 t=1 ∞ n1 8logt≤2/summationdisplay +/summationdisplay IP(Tk(t)≤ ,πt=k)t3∆2 t=1 t=1 k ∞ n ≤2/summationdisplay1 gn+/summationdisplay 8loIP(Tk(t)≤",
    ",πt=k)t3∆2 t=1 t=1 k ∞ ∞/summationdisplay1 8 o≤2 + (3 t=1/summationdisplay l gnIPs≤ )t ∆2 s=1 k ∞ ≤2/summationdisplay1 8log n+t2∆2 t=1 k π28logn= + ,3∆2 k wheresis the counter of pulling arm k. Therefore we have K Rn=/summationdisplay /summationdisplay π28logn∆kIE[Tk(n)]≤ ∆k(1+ + ) ,3∆2 k k,∆kk =1 >0 which furnishes the proof. Consider the case K= 2 at ﬁrst, then from the theorem above we know Rn∼logn,∆ which is consistent with intuition that when the diﬀerence of two arm is small, it is hard to distinguish which to choose. On the other hand, it always hold that Rn≤n∆. Combining logn log(n∆2)these two results, we have Rn≤ ∧ n∆, whereby Rn≤ up to a constant.∆ ∆ Actually it turns out to be the optimal bound. When K≥3, we can similarly get the log(n∆2)result that Rn≤/summationtext k kk. This, however, is not the optimal bound. The optimal bound∆ should be/summationtextlog(n/H) kk, which includes the harmonic sum and H=1. See [Lat15].∆/summationtext k∆2 k 3.4 Bounded Regret From above we",
    "know UCB policy can give regret that increases with at most rate log nwith n. In this section we would consider whether it is possible to have bounded regret. Actually it turns out that if there is a known separator between the expected reward of optimal arm and other arms, there is a bounded regret policy. We would only consider the case when K= 2 here. Without loss of generality, we assumeµ1=∆andµ∆2=−, then there is a natural separator 0.2 2 4Algorithm 2 Bounded Regret Policy (BRP) π1= 1 and π2= 2 fort= 3 tondo ifmaxkµˆk,t>0then thenπt= argmaxkµˆk,t else πt= 1,πt+1= 2 end if end for Theorem: BRP has regret 16Rn≤∆+.∆ Proof. IP(πt= 2) = IP( µˆ2,t>0,πt= 2)+IP( µˆ2,t≤0,πt= 2) Note that n n /summationdisplay IP(µˆ2,t>0,πt= 2)≤IE 1I(µˆ2,t>0,πt= 2) t=3/summationdisplay t=3 n ≤IE/summationdisplay 1I(µˆ2,t−µ2>0,πt= 2) t=3 ∞ ≤/summationdisplay 2 e−s∆ 8 s=1 8=,∆2 wheresis the counter of pulling arm 2 and the third inequality is a Chernoﬀ bound. Similarly, n n/summationdisplay IP(µˆ2,t≤0,πt= 2)",
    "=/summationdisplay IP(µˆ1,t≤0,πt−1= 1) t=3 t=3 8≤,∆2 Combining these two inequality, we have 16Rn≤∆(1+ ) ,∆2 References [Lat15] Tor Lattimore, Optimally conﬁdent UCB : Improved regret for ﬁnite-armed bandits , Arxiv:1507.07880, 2015. 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_19.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Alexander Rakhlin Lecture 19 Scribe:Kevin Li Nov. 16, 2015 4. PREDICTION OF INDIVIDUAL SEQUENCES In this lecture, we will try to predict the next bit given the previous bits in the sequence. Given completely random bits, it would be impossible to correctly predict more than half of the bits. However, certain cases including predicting bits generated by a human can be correct greater than half the time due to the inability of humans to produce truly random bits. We will show that the existence of a prediction algorithm that can predict better than a given threshold exists if and only if the threshold satiﬁes certain probabilistic inequalities. For more information on this topic, you can look at the lecture notes at http://stat.wharton.upenn.edu/ ~rakhlin/courses/stat928/stat928_notes.pdf 4.1 The Problem To state the problem formally, given a sequence y1,...,yn,...∈ {−1,+1}, we want to ﬁnd a prediction algorithm yˆt=yˆt(y1,...,yt1) that",
    "correctly predicts ytas much as possible. − iidIn order to get a grasp of the problem, we will consider the case where y1,...,yn∼Ber(p). It is easy to see that we can get n1IE/bracketleftBigg n/summationdisplay = =1{yˆtyt t}/bracketrightBigg →min{p,1−p} by letting yˆtequal majority vote of the ﬁrst t−1 bits. Eventually, the bit that occurs with higher probability will alway/BDs have occurred more times. So the central limit theorem shows that our loss will approach min {1p,1−p}at the rate of O(√).n Knowing that the distribution of the bits are iid Bernoulli random variables made the prediction problem fairly easy. More surprisingly is the fact that we can achieve the same for any individual sequence. Claim: There is an algorithm such that the following holds for any sequence y1,...,yn,.... n1limsup/summationdisplay {yˆt=yt}−min{y¯n,1n nnt=1−y¯} ≤0 a.s. →∞ It is clear that no deterministic strategy can achieve this bound. For any deterministic strategy, we can just choose yt=−yˆtand",
    "the predictions would be wrong every time. So we need a non-deterministic algorithm that chooses qˆt= IE[yˆt]∈[−1,1]. To prove this claim, we will look at a more general problem. Take a ﬁxed horizon n≥1, and function φ:{±1}n→ /CA. Does/BDthere exist a randomized prediction strategy such that for anyy1,...,ynn1IE[/summationdisplay {yˆt=ytnt=1}]≤φ(y1,...,yn) ? 1 /BD/ne}ationslash /ne}ationslash /ne}ationslashFor certain φsuch asφ≡0, it is clear that no randomized strategy exists. However for 1φ≡, the strategy of randomly predicting the next bit ( qˆt= 0) satisﬁes the inequality.2 Lemma: For a stable φ, the following are equivalent n1a)∃(qˆt)t=1,...,n∀y1,...,ynIE[/summationdisplay {yˆt=yt}]≤φ(y1,...,yn)nt=1 1b) IE[φ(ǫ1,...,ǫn)]≥whereǫ1,...,ǫnare Rademacher random variables2 where stable is deﬁned as follows Deﬁnition (Stable Function): A function φ:{±1}n→is stable if 1|φ(...,yi,...)−φ(...,−yi,...)| ≤n Proof.(a= 1⇒1b)SupposeIE φ <. Take(y1,...,yn) = (ǫ1,... /CA ,ǫn). ThenIE[1nynt=1{ˆt=2",
    "ǫt}] =>IE[φ]sotheremustexistasequence(nǫ1,...,ǫ1n)suchthatIE[/summationtext /summationtext t{yˆt=ǫt}]>2 n=1 φ(ǫ1,...,ǫn). (b=⇒a) Recu/BDrsively deﬁne V(y1,...,yt) such that ∀y1,...,yn /BD 1V(y1,...,yt1) = min max/parenleftBig IE[{yˆt=yt}]+V(y1,...y /BD − n) qt∈[−1,1]yt∈±1n/parenrightBig Looking at the deﬁnition, we can see that IE[1n t=1{yˆt=yt}] =V( )n∅ −V(y1,...,yn). Now we note that V(y1,...,yt) =−t−IE[φ(y1,/summationtext . /BD ..,yt,ǫt n +1,...,ǫn)] satisﬁes the recursive2 deﬁnition since 1 tminmax IE[yˆt=yt] IE[φ(y1,...,yt,ǫt+1,...,ǫn)] qˆtytn{ } − /BD −2n qˆtyt t1=minmax−−IE[φ(y1,...,yt,ǫt+1,...,ǫn)]− qˆtyt2n−2n qˆ t1q 1=minm x {−t −ˆ ta −tIE[φ(y1,...,yt1,1,ǫt+1,...,ǫn)]−,−IE[φ(y . −1, − 1, ..,yt−1,ǫt+1,...,ǫn)]− qˆt 2n 2n2n−2n} t1=−IE[φ(y1,...,yt1,ǫt,ǫt+1,...,ǫ − n)]−−2n =V(y1,...,yt1)− The ﬁrst equality uses the fact that for a,b∈ {±1},{1a=b}=−ab, the second uses the2 fact that yt∈ {±1}, the third minimizes the entire expression by choosing qˆtso that the two expressions in",
    "the max are equal. Here the fact that φis stable means qˆt∈[−1,1] and is the only place where we need φto be stable. Therefore we have /BD n1IE[/summationdisplay 1{yˆt=yt}] =V(∅)−V(y1,...,yn) =−IE[φ(ǫ1,...,ǫn)]++φ(y1,...,yn)n 2t=1≤φ(y1,...,yn) 2 /BD/ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash/BD/ne}ationslashby b). By choosing φ= min{y¯,1−y¯}+c√, this shows there is an algorithm that satisﬁes ourn original claim. 4.2 Extensions 4.2.1 Supervised Learning We can extend the problem to a regression type problem by observing xtand trying to predictyt. In this case, the objective we are trying to minimize would be 1 1ln/summationdisplay (yˆt,yt)−inf f∈F,n/summationdisplay l(f(xt)yt) It turns out that the best achievable performance in such problems is governed by martin- gale (or, sequential) analogues of Rademacher averages, covering numbers, combinatorial dimensions, and so on. Much of Statistical Learning techniques extend to",
    "this setting of online learning. In addition, the minimax/relaxation framework gives a systematic way of developing new prediction algorithms (in a way similar to the bit prediction problem). 4.2.2 Equivalence to Tail Bounds We can also obtain probabilistic tail bound on functions φon hypercube by using part a) of the earlier lemma. Rearranging part a) of the lemma we get 1 −2φ(1y1,...,yn)≤qˆtyt.n This implies/summationtext 2 IP/parenleftbig 1µ 1 µφ(ǫ1,...,ǫn)<−/parenrightbig = IP/parenleftbig 1−2φ(ǫ1,...,ǫn)> µ≤IP2 n/summationdisplay qˆtǫt> µ≤e−2n So IEφ≥1=⇒existence of a strategy = ⇒tail boun/parenrightbig d forφ/parenleftbig 1<./parenrightbig 2 2 We can extend the results to higher dimensions. Consider z1,...,zn∈B2whereB2is a ball in a Hilbert space. We can deﬁne recursively yˆ0= 0 and yˆt+1= ProjB2(yˆt−1√zt).n Based on the properties of projections, for every∗∈, we have1/summationtext/an}bracketle{tˆ−∗/an}bracketri}ht ≤1y B 2 yty ,zt n√.n zTakingy∗ t= ,/bardbl/summationtext",
    "/summationtextzt/bardbl n n ∀z1,...,zn,/summationdisplay zt√/bardbl t=1/bardbl−n≤/summationdisplay yˆt, zt t=1/an}bracketle{t − /an}bracketri}ht Take a martingale diﬀerence sequence Z1,...,Z nwith values in B2. Then n n IP/parenleftbig /bardbl/summationdisplay 2nµZt√ t=1/bardbl−n > µ/parenrightbig ≤IP(/summationdisplay t=1/an}bracketle{tyˆt,−Zt/an}bracketri}ht> µ)≤e−2 Integrating out the tail, n IE/bardbl/summationdisplay Zt t=1/bardbl ≤c√n 3It can be shown using Von Neumann minimax theorem that n n ∃(yˆt)∀z1,...,zn,y∗∈B2/summationdisplay Wt√/an}bracketle{tyˆt−y∗,zt/an}bracketri}ht ≤ supE c n MDSWt1,...,W=1n/bardbl/summationdisplay t=1/bardbl ≤ where the supremum is over all martingale diﬀerence sequences (MDS) with values in B2. By the previous part, this upper bound is c√n. We conclude an interesting equivalence of (a) deterministic statements that hold for all sequences, (b) tail bounds on the size of a martingale, and (c) in-expectation bound on this size. In fact, this connection",
    "between probabilistic bounds and existence of prediction strate- gies for individual sequences is more general and requires further investigation. 4MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_2.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 2 Scribe: Jonathan Weed Sep. 14, 2015 Part I Statistical Learning Theory 1. BINARY CLASSIFICATION In the last lecture, we looked broadly at the problems that machine learning seeks to solve and the techniques we will cover in this course. Today, we will focus on one such problem, binary classi cation , and review some important notions that will be foundational for the rest of the course. Our present focus on the problem of binary classi cation is justi ed because both binary classi cation encompasses much of what we want to accomplish in practice and because the response variables in the binary classi cation problem are bounded. (We will see a very important application of this fact below.) It also happens that there are some nasty surprises in non-binary classi cation, which we avoid by focusing on the binary case here. 1.1 Bayes Classi er Recall the setup of binary classi cation: we observe a sequence (",
    "X1;Y1);:::; (Xn;Yn) ofn independent draws from a joint distribution PX;Y. The variable Y(called the label) takes values inf0;1g, and the variable Xtakes values in some space Xrepresenting \\features\" of the problem. We can of course speak of the marginal distribution PXofXalone; moreover, sinceYis supported onf0;1g, the conditional random variable YjXis distributed according to a Bernoulli distribution. We write YjX\u0018Bernoulli(\u0011 (X)), where \u0011(X) = I P(Y = 1jX ) = I E[YjX]: (The function \u0011is called the regression function .) We begin by de ning an optimal classi er called the Bayes classi er. Intuitively, the Bayes classi er is the classi er that \\knows\" \u0011|it is the classi er we would use if we had perfect access to the distribution YjX. De nition: The Bayes classi er ofXgivenY, denotedh\u0003, is the function de ned by the rule (h\u001a1 if\u0011 x)>1=2 \u0003(x) =0 if\u0011(x)\u00141=2. In other words, h\u0003(X) = 1 whenever I P(Y = 1jX)>I P(Y = 0jX). Our measure of performance for any classi er h(that is, any function",
    "mapping Xto f0;1g) will be the classi cation error :R(h) = I P(Y=h(X)). The Bayes risk is the value R\u0003=R(h\u0003) of the classi cation error associated with the Bayes classi er. The following theorem establishes that the Bayes classi er is optimal with respect to this metric. 16Theorem: For any classi er h, the following identity holds: R(h)\u0000R(h\u0003) =Z j2\u0011(x)\u00001jPx(dx) = I E X[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))] (1.1) h=h\u0003 Whereh=h\u0003is the (measurable) set fx2Xjh(x) =h\u0003(x)g. In particular, since the integrand is nonnegative, the classi cation error R\u0003of the Bayes classi er is the minimizer of R(h) over all classi ers h. Moreover, 1R(h\u0003) = I E[min(\u0011 (X);1\u0000\u0011(X))]\u0014: (1.2)2 Proof. We begin by proving Equation (1.2). The de nition of R(h) implies R(h) = I P(Y=h(X)) = I P(Y = 1;h(X ) = 0) + I P( Y= 0;h(X ) = 1); where the second equality follows since the two events are disjoint. By conditioning on X and using the tower law, this last quantity is equal to I E[I E[1(Y = 1;h(X ) = 0)jX ]] + I E[I E[1(Y = 0;h(X",
    ") = 1)jX]] Now,h(X) is measurable with respect to X, so we can factor it out to yield I E[1(h(X ) = 0)\u0011(X) +1(h(X ) = 1)(1\u0000\u0011(X))]]; (1.3) where we have replaced I E[ YjX] by\u0011(X). In particular, if h=h\u0003, then Equation 1.3 becomes I E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))]: But\u0011(X)\u00141=2 implies \u0011(X)\u00141\u0000\u0011(X) and conversely, so we nally obtain R(h\u0003) = I E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))] = I E[(1(\u0011 (X)\u00141=2) + 1(\u0011(x)>1=2)) min(\u0011(X);1\u0000\u0011(X))] = I E[min(\u0011 (X);1\u0000\u0011(X))]; as claimed. Since min(\u0011 (X);1\u0000\u0011(X))\u00141=2, its expectation is also certainly at most 1 =2 as well. Now, given an arbitrary h, applying Equation 1.3 to both handh\u0003yields R(h)\u0000R(h\u0003) = I E[ 1(h(X ) = 0)\u0011 (X) +1(h(X ) = 1)(1\u0000\u0011(X))] \u00001(h\u0003(X) = 0)\u0011 (X) +1(h\u0003(X) = 1)(1\u0000\u0011(X))]]; which is equal to I E[(1(h(X ) = 0)\u00001(h\u0003(X) = 0))\u0011 (X) + (1(h(X ) = 1)\u00001(h\u0003(X) = 1))(1\u0000\u0011(X))]: Sinceh(X) takes only the values 0 and 1, the second term can be rewritten as \u0000(1(h(X ) = 0)\u00001(h\u0003(X) = 0)). Factoring yields I E[(2\u0011 (X)\u00001)(1(h(X ) = 0)\u00001(h\u0003(X) =",
    "0))]: 266 6 6The term 1(h(X ) = 0)\u00001(h\u0003(X) = 0) is equal to \u00001, 0, or 1 depending on whether h andh\u0003agree. When h(X) =h\u0003(X), it is zero. When h(X) =h\u0003(X), it equals 1 whenever h\u0003(X) = 0 and\u00001 otherwise. Applying the de nition of the Bayes classi er, we obtain I E[(2\u0011 (X)\u00001)1(h(X ) =h\u0003(X)) sign(\u0011\u00001=2)] = I E[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))]; as desired. We make several remarks. First, the quantity R(h)\u0000R(h\u0003) in the statement of the theorem above is called the excess risk ofhand denotedE(h). (\\Excess,\" that is, above the Bayes classi er.) The theorem implies that E(h)\u00150. Second, the risk of the Bayes classi er R\u0003equals 1=2 if and only if \u0011(X) = 1=2 almost surely. This maximal risk for the Bayes classi er occurs precisely when Y\\contains no information\" about the feature variable X. Equation (1.1) makes clear that the excess risk weighs the discrepancy between handh\u0003according to how far \u0011is from 1=2. When \u0011is close to 1=2, no classi er can perform well and the excess risk is low. When \u0011is far",
    "from 1=2, the Bayes classi er performs well and we penalize classi ers that fail to do so more heavily. As noted last time, linear discriminant analysis attacks binary classi cation by putting some model on the data. One way to achieve this is to impose some distributional assump- tions on the conditional distributions XjY= 0 andXjY= 1. We can reformulate the Bayes classi er in these terms by applying Bayes' rule: I P(X =xY= 1)I P(Y= 1)\u0011(x) = I P(Y = 1jjX=x) = :I P(X =xjY= 1)I P(Y= 1) + I P(X=xjY= 0)I P(Y= 0) (In general, when PXis a continuous distribution, we should consider in nitesimal proba- bilities I P(X2dx).) Assume that XjY= 0 andXjY= 1 have densities p0andp1, and I P(Y = 1) =\u0019is some constant re ecting the underlying tendency of the label Y. (Typically, we imagine that\u0019is close to 1=2, but that need not be the case: in many applications, such as anomaly detection,Y= 1 is a rare event.) Then h\u0003(X) = 1 whenever \u0011(X)\u00151=2, or, equivalently, whenever p1(x) 1\u0000\u0019:p0(x)\u0015\u0019 When\u0019= 1=2,",
    "this rule amounts to reporting 1 or 0 by comparing the densities p1 andp0. For instance, in Figure 1, if \u0019= 1=2 then the Bayes classi er reports 1 whenever p1\u0015p0, i.e., to the right of the dotted line, and 0 otherwise. On the other hand, when \u0019is far from 1=2, the Bayes classi er is weighed towards the underlying bias of the label variable Y. 1.2 Empirical Risk Minimization The above considerations are all probabilistic , in the sense that they discuss properties of some underlying probability distribution. The statistician does nothave access to the true probability distribution PX;Y; she only has access to i.i.d. samples (X 1;Y1);:::; (Xn;Yn). We consider now this statistical perspective. Note that the underlying distribution PX;Y still appears explicitly in what follows, since that is how we measure our performance: we judge the classi ers we produced on future i.i.d. draws from PX;Y. 36 6 6Figure 1: The Bayes classi er when \u0019= 1=2. Given dataDn=f ^ (X1;Y1);:::; (Xn;Yn)g, we build",
    "a classi er hn(X), which is random in two senses: it is a function of a random variable Xand also depends implicitly on the ^ random dataDn. As above, we judge a classi er according to the quantity E(hn). This is a random variable: though we have integrated out X, the excess risk still depends on the dataDn. We therefore will consider bounds both on its expected value and bounds that ^ hold in high probability. In any case, the bound E(hn)\u00150 always holds. (This inequality does not merely hold \\almost surely,\" since we proved that R(h)\u0015R(h\u0003) uniformly over all choices of classi er h.) Last time, we proposed two di erent philosophical approaches to this problem. In particular, generative approaches make distributional assumptions about the data, attempt to learn parameters of these distributions, and then plug the resulting values into the model. The discriminative approach|the one taken in machine learning|will be described in great detail over the course of this semester. However,",
    "there is some middle ground, which is worth mentioning brie y. This middle ground avoids making explicit distributional assumptions aboutXwhile maintaining some of the avor of the generative model. The central insight of this middle approach is the following: since by de nition h\u0003(x) = ^ 1(\u0011(X)>1=2), we estimate \u0011by some\u0011^nand thereby produce the estimator hn= 1(\u0011^n(X)>1=2). The result is called a plug-in estimator. Of course, achieving good performance with a plug-in estimator requires some assump- tions. (No-free-lunch theorems imply that we can't avoid making an assumption some- where!) One possible assumption is that \u0011(X) is smooth; in that case, there are many nonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet bases, etc.). We could also assume that \u0011(X) is a function of a particular form. Since \u0011(X) is only supported on [0; 1], standard linear models are generally inapplicable; rather, by applying the logit transform we obtain logistic",
    "regression , which assumes that \u0011satis es an identity of the form log\u0012\u0011(X) 1\u0000\u0011(X)\u0013 =\u0012TX: Plug-in estimators are called \\semi-paramteric\" since they avoid making any assumptions about the distribution of X. These estimators are widely used because they perform fairly well in practice and are very easy to compute. Nevertheless, they will not be our focus here. In what follows, we focus here on the discriminative framework and empirical risk min- imization. Our benchmark continues to be the risk function R(h) = I E1(Y =h(X)), which 46is clearly not computable based on the data alone; however, we can attempt to use a na \u0010ve statistical \\hammer\" and replace the expectation with an average. De nition: The empirical risk of a classi er his given by n1^Rn(h) =X 1(Yi=h(Xi)):ni=1 Minimizing the empirical risk over the family of all classi ers is useless, since we can always minimize the empirical risk by mimicking the data and classifying arbitrarily other- wise. We therefore limit our",
    "attention to classi ers in a certain family H. ^ De nition: The Empirical Risk Minimizer (ERM) overHis any element1hermof the set ^ argminhRn(h).2H In order for our results to be meaningful, the class Hmust be much smaller than the ^ space of all classi ers. On the other hand, we also hope that the risk of hermwill be close to the Bayes risk, but that is unlikely if His too small. The next section will give us tools for quantifying this tradeo . 1.3 Oracle Inequalities An oracle is a mythical classi er, one that is impossible to construct from data alone but \u0016 whose performance we nevertheless hope to mimic. Speci cally, given Hwe de nehto be an element of argminhR(h)|a classi er in2H Hthat minimizes the true risk. Of course, \u0016 we cannot determine h, but we can hope to prove a bound of the form ^R(h)\u0014 \u0016R(h) + something small: (1.4) \u0016 Sincehis the best minimizer in Hgiven perfect knowledge of the distribution, a bound of ^ the form given in Equation 1.4 would imply that hhas",
    "performance that is almost best-in- class. We can also apply such an inequality in the so-called improper learning framework, ^ where we allow hto lie in a slightly larger class H0 ^\u001bH; in that case, we still get nontrivial \u0016 guarantees on the performance of hif we know how to control R(h) There is a natural tradeo between the two terms on the right-hand side of Equation 1.4. WhenH \u0016 is small, we expect the performance of the oracle hto su er, but we may hope \u0016 to approximate hquite closely. (Indeed, at the limit where His a single function, the \\something small\" in Equation 1.4 is equal to zero.) On the other hand, as Hgrows the oracle will become more powerful but approximating it becomes more statistically di\u000ecult. (In other words, we need a larger sample size to achieve the same measure of performance.) ^ SinceR(h) is a random variable, we ultimately want to prove a bound in expectation or tail bound of the form ^ I P(R(h)\u0014 \u0016R(h) + \u0001n;\u000e(H))\u00151\u0000\u000e; where \u0001n;\u000e(H) is some explicit term",
    "depending on our sample size and our desired level of con dence. 1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classi er ^ ^^ hsatisfying Rn(h)\u0014infhR2H n(h) + \". 56In the end, we should recall that E^ ^\u0000\u0003 ^\u0000 \u0016 \u0016 (h) =R(h)R(h) = (R(h)R(h)) + (R (h)\u0000R(h\u0003)): The second term in the above equation is the approximation error, which is unavoidable once we x the class H. Oracle inequalities give a means of bounding the rst term, the stochastic error. 1.4 Hoe ding's Theorem Our primary building block is the following important result, which allows us to understand how closely the average of random variables matches their expectation. Theorem (Hoe ding's Theorem): LetX1;:::;Xnbenindependent random vari- ables such that Xi2[0;1] almost surely. Then for any t>0, I P n1X XiI EXini=1\u0000 >! 2t\u00142e\u00002nt: In other words, deviations from the mean deca y exponentially fast in nandt. Proof. De ne centered random variables Zi=Xi\u0000I EXi. It su\u000eces to show that",
    "\u00121X\u0013 \u0014\u00002nt2I PZi>t e ;n since the lower tail bound follows analogously. (Exercise!) We apply Cherno bounds. Since the exponential function is an order-preserving bijec- tion, we have for any s>0 I P\u00121X Zstni>t\u0013 = I P\u0010 exp\u0010 sX Zstn s Z ii ]n\u0011 >e\u0011 \u0014e\u0000I E[eP (Markov) =e\u0000stnI E[esZi]; (1.5) where in the last equality we have used the independence of theY Zi. We therefore need to control the term I E[ esZi], known as the moment-generating func- tion ofZi. If theZiwere normally distributed, we could compute the moment-generating function analytically. The following lemma establishes that we can do something similar when theZiare bounded. Lemma (Hoe ding's Lemma): IfZ2[a;b] almost surely and I EZ = 0, then 2 2 \u0014s(b\u0000a)I EesZe 8: Proof of Lemma. Consider the log-moment generating function (s) = log I E[esZ], and note that it su\u000eces to show that (s)\u0014s2(b\u0000a)2=8. We will investigate by computing the 6 rst several terms of its Taylor expansion. Standard regularity conditions imply that we can",
    "interchange the order of di erentiation and integration to obtain I E[ZesZ] 0(s) = ;I E[esZ] 2I E[Z2esZ]I E[esZ] I E[ZesZ]2esZesZ 00(s) =\u0000= I E\u0014 Z2\u0015 \u0000\u0012 I EZI E[esZ]2 I E[esZ]\u0014 I E[esZ]\u0015\u0013 : SinceesZ sZintegrates to 1, we can interpret 00(s) as the variance of Zunder the probabilityI E[e] measuredF=esZ sZdI E. We obtainI E[e] 00(s) = var F(Z) = var F\u0012a+bZ\u00002\u0013 ; since the variance is una ected under shifts. But jZ\u0000a+b 2j\u0014b\u0000aalmost surely since2 Z2[a;b] almost surely, so varF\u0012+bZ\u00002\u0013 \u0014F\"\u00122a a+bZ\u00002\u0013# (b\u0000a)2 \u0014:4 Finally, the fundamental theorem of calculus yields s us2(b a)2 (s) =Z (u du 0Z 00)\u0000: 0\u00148 This concludes the proof of the Lemma. Applying Hoe ding's Lemma to Equation (1.5), we obtain I P\u00121X2Z >t\u0013 \u0014e\u0000stnY es2=8=ens =8stni\u0000;n for anys>0. Plugging in s= 4t>0 yields I P\u00121X Zi>t\u0013 \u0014e\u00002nt2;n as desired. Hoe ding's Theorem implies that, for any classi er h, the bound log(2=\u000e )j^Rn(h)\u0000R(h)j\u0014r 2n holds with probability 1 \u0000\u000e. We can immediately apply this formula to yield a maximal inequality:",
    "ifHis a nite family, i.e., H=fh1;:::;hMg, then with probability 1 \u0000\u000e=M the bound logj^Rn(hj)\u0000R(hj)j\u0014r (2M=\u000e) 2n 7^ holds. The event that max jjRn(hj)\u0000R(hj)j ^ >tis the union of the events jRn(hj)\u0000R(hj)j> tforj= 1;:::;M , so the union bound immediately implies that log(2M=\u000e )maxj^Rn(hj)\u0000R(hj) jj\u0014r 2n with probability 1 \u0000\u000e. In other words, for such a family, we can be assured that the empirical risk and the true risk are close. Moreover, the logarithmic dependence on Mimplies that we can increase the size of the family Hexponentially quickly with nand maintain the same guarantees on our estimate. 8MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_20.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 20 Scribe:Vira Semenova Nov. 23, 2015 In this lecture, we talk about the adversarial bandits under limited feedback. Adver- sarial bandit is a setup in which the loss function l(a,z) :AxZis determinitic. Lim- ited feedback means that the information available to the DM after the step tisIt= {l(a1,z1),...,l(at−1,zt)}, namely consits of the realised losses of the past steps only. 5. ADVERSARIAL BANDITS Consider the problem of prediction with expert advice. Let the set of adversary moves be Z and the set of actions of a decision maker A={e1,...,eK}. At time t,at∈ Aandzt∈ Zare simultaneously revealed.Denote the loss associated to the decision at∈ Aand his adversary playingztbyl(at,zt). We compare the total loss after nsteps to the minimum expert loss, namely: n min ≤ ≤/summationdisplay lt(ej,zt), 1j Kt=1 whereejis the choice of expert j∈ {1,2,..,K}. The cumulative regret is then deﬁned as n n Rn=/summationdisplay",
    "lt(at,zt)−min/summationdisplay lt(ej,zt) 1≤j≤Kt=1 t=1 The feedback at step tcan be either full or limited. The full feedback setup means that thevector f= (l(e1,z⊤t),...,l(eK,zt)) oflossesincurredatapairofadversary’schoice ztand each bandit ej∈ {e1,...,eK}is observed after each step t. Hence, the information available to the DM after the step tisI=∪t ′t ′{l(a1,zt),...,l(aK,zt=1 t′)}. The limited feedback means that the time −tfeedback consists of the realised loss l(at,zt) only. Namely, the information available to the DM is It={l(a1,z1),...,l(at,zt)}. An example of the ﬁrst setup is portfolio optimization problems, where the loss of all possible portfolios is observed at time t. An example of the second setup is a path planning problem and dynamic pricing, where the loss of the chosen decision only is observed. This lecture has limited feedback setup. The two strategies, deﬁned in the past lectures, were exponential weights, which yield the regret of order Rn≤c√nlogKand Follow the",
    "Perturbed Leader. We would like to play exponential weights, deﬁned as: 1exp(tηpt,j/summationtext−l=s=1(ej,zs))/summationtextk− l=1exp(−η/summationtextt−1ls=1(ej,zs)) This decision rule is not feasible, since the loss l(ej,zt) are not part of the feedback if ej=at. We will estimate it by l(ej,zt)1I(aˆt=ej)l(ej,zt) =P(at=ej) 1/ne}ationslashˆ Lemma: l(ej,zt) is an unbiased estimator of l(ej,zt) K I(P ˆl(e e =roof.Ek,zt)1ket) atl(ej,zt) =/summationtextPk=1 (a=e) =l(e ,z)P(at=ej) tk j t Deﬁnition (Exp 3 Algorithm): Letη >0 be ﬁxed. Deﬁne the exponential weights as −/summationtextt−1ηˆ(j) exp( l(ej,zs))ps=1 t+1,j=/summationtextk l=1exp(−η/summationtextt−1ˆls=1(ej,zs)) (Exp3 stands for Exponential weights for Exploration and Exploitation.) We will show that the regret of Exp3 is bounded by√2nKlogK. This bound is√ K times bigger than the bound on the regret under the full feedback. The√ Kmultiplier is the price of have smaller information set at the time t. The are methods that allow to get",
    "rid of log Kterm in this expression. On the other hand, it can be shown that√ 2nKis the optimal regret. −/summationtextt/summationtextK−1ˆW eProof.LetWt,j= exp(kη l(e ,=jzs)),W1 t=/summationtextWs j=1t,j, andp=j=1t,j j t W.t W/summationtextK(−η/summationtext−1exptlˆe ,z ηl ˆe ,zt+1 j=1 s=1(j s))exp( ( j t)) log( ) = log( ) (5.1)W/summationtextKet− xp(−/summationtext−1t ηˆls=1(ej,zj=1 s)) t−1 ˆ = log(IE J∼ptexp(−η/summationdisplay l(eJ,zs))) (5.2) s=1 ≤∗ η2 log(1−2ηˆIEJ∼ptl(eJ,z ˆs)+IEJ∼ptl(eJ,zs) (5.3)2 ∗ ˆ where inequality is obtained by plugging in IE J∼ptl(eJ,zt) into the inequality η2x2 expx≥1−ηx+2 . K Kl(ej,zt)1I(a e)ˆIEJ∼ptl(eJzt) =/summationdisplayˆt=j, pt,jl(eJ,zt) =/summationdisplay pt,j =l(at,zt) (5.4)P(at=ej)j=1 j=1 K Kl2 2 (ej,zt)1I(a)ˆ ˆ t=ejIEJ∼ptl(eJ,zt) =/summationdisplay pt,jl2(eJ,zt) =/summationdisplay pt,j (5.5)P2(at=ej)j=1 j=1 l2(ej,zt)1=Pat,t≤ (5.6)Pat,t Summing from 1 through n, we get log(Wt+1)≤2log(n 1)−η/summationtextlt=1(at,zt)+ηW2/summationtext 1. Pa ,tt",
    "2Fort= 1, we initialize w1,j= 1, soW1=K. Since IE1 p J=Pa ,t/summationtextK j,tKj=1= , the expression above becomespt j,t IElog(n η2KnWn+1)−logK≤ −η/summationtext tl a=1(t,zt)+2ˆ Noting that log( Wn+1) = log(/summationtextK j=1exp(−η/summationtextt−1l e ,zs=1(j s)) and deﬁning j∗= argmin1≤j≤K/summationtextnlt=1(ej,zt), we obtain: K t−1 t−1 log(Wn+1)≥log(/summationdisplay exp(−η/summationdisplay l(ej,zs))) =−η/summationdisplay l(ej,zs) j=1 s=1 s=1 Together: n nlogKηKn /summationdisplay l(at,zt)−min t≤j≤/summationdisplay l(ej,z) Kt t=1≤ + 1 η2=1 The choice of η:=√2logKnKyields the bound Rn√≤2KlogKn. 3MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_21.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 21 Scribe:Ali Makhdoumi Nov. 25, 2015 6. LINEAR BANDITS Recall form last lectures that in prediction with expert advise, at each time t, the player playsat∈ {e1,...,ek}and the adversary plays ztsuch that l(at,zt)≤1 for some loss function. One example of such loss function is linear function l(at,zt) =aT tztwhere|zt|∞≤ 1. Linear bandits are a more general setting where the player selects an action at∈ A ⊂Rk, whereAis a convex set and the adversary selects zTt∈ Zsuch that |ztat| ≤1. Similar to the prediction with expert advise, the regret is deﬁned as Rn=E/bracketleftBiggn n/summationdisplayTAtzt/bracketrightBigg −minTa zt, a∈Kt=1/summationdisplay t=1 whereAtis a random variable in A. Note that in the prediction with expert advise, the set Awas essentially apolyhedronandwehadminn Ta∈K aTzt=1 t= min1≤j≤ke zjt. However, in the linear bandit setting the minimizer of aTztcan be any point of the set Aand essentially",
    "the umber experts that the player tries to ”comp/summationtext ete” with are inﬁnity. Similar, to the prediction with expert advise we have two settings: 1Full feedback: after time t, the player observes zt. 2Bandit feedback: after time t, the player observes AT tzt, whereAtis the action that player has chosen at time t. We next, see if we can use the bounds we have developed in the prediction with expert advise in this setting. In particular, we have shown the following bounds for prediction with expert advise: 1Prediction with kexpert advise, full feedback: Rn√≤2nlogk. 2Prediction with kexpert advise, bandit feedback: Rn√≤2nklogk. The idea to deal with linear bandits is to discretize the set A. Suppose that Ais bounded (e.g.,A ⊂B2, whereB2is thel2ball inRk). We can use a1-covering ofnAwhich we have shown to be of size (smaller than) O(nk). This means there exist y1,...,y|N|such that for anya∈ A, there exist yisuch that ||yi−a|| ≤1. We now can bound the regret forn general case,",
    "where the experts can be any point in A, based on the regret on the discrete set,N={y1,...,y|N|},as follows. Rn=E/bracketleftBiggn/bracketrightBiggn /summationdisplayTAtzt t1−minTa zta∈A= t=1/bracketleftBiggn/summationdisplay n =E/summationdisplayTAtzt/bracketrightBigg −minTa zt+o(1). a∈Nt=1/summationdisplay t=1 Therefore, werestrictactions Attoacombinationoftheactionsthatbelongto {y1,...,y|N|} (we can always do this), then using the bounds for the prediction with expert advise, we obtain the following bounds: 11Linear bandit, full feedback: Rn≤/radicalbig 2nlog(nk) =O(√knlogn), which in terms of dependency to nis of order O(√n) that is what we expect to have. 2Linear bandit, bandit feedback: Rn≤/radicalbig 2nnklog(nk) = Ω(n), which is useless in terms of dependency of nas we expect to obtain O(√n) behavior. The topic of this lecture is to provide bounds for the linear bandit in the bandit feedback. Problem Setup: Let us recap the problem formulation: •at timet, player chooses action",
    "at∈ A ⊂[−1,1]k. •at timet, adversary chooses zt∈ Z ⊂Rk, whereaT tzt=/an}bracketle{tat,zt/an}bracketri}ht ∈[0,1]. •Bandit feedback: player observes /an}bracketle{tat,zt/an}bracketri}ht( rather than ztin the full feedback setup). Literature: O(n3/4) regret bound has been shown in [BB04]. Later on this bound has been improved to O(n2/3) in [BK04] and [VH06] with ”Geometric Hedge algorithm”, which we will describe and analyze below. We need the following assumption to show the results: Assumption: There exist δsuch that δe1,...,δe k∈ A. This assumption guarantees that Ahas full-dimension around zero. We also discretize Awith a1-net of size Cnkand only consider the resulting discrete setn and denote it by A, where|A| ≤(3n)k. All we need to do is to bound Rn=E/bracketleftBiggn/summationdisplayTAtzt/bracketrightBiggn −min/summationdisplayTa zt. a∈At=1 t=1 For anytanda, we deﬁne t−1expη t(/parenleftBig − zs=1ˆT sa p a) = ,t−1 a∈Aexp/summationtext/parenrightBig /parenleftBig −η",
    "zs=1ˆTsa/parenrightBig whereηis a parameter (that we will/summationtext choose later) an/summationtext dzˆtis deﬁnedto incorporate the idea of exploration versus exploitation. The algorithm which is termed Geometric Hedge Algorithm is as follows: At timetwe have •Exploitation: with probability 1 −γdrawataccording to ptand letzˆt= 0. •Exploration: with probabilityγletat=δejfor some 1k≤j≤kandzˆt= k j) 2/an}bracketle{t(akt,ztaγ/an}bracketri}htt=z eδ γtj. Note that δis the the parameter that we have by assumption on the set A, andηandγ are the parameters of the algorithm that we shall choose later. Theorem: Using Geometric Hedge algorithm for linear bandit with bandit feedback, withγ=1=g 1/3andη/radicalBig lon 4/3, we haven kn E[Rn]≤2/3Cn/radicalbig log3/2n k . 2Proof.Let the overall distribution of atbeqtdeﬁned as qt= (1−γ)pt+γU, whereUis a uniform distribution over the set {δe1,...,δe k}. Under this distribution, zˆtis an unbiased estimator of zt, i.e., kγk(Eat∼qt[zˆt] =",
    "0(1−j)γ)+/summationdisplay ztej=zt.k γj=1 following thesamelines of theproofthat wehadforanalyzingexponential weight algorithm, we will deﬁne 1 wt= e p z a∈A/parenleftBiggt− x−Tηaˆs s=1/parenrightBigg/summationdisplay/summationdisplay . We then have log/parenleftbiggwt+1/parenrightbigg = log/parenleftBigg/summationdisplayTpt(a)exp ηa zˆtwta∈/parenleftbig − A/parenrightBigg /parenrightbig e−2x≤1−x+x ≤2log/parenleftBigg/summationdisplayT1( )/parenleftbigg 1−ˆ +2(T2pta ηa z tη a zˆs)2a∈A/parenrightbigg/parenrightBigg (−T1= log/parenleftBigg 1+/summationdisplay )/parenleftbigg ˆ +2(T2pta ηa z tη a zˆt)2a∈A/parenrightbigg/parenrightBigg log(1+x)≤x ≤/summationdisplayT12pt(T2a)/parenleftbigg −ηa zˆt+η(a zˆt)2a∈A/parenrightbigg . Taking expectation from both sides leads to Eat∼qt/bracketleftbiggwt+1log/parenleftbigg wt/parenrightbigg/bracketrightbigg ≤ −ηEat∼qt/bracketleftBigg pt(a)Ta zˆt a∈A/bracketrightBigg/bracketleftBigg/summationdisplay η2 +TEat∼qt/summationdisplay2pt(a)(a",
    "zˆt)2a∈A/bracketrightBigg =−/bracketleftbig2 TηηEt∼ptatzˆt/bracketrightbig +2E a a∼qpt(a)(Ta zˆt)2t t/bracketleftBigg a/summationdisplay ∈A/bracketrightBigg 2qt=(1−γ)pt+γUη=− γEat∼qt/bracketleftbigT ηatzˆT 2E t a−γ/bracketrightbig +η1 −∼U1tzˆTa t+Eγt a2t∼qt/bracketleftBigg pt(a)(a zˆt) a∈A/bracketrightBigg aTzt≤1−η/bracketleftbig /bracketrightbig ηγ η2/bracketleftbig /bracketrightbig /summationdisplay t≤TEat∼qtatzˆt+ + ∼qt/bracketleftBigg/summationdisplayT2Eat pt(a)(a zˆ1−γ 1−t)γ2a∈A/bracketrightBigg . We next, take summation of the previous relation for t= 1 up to nand use a telescopic cancellation to obtain n nηT ηγη2 T2E[logw E+]≤[logw1]−E n1/bracketleftBigg/summationdisplay atzˆt/bracketrightBigg +n+E pt(a)(a zˆ1−t)γ 1γ2t=1−/bracketleftBigg/summationdisplay t=1a/summationdisplay ∈A/bracketrightBigg n nηγ η2 ≤E[log ]−ηE/bracketleftBigg/summationdisplayT Tatzˆ2w1 t/bracketrightBigg +n+E pt(a)(a zˆt).(6.1)1γ2t=1−/bracketleftBigg/summationdisplay",
    "t=1a∈A/bracketrightBigg/summationdisplay 3Note that for all a∗∈ Awe have n log(wn+1) = log/parenleftBigg /parenleftBiggn a/parenrightBigg/parenrightBigg/summationdisplay exp−η ∈A/summationdisplayTa zˆs s=1≥ −η/summationdisplay . s=/an}bracketle{t∗a ,zˆs 1/an}bracketri}ht UsingE[zˆs] =zs, leads to n E[log(wn+1)]≥ −η/summationdisplay∗a ,zs. (6.2) s=1/an}bracketle{t /an}bracketri}ht We also have that log(w1) = log|A| ≤2klogn. (6.3) Plugging (6.2) and (6.3) into (6.1), leads to γηE[Rn]≤n+E/bracketleftBiggn/bracketrightBigg/summationdisplay/summationdisplay pta)(T 2n(at)2klogzˆ+ . (6.4)1−γ2 ηt=1a∈A nIt remains to control the quadratic term E/bracketleftbig/summationtextp a aTz2 t=1a∈At( )( ˆ t) . We use the fact that |(j)zt|,|(j)at| ≤1 to obtain/summationtext /bracketrightbig E/bracketleftBiggn n /summationdisplay/summationdisplayT2 T2pt(a)(a zˆt) = p a)Et(qt[(a zˆt) ] t=1a∈A/bracketrightBigg/summationdisplay t=1a/summationdisplay ∈A n k =/summationdisplay/summationdisplay2γ k",
    "(j)pt(a)/parenleftbigg (1−γ)0+/summationdisplay/parenrightbigg [j2a zt]k γt=1a∈A j=1 (j)n|ajzt|≤1 ≤/summationdisplay/summationdisplay k2 (a)/parenleftbigg2 t/parenrightbiggkp =n.γ γt=1a∈A Plugging this bound into (6.4), we have ηk22klognE[Rn]≤γn+n+ .2γ η Lettinglognγ=1 /3andη=n1/radicalBig kn4/3leads to E[3/2 2/3Rn]≤Ck n/radicalbig logn. Literature: The bound we just proved has been improved in [VKH07] where they show O(d3/2√nlogn) bound with a better exploration in the algorithm. The exploration that we used in the algorithm was coordinate-wise. The key is that we have a linear problem and we can use better tools from linear regression such as least square estimation. However, we will describe a slightly diﬀerent approach in which we never explore and the exploration is com- pletely done with the exponential weighting. This approach also gives a better performance in terms of the dependency on k. In particular, we obtain the bound O(d√nlogn) which coincides with the bound",
    "recently shown in [BCK 12] using a John’s ellipsoid. 4−1Theorem: LetCt=Eat∼qt[a aT t],zˆt= (aTt tzt)Ctat, andγ= 0(sothat pt=qt). Using Geometric Hedge algorithm with η= 2/radicalBig lognfor linear bandit with bandit feedbackn leads to E[Rn]≤CK/radicalbig nlogn. Proof.We follow the same lines of the proof as the previous theorem to obtain (6.4). Note that the only fact that we used in order to obtain (6.4) is unbiasedness, i.e., E[zˆt] =zt, which holds here as well since E[zˆ−Et] = [1TCtatatzt] =−1CEt[Tatat]zt=zt. Note that we can use pseudo-inverse instead of inverse so that invertibility is not an issue. Therefore, rewriting (6.4) with γ= 0, we obtain /bracketleftBiggnη/summationdisplay/summationdisplayT22klognE[Rn]≤Eat∼pt pt(a)(a zˆt) ∈/bracketrightBigg + .2 ηt=1aA We now bound the quadratic term as follows n n T2 T2Eat∼pt/bracketleftBigg/summationdisplay/summationdisplay pt(a)(a zˆt)/bracketrightBigg =/summationdisplay/summationdisplay p a)Et(at∼pt(a zˆt) t=1a∈A",
    "t=1a∈A/bracketleftbig /bracketrightbig CT t=C−1n nTt, zˆt=(atzt)Ctat=/summationdisplay a)Tp aE t(zˆT 2tˆa= pt(T t a)Tz aE(−1T−1atzt)CtatatCta t=1a/summationdisplay ∈A/bracketleftbig /bracketrightbig/summationdisplay t=1a T/summationdisplay ∈A n n|a/bracketleftbig tzt|≤1 E/bracketrightbig /summationdisplay/summationdisplayT−1 T−1[a≤taT p(a)t]=CtT−1a CE t tatatCta= pt(a)a Cta t=1a∈A/bracketleftbig /bracketrightbig /summationdisplay t=1a/summationdisplay ∈A n n /summationdisplay/summationdisplaytr(AB)=tr(BA)= ( )tr(T−1 1pt r(−ta a C a) =/summationdisplay/summationdisplay pt(a)tTCtaa) t=1a∈A t=1a∈A n n n =/summationdisplay tr(−1CEta∼pt[Taa]) =/summationdisplay tr(−1CtCt) =tr(Ik) =kn. t=1 t=1/summationdisplay t=1 Plugging this bound into previous bound yields η2klognE[Rn]≤nk+ .2 η lognLettingη= 2/radicalBig , leads to E[Rn]n≤Ck√nlogn. References [BCK 12] Bubeck, Sbastien, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards mini- max policies for online linear optimization with bandit feedback",
    ". arXiv preprint arXiv:1202.3079 (2012). APA 5[BB04] McMahan, H. Brendan, and Avrim Blum. Online geometric optimization in the bandit setting against an adaptive adversary . Conference on Learning theory (COLT) 2004. [VH06] Dani, Varsha, and ThomasP. Hayes. Robbing the bandit: Less regret in online geo- metric optimization against an adaptive adversary . Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and Applied Mathematics, 2006. [BK04] Awerbuch, Baruch, and Robert D. Kleinberg. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches .Proceedings of thethirty- sixth annual ACM symposium on Theory of computing. ACM, 2004. [VKH07] Dani, Varsha, Sham M. Kakade, and Thomas P. Hayes, The price of bandit in- formation for online optimization , Advances in Neural Information Processing Systems. 2007. 6MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information",
    "about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_22.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 22 Scribe:Aden Forrow Nov. 30, 2015 7. BLACKWELL’S APPROACHABILITY 7.1 Vector Losses David Blackwell introduced approachability in 1956 as a generalization of zero sum game theory to vector payoﬀs. Born in 1919, Blackwell was the ﬁrst black tenured professor at UC Berkeley and the seventh black PhD in math in the US. Recall our setup for online linear optimization. At time t, we choose an action at∈∆K and the adversary chooses zt∈B∞(1). We then get a loss ℓ(at,zt) =/an}bracketle{tat,zt/an}bracketri}ht. In the full information case, where we observe ztand not just ℓ(at,zt), this is the same as prediction with expert advice. Exponential weights leads to a regret bound Rn≤/radicalbiggn 2log(K). Th e setup of a zero sum game is nearly identical: •Player 1 plays a mixed strategy p∈∆n. •Player 2 plays q∈∆m. •Player 1’s payoﬀ is p⊤Mq. HereMis the game’s payoﬀ matrix. Theorem: Von Neumann Minimax Theorem max min⊤p Mq=",
    "min max⊤p Mq. p∈∆nq∈∆m q∈∆mp∈∆n The minimax is called the value of the game. Each player can prevent the other from doing any better than this. The minimax theorem implies that if there is a good response pqto any individual q, then there is a silver bullet strategy pthat works for any q. Corollary: If∀q∈∆n,∃psuch that p⊤Mq≥c, then∃psuch that ∀q,p⊤Mq≥c. Von Neumann’s minimax theorem can be extended to more general sets. The following theorem is due to Sion (1958). Theorem: Sion’s Minimax Theorem LetAandZbe convex, compact spaces, and f:A×Z→R. Iff(a,·) is upper semicontinuous and quasiconcave on Z∀a∈Aand 1f(·,z) is lower semicontinuous and quasiconvex on A∀z∈Z, then inf supf(a,z) = sup inf f(a,z). a∈Az∈Z z∈aZ∈A (Note - this wasn’t given explicitly in lecture, but we do use it later.) Quasiconvex and quasiconcave are weaker conditions than convex and concave respectively. Blackwell looked at the case with vector losses. We have the following setup: •Player 1 plays a∈A •Player 2 plays",
    "z∈Z •Player 1’s payoﬀ is ℓ(a,z)∈dR We suppose AandZare both compact and convex, that ℓ(a,z) is bilinear, and that /bardblℓ(a,z)/bardbl ≤R∀a∈A,z∈Z. All norms in this section are Euclidean norms. Can we translate the minimax theorem directly to this new setting? That is, if we ﬁx a set S⊂dR, and if∀z∃asuch that ℓ(a,z)∈S, does there exist an asuch that ∀z ℓ(a,z)∈S? No. We’ll construct a counterexample. Let A=Z= [0,1],ℓ(a,z) = (a,z), and S={(a,z)∈[0,1]2:a=z}. Clearly, for any z∈Zthere is an a∈Asuch that a=zand ℓ(a,z)∈S, but there is no a∈Asuch that ∀z,a=z. Instead of looking for a single best strategy, we’ll play a repeated game. At time t, player 1 plays at=at(a1,z1,...,at−1,zt−1) and player 2 plays zt=zt(a1,z1,...,a t−1,zt−1). Player 1’s average loss after niterations is 1ℓ¯n=n/summationdisplay ℓ(at,zt)nt=1 Letd(x,S) be the distance between a point x∈dRand the set S, i.e. d(x,S) = inf s∈S/bardblx−s/bardbl. IfSis convex, the inﬁmum is a minimum attained only at the projection of xinS.",
    "Deﬁnition: AsetSisapproachable ifthereexistsastrategy at=at(a1,z1,...,a t−1,zt−1) ¯ such that lim n→∞d(ℓn,S) = 0. Whether a set is approachable depends on the loss function ℓ(a,z). In our example, we can choosea0= 0 and at=zt−1to get n1ℓ¯limn= lim/summationdisplay (zt−1,zt) = (z¯,z¯)∈S. n→∞ n→∞nt=1 So thisSis approachable. 7.2 Blackwell’s Theorem We have the same conditions on A,Z, andℓ(a,z) as before. 2Theorem: Blackwell’s Theorem LetSbe a closed convex set of2Rwith/bardblx/bardbl ≤R ∀x∈S. If∀z,∃asuch that ℓ(a,z)∈S, thenSis approachable. Moreover, there exists a strategy such that 2Rd ℓ¯(n,S)≤√n Proof.We’ll prove the rate; approachability of Sfollows immediately. The idea here is to transform the problem to a scalar one where Sion’s theorem applies by using half spaces. Suppose we have a half space H={x∈dR:/an}bracketle{tw,x/an}bracketri}ht ≤c}withS⊂H. By assumption, ∀z∃asuch that ℓ(a,z)∈H. That is, ∀z∃asuch that /an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c, or",
    "maxmin/an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c. z∈Z a∈A By Sion’s theorem, minmax/an}bracketle{tw,ℓ(a,z) a∈A z∈Z/an}bracketri}ht ≤c. So∃a∗ Hsuch that ∀z ℓ(a,z)∈H. This works for any Hcontaining S. We want to choose Htso thatℓ(at,zt) brings the ¯averageℓ ¯tcloser to Sthanℓt−1. An intuitive choice is to have the hyperplane Wbounding H ¯tbe the separating hyperplane between Sandℓt−1closest to S. This is Blackwell’s ¯ strategy: let Wbe the hyperplane through πt∈argminµ∈S/bardblℓt−1−µ/bardblwith normal vector ℓ¯t−1−πt. Then H={x∈dR:/an}bracketle{tx−πt,ℓ¯t−1−πt/an}bracketri}ht ≤0}. Finda∗ Hand play it. We need one more equality before proving convergence. The average loss can be ex- panded: t1ℓ¯t t−1 tt t t=−1 t1 1¯(ℓt−1−πt)+−πt+ℓtt t t Now we look at the distance of the average from S, using the above equation and the deﬁnition of πt+1: d¯(ℓ ,S)2ℓ¯t=/bardblt−πt+1/bardbl2 ≤ /bardblℓ¯t−πt/bardbl2 =/vextenddouble/vextenddouble/vextenddouble2t−1 1¯(ℓt−1−πt)+ (ℓt−πt)t/vextenddouble",
    "/vextenddoublet ℓt/vextenddouble =/parenleftbiggt−1/parenrightbigg2 d¯(ℓ2 π2tt1¯t−1,S) +/bardbl −/vextenddouble/vextenddouble /bardbl+2−/an}bracketle{tℓt−πt,ℓt t2 t2t−1−πt/an}bracketri}ht Sinceℓt∈H, the last term is negative; since ℓtandπtare both bounded by R, the middle 2term is bounded by4R 2. Letting µ2 t=t2d(ℓ¯2t,S) , we have a recurrence relationt µ2 t≤µ2 t−1+4R2, 3=−¯ℓ+1ℓimplying µ2 n≤4nR2. Rewriting in terms of the distance gives the desired bound, 2Rd ℓ¯(t,S)≤√n Note that this proof fails for nonconvex S. 7.3 Regret Minimization via Approachability Consider the case A= ∆KK,Z=B∞(1). As we showed before, exponential weights Rn≤ c/radicalbig nlog(K). We can get the same dependence on nwith an approachability-based strategy. First recall that n n1 1Rn=/summationdisplay 1ℓ(at,zt)−min ℓ(ej,zt)n n jnt=1/summationdisplay t=1 n n = m x/bracketleftBigg 1a/summationdisplay 1ℓ(at,zt) ℓ(ej,zt) jn nt=1−/summationdisplay t=1/bracketrightBigg If we deﬁne a vector average loss",
    "n1ℓ¯n=/summationdisplay (ℓ(at,zt)−ℓ(e1,zt),...,ℓ(aKR t,zt)e t=1−ℓ(K,zt))n∈, Rn ¯ ¯ n→0 if and only if all components of ℓnare nonpositive. That is, we need d(−ℓn,OK)→0, where−O={x∈KR:−1≤xi≤0, iK∀ }is the nonpositive orthant. Using Blackwell’s approachability strategy, we get Rn≤d(ℓ¯−n,O/radicalbigg K)≤c .nKn TheKdependence is worse than exponential weights,√ Kinstead of log( K). How do we ﬁnd a∗ H? As a concrete example, let K= 2. We need a/radicalbig ∗ Htp satisfy /an}bracketle{t∗w,ℓ(∗aH,z)/an}bracketri}ht=/an}bracketle{tw,/an}bracketle{taH,z/an}bracketri}hty−z/an}bracketri}ht ≤c for allz. Hereyis the vector of all ones. Note that c≥0 since 0 is in Sand therefore in H. Rearranging, /an}bracketle{t∗aH,z/an}bracketri}ht/an}bracketle{tw,y/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c, Choosing a∗ H=wwill work; the inequality reduces to/angbracketleftw,y/angbracketright /an}bracketle{tw,z/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c. Approachability in the",
    "banditsetting with only partial feedback is still an openproblem. 4References [Bla56] D. Blackwell, An analog of the minimax theorem for vector payoﬀs , Paciﬁc J. Math. 6 (1956), no. 1, 1–8 [Sio58] M. Sion, On general minimax theorems . Paciﬁc J. Math. 8 (1958), no. 1, 171–176. 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_23.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 23 Scribe: Jonathan Weed Dec. 2, 2015 1. POTENTIAL BASED APPROACHABILITY Last lecture, we saw Blackwell's celebrated Approachability Theorem, which establishes a procedure by which a player can ensure that the average (vector) payo in a repeated game approaches a convex set. The central idea was to construct a hyperplane separating the \u0016 convex set from the point `t1, the average loss so far. By projecting perpendicular to\u0000 this hyperplane, we obtained a scalar-valued problem to which von Neumann's minimax theorem could be applied. The set Sis approachable as long as we can always nd a \\silver bullet,\" a choice of action atfor which the loss vector `tlies on the side of the hyperplane containingS. (See Figure 1.) Figure 1: Blackwell approachability Concretely, Blackwell's Theorem also implied the existence of a regret-minimizing algo- rithm for expert advice. Indeed, if we de ne the vector loss `tby",
    "(`t)i=`(at;zt)\u0000`(ei;zt), then the average regret at time tis equivalent to the sup-norm distance between the average \u0016loss`tand the negative orthant. Approaching the negative orthant therefore corresponds to achieving sublinear regret. However, this reduction yielded suboptimal rates. To bound average regret, w the sup-norm distance by the Euclidean distance, which led to an extra factor ofpe replaced kappear- ing in our bound. In the sequel, we develop a more sophisticated version of approachability that allows us to adapt to the geometry of our problem. (Much of what follows resem- bles out development of the mirror descent algorithm, though the two approaches di er in crucial details.) 1.1 Potential functions We recall the setup of mirror descent, rst described in Lecture 13. Mirror descent achieved accelerated rates by employing a potential function which was strongly convex with respect 1to the given norm. In this case, we seek what is in some sense the opposite: a function whose",
    "gradient does not change too quickly. In particular, we make the following de nition. De nition: A function \b : I Rd!I R is a potential forS2I R if it satis es the following properties: \u000f\b is convex. \u000f\b(x)\u00140 forx2S. \u000f\b(y) = 0 fory2@S. \u000f\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u0014hx2k \u0000yk2, where by abuse of notation we use r\b(x) to denote a subgradient of \b at x. Given such a function, we recall two associated notions from the mirror descent algo- rithm. The Bregman divergence associated to \b is given by D\b(y;x) = \b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi: Likewise, the associated Bregman projection is \u0019(x) = argmin D\b(y;x): y2S We aim to use the function \b as a stand-in for the Euclidean distance that we employed in our proof of Blackwell's theorem. To that end, the following lemma establishes several properties that will allow us to generalize the notion of a separating hyperplane. Lemma: For any convex, closed set Sandz2S,x2SC, the following properties hold. \u000f hz\u0000\u0019(x);r\b(x)i\u0014 0; \u000f hx\u0000\u0019(x);r\b(x)i\u0015 \b(x): In particular, if \b",
    "is positive on SC, thenH:=fyjhy\u0000\b(x);r\b(x)i = 0g is a separating hyperplane. Our proof requires the following proposition, whose proof appears in our analysis of the mirror descent algorithm and is omitted here. Proposition: For allz2S, it holds hr\b(\u0019 (x))\u0000r\b(x);\u0019 (x)\u0000zi\u00140: Proof of Lemma. Denote by\u0019the projection \u0019(x). The rst claim follows upon expanding the expression on the left-hand side as follows hz\u0000\u0019;r\b(x)i =hz\u0000\u0019;r\b(x)\u0000r\b(\u0019 )i+hz\u0000\u0019;r\b(\u0019 )i: 2The above Proposition implies that the rst term is nonpositive. Since the function \b is convex, we obtain 0\u0015\b(z)\u0015\b(\u0019) +hz\u0000\u0019;r\b(\u0019 )i: Since\u0019lies on the boundary of S, by assumption \b( \u0019) = 0 and the claim follows. For the second claim, we again use convexity: \b(\u0019)\u0015\b(x) +h\u0019\u0000x;r\b(x)i: Since \b(\u0019 ) = 0, the claim follows. 1.2 Potential based approachability With the de nitions in place, the algorithm for approachability is essentially the same as it before we introduced the potential function. As before, we will use a projection de ned by the",
    "hyperplane H=fyjhy\u0000\u0016 \u0016\u0019(`t\u00001);r\b(`t= 0 and von Neumann's minmax theorem\u00001i g to nd a \\silver bullet\" a\u0003 tsuch that`t=`(a\u0003 t;zt) satis es h`t\u0000 \u0016\u0019t;r\b(`t\u00001)i\u0014 0: All that remains to do is to analyze this procedure's performance. We have the following theorem. Theorem: Ifk`(a;z )k\u0014Rholds for all z2A;z2Z and all assumptions above are satis ed, then 4R2hlogn\u0016\b(`n)\u0014 :n Proof. The de nition of the potential \b required that \b be upper bounded by a quadratic function. The proof below is a simple application of that bound. As before, we note the identity `\u0016 \u0016t t\u00001`t=`t\u00001+ :t This expression and the de nition of \b imply. 1 h\u0016\u0014 \u0016 )h \u0016 \b(`t\b(`t\u00001) +`t\u0000\u0016 \u0016`2t1; )\u0000r\b(`t1i+k`\u0000 t`t 22\u0000t\u00001tk: \u0016 The last term is the easiest to control. By assumption, `tand`t1are contained in a ball\u0000 of radiusR, sok`t\u0000\u0016`t\u00001k2\u00144R2. To bound the second term, write 1 1 1h \u0016 `t\u0000\u0016 \u0016 \u0016 \u0016 `t1;r\b(`t1)i=h`t\u0000\u0019t;r\b(`t1) +\u0019 ` ; \b(` ):t\u0000 \u0000 \u0000ithtt\u0000t\u00001rt\u00001i The rst term is nonpositive by assumption, since this is how the algorithm",
    "constructs the silver bullet. By the above Lemma, the inner product in the second term is at most \u0000\u0016\b(`t\u00001). We obtain \u0012t\u00001 2\u0016\b(`t)\u0014\u0013hR2\u0016\b(`t)\u00001+:t t2 3\u0000\u0016`\u0016 De ningut=t\b(`t) and rearranging, we obtain the recurrence 2hR2 ut\u0014ut\u00001+;t Son un=Xn ut\u0000ut\u00001\u00142hR2X1 tt=1 t=1 Applying the de nition of unproves the claim. 1.3 Application to regret minimization We now show that potential based approachability providespan improved bound on regret minimization. Our ultimate goal is to replace the bound nk(which we proved last lecture) bypnlogk(which we know to be the optimal bound for prediction with expert advice). We will be able to achieve this goal up to logarithmic terms in n. (A more careful analysis of the potential de ned below does actually yields an optimal rate.) Recall thatRn \u0016 =d(`n;On K\u0000), whereRnis the cumulative regret after nrounds and O\u00001 K is the negative orthant. It is not hard to see that d=kx+k, wherex+is the positive 11 part of the vector x. We de ne the following potential",
    "function: K1 1\b(x) = log\u00110 KX e\u0011(xj)+ j=11 : The function \b is a kind of \\soft max\" of the@ positive entriesA ofx. (Note that this de nition does not agree with the use of the term soft max in the literature|the di erence is the presence of the factor1.) The terminology soft max is justi ed by noting thatK 1 1kx+k= max(x log+logK logK j)+\u0014max e\u0011(xj)+ :\u0011\u0014\b(x) + 1j j\u0011 K \u0011 The potential function therefore serves as an upper bound on the sup distance, up to an additive logarithmic factor. The function \b de ned in this way is clearly convex and zero on the negative orthant. To verify that it is a potential, it remains to show that \b can be bounded by a quadratic. Away from the negative orthant, \b is twice di erentiable and we can compute the Hessian explicitly: r2\b(x) =\u0011diag(r\b(x))\u0000\u0011r\br\b>: For any vector usuch thatkuk2= 1, we therefore have K K u>r2\b(x)u =\u0011X u2 j(r\b(x ))j\u0000\u0011(u> jr\b(x))2\u0014\u0011 =1X u2 j( j=1r\b(x))j\u0014\u0011; sincekuk2= 1 andkr\b(x) 2k1\u00141. We conclude that r\b(x)\u0016\u0011I, which for nonnegative",
    "xandyimplies the bound \u0011\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u00142ky\u0000xk2: 4\u00144hR2logn:In fact, this bound holds everywhere. Therefore \b is a valid potential function for the negative orthant, with h=\u0011. The above theorem then implies that we can ensure Rn logK 4R2\u0011lognlogK\u0014 \u0016\b(`n) +\u0014 +:n \u0011 n \u0011 To optimize this bound, we pick \u0011=1q nlogKand obtain the bound2R logn Rn\u00144Rp nlognlogK: As alluded to earlier, a more careful analysis can remove the log nterm. Indeed, for this particular choice of \b, we can modify the above Lemma to obtain the sharper bound hx\u0000\u0019(x);r\b(x)i\u0015 2\b(x): When we substitute this expression into the above proof, we obtain the recurrence relationt\u0016\b(`t)\u00002 c\u0014 \u0016\b(`t\u00001) +:t t2 This small change is enough to prevent the appearance of log nin the nal bound. 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_3.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 3 Scribe:James Hirst Sep. 16, 2015 1.5 Learning with a ﬁnite dictionary Recall from the end of last lecture our setup: We are working with a ﬁnite dictionary H={h1,...,h M}of estimators, andwewouldliketounderstandthescaling ofthis problem with respect to Mand the sample size n. GivenH, one idea is to simply try to minimize ˆ the empirical risk based on the samples, and so we deﬁnethe empirical risk minimizer, herm, by ˆ ˆ herm∈argminRn(h). h∈H ˆ ˆ In what follows, we will simply write hinstead of hermwhen possible. Also recall the ¯ deﬁnition of the oracle, h, which (somehow) minimizes the true risk and is deﬁned by ¯h∈argminR(h). h∈H ˆ ¯ The following theorem shows that, although hcannot hope to do better than hin general, the diﬀerence should not be too large as long as the sample size is not too small compared to M. ˆ Theorem: The estimator hsatisﬁes ˆ ¯R(h)≤R(h)+/radicalbigg 2log(2M/δ) n with probability",
    "at least 1 −δ. In expectation, it holds that /radicalbigg 2log(2M)ˆ ¯IE[R(h)]≤R(h)+ .n Proof. ˆ ˆˆˆ¯ From the deﬁnition of h, we have Rn(h)≤Rn(h), which gives ˆ ¯ ˆ¯ ¯ ˆ ˆˆ R(h)≤R(h)+[Rn(h)−R(h)]+[R(h)−Rn(h)]. The only term here that we need to control is the second one, but since we don’t have ¯ any real information about h, we will bound it by a maximum over Hand then apply Hoeﬀding: log(2M/δ)ˆ¯ ¯ ˆ ˆˆ ˆ [Rn(h)−R(h)]+[R(h)−Rn(h)]≤2max|Rn(hj)−R(hj)| ≤2 j/radicalbigg 2n with probability at least 1 −δ, which completes the ﬁrst part of the proof. 1To obtain the bound in expectation, we start with a standard trick from probability which bounds a max by its sum in a slightly more clever way. Here, let {Zj}jbe centered random variables, then /bracketleftbigg /bracketrightbigg1/parenleftbigg /bracketleftbigg1IE max|Zj|= logexp sIE max|Zj|/bracketrightbigg/parenrightbigg ≤logIE/bracketleftbigg exp/parenleftbigg smax|Zj| j s j s j/parenrightbigg/bracketrightbigg , where the last inequality",
    "comes from applying Jensen’s inequality to the convex function exp(·). Now we bound the max by a sum to get 2M1 )≤log/summationdisplay 1 s2log(2M sIE[exp(sZj)]≤log/parenleftbigg 2Mexp/parenleftbigg /parenrightbigg/parenrightbigg = + ,s s 8n s 8nj=1 ˆ where we used Zj=Rn(hj)−R(hj) in our case and then applied Hoeﬀding’s Lemma. Bal- ancing terms by minimizing over s, this gives s= 2/radicalbig 2nlog(2M) and plugging in produces /bracketleftbigglog(2M)ˆ IE max|Rn(hj)−R(hj)| ≤ j/bracketrightbigg/radicalbigg ,2n which ﬁnishes the proof. 2. CONCENTRATION INEQUALITIES Concentration inequalities are results that allow us to bound the deviations of a function of randomvariablesfromitsaverage. Theﬁrstofthesewewillconsiderisadirect improvement to Hoeﬀding’s Inequality that allows some dependence between the random variables. 2.1 Azuma-Hoeﬀding Inequality Given a ﬁltration {Fi}iof our underlying space X, recall that {∆i}iare called martingale diﬀerences if, for every i, it holds that ∆ i∈ Fiand",
    "IE[∆ i|Fi] = 0. The following theorem gives a very useful concentration bound for averages of bounded martingale diﬀerences. Theorem (Azuma-Hoeﬀding): Suppose that {∆i}iare margingale diﬀerences with respect to the ﬁltration {Fi}i, and let Ai,Bi∈ Fi−1satisfyAi≤∆i≤Bialmost surely for every i. Then IP/bracketleftBigg 1/summationdisplay 2n∆i> t/bracketrightBigg 2t2 ≤expni/parenleftbigg −/summationtextn i=1/bardblBi−Ai/bardbl2∞/parenrightbigg . In comparison to Hoeﬀding’s inequality, Azuma-Hoeﬀding aﬀords not only the use of non-uniform boundedness, but additionally requires no independence of the random vari- ables. Proof.We start with a typical Chernoﬀ bound. IP/bracketleftBigg /bracketrightBigg/summationdisplay ∆i> t≤IE/bracketleftBig es/summationtext∆i/bracketrightBig e−st= IE i/bracketleftBig IE/bracketleftBig es/summationtext∆i|Fn−1/bracketrightBig/bracketrightBig e−st 2n−1 n−1 2 2= IE/bracketleftBig es/summationtext∆iIE[es∆n|Fn1]e−st− ≤IE[es/summationtext∆i·es(Bn−An)/8]e−st, where",
    "we have used the fact that the ∆/bracketrightBig i,i < n, are allFnmeasureable, and then applied Hoeﬀding’s lemma on the inner expectation. Iteratively isolating each ∆ ilike this and applying Hoeﬀding’s lemma, we get IP/bracketleftBiggn/summationdisplay s2 ∆> t/bracketrightBigg ≤exp/parenleftBigg/summationdisplay /bardblB−A/bardbl2/parenrightBigg e−sti i i8∞. i i=1 Optimizing over sas usual then gives the result. 2.2 Bounded Diﬀerences Inequality Although Azuma-Hoeﬀding is a powerful result, its full generality is often wasted and can be cumbersome to apply to a given problem. Fortunately, there is a natural choice of the {Fi}iand{∆i}i, giving a similarly strong result which can be much easier to apply. Before we get to this, we need one deﬁnition. Deﬁnition (Bounded Diﬀerences Condition): Letg:X →IR and constants cibe given. Then gis said to satisfy the bounded diﬀerences condition (with constants ci) if sup|g(x ,...,x )−g(x ,...,x′1 n 1 i,...,x n)| ≤ci x′1,...,xn,xi for every i.",
    "Intuitively, gsatisﬁes the bounded diﬀerences condition if changing only one coordinate ofgat a time cannot make the value of gdeviate too far. It should not be too surprising that these types of functions thus concentrate somewhat strongly around their average, and this intuition is made precise by the following theorem. Theorem (Bounded Diﬀerences Inequality): Ifg:X →IR satisﬁes the bounded diﬀerences condition, then 2t2 IP[|g(X1,...,X n)−IE[g(X1,...,X n)|> t]≤2exp/parenleftbigg −/summationtext ic2 i/parenrightbigg . Proof.Let{Fi}ibe given by Fi=σ(X1,...,X i), and deﬁne the martingale diﬀerences {∆i}iby ∆i= IE[g(X1,...,X n)|Fi]−IE[g(X1,...,X n)|Fi−1]. Then IP/bracketleftBigg |/summationdisplay ∆i|> t/bracketrightBigg = IP/vextendsingle g(X1,...,X n)−IE[g(X1,...,X n) i/vextendsingle > t , exactly the quantity we want to bou/bracketleftbig/vextendsingle nd. Now, note that/vextendsingle/bracketrightbig ∆i≤IE/bracketleftbigg supg(X1,...,x i,...,X n)|Fi−IE[g(X1,...,X n)|Fi−1]",
    "xi/bracketrightbigg 3= IE/bracketleftbigg supg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1 xi/bracketrightbigg =:Bi. Similarly, ∆i≥IE/bracketleftbigg infg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1=:Ai. xi/bracketrightbigg At this point, our assumption on gimplies that /bardblBi−Ai/bardbl∞≤cifor every i, and since Ai≤∆i≤BiwithAi,Bi∈ Fi−1, an application of Azuma-Hoeﬀding gives the result. 2.3 Bernstein’s Inequality Hoeﬀding’s inequality is certainly a powerful concentration inequality for how little it as- sumes about the random variables. However, one of the major limitations of Hoeﬀding is just this: Since it only assumes boundedness of the random variables, it is completely obliv- ious to their actual variances. When the random variables in question have some known variance, an ideal concentration inequality should capture the idea that variance controls concentration to some degree. Bernstein’s inequality does exactly this. Theorem (Bernstein’s Inequality): LetX1,...,X nbe independent,",
    "centered ran- dom variables with |X| ≤cfor every i, and write σ2=n−1i iVar(Xi) for the average variance. Then/summationtext IP/bracketleftBigg 1/summationdisplay nt2 Xi> t/bracketrightBigg ≤exp/parenleftBigg −n 2σ2+2tci 3/parenrightBigg . Here, one should think of tas being ﬁxed and relatively small compared to n, so that strength of the inequality indeed depends mostly on nand 1/σ2. Proof.The idea of the proof is to do a Chernoﬀ bound as usual, but to ﬁrst use our assumptions on the variance to obtain a slightly better bound on the moment generating functions. To this end, we expand ∞(sk ∞X) skck−2iIE[esXi] = 1+IE[ sXi]+IE/bracketleftBigg /bracketrightBigg/summationdisplay ≤1+Var(Xi)/summationdisplay ,k! k!k=2 k=2 where we have used IE[ Xk i]≤IE[X2 i|Xi|k−2]≤Var(Xk−2i)c. Rewriting the sum as an exponential, we get esc sXi2 −sc−1IE[e]≤sVar(Xi)g(s), g(s) := .c2s2 The Chernoﬀ bound now gives IP/bracketleftBigg 1/summationdisplay Xi> t/bracketrightBigg ≤exp/parenleftBigg",
    "inf[s2(/summationdisplay Var(Xi))g(s)−nst]/parenrightBigg = exp/parenleftbigg n·inf[s2σ2g(s)−st],n s>0 s>0i i/parenrightbigg and optimizing this over s(a fun calculus exercise) gives exactly the desired result. 43. NOISE CONDITIONS AND FAST RATES ˆ To measure the eﬀectiveness of the estimator h, we would like to obtain an upper bound ˆ ˆ on the excess risk E(h) =R(h)−R(h∗). It should be clear, however, that this must depend signiﬁcantly on the amount of noise that we allow. In particular, if η(X) is identically equal ˆ to 1/2, then we should not expect to be able to say anything meaningful about E(h) in general. Understanding this trade-oﬀ between noise and rates will be the main subject of this chapter. 3.1 The Noiseless Case A natural (albeit somewhat na¨ ıve) case to examine is the completely noiseless case. Here, we will have η(X)∈ {0,1}everywhere, Var( Y|X) = 0, and E(h) =R(h)−R(h∗) = IE[|2η(X)−1|1I(h(X) =h∗(X))] = IP[h(X) =h∗(X)]. Let us now denote ¯ ˆ Zi= 1I(h(Xi) =Yi)−1I(h(Xi)",
    "=Yi), ¯ and write Zi=Zi−IE[Zi]. Then notice that we have ˆ ¯ |Zi|= 1I(h(Xi) =h(Xi)), and Var(Zi)≤IE[Z2ˆ ¯i] = IP[h(Xi) =h(Xi)]. ˆ For any classiﬁer hj∈ H, we can similarly deﬁne Zi(hj) (by replacing hwithhjthrough- out). Then, to set up an application of Bernstein’s inequality, we can compute n1/summationdisplay¯ Var(Zi(hj))≤IP[hj(Xi) =h(Xi)] =:σ2 nj. i=1 At this point, we will make a (fairly strong) assumption about our dictionary H, which ¯ is thath∗∈ H, which further implies that h=h∗. Since the random variables Zicompare ¯ ˆ toh, this will allow us to use them to bound E(h), which rather compares to h∗. Now, ¯ applying Bernstein (with c= 2) to the {Zi(hj)}ifor every jgives /bracketleftBiggn1/bracketrightBigg/summationdisplay nt2δ¯ IP Zi(hj)> t≤exp/parenleftBigg − =2σ2 i=1 j+4t3/parenrightBigg :,n M and a simple computation here shows that it is enough to take /radicalBigg 2σ2 jlog(M/δ)4t≥max ,log(M/δ)n 3n =:t0(j) ¯ for this to hold. From here, we may use the assumption h=h∗to",
    "conclude that ˆ ˆ IP/bracketleftBig E(h)> t0(ˆj)/bracketrightBig ≤δ, hˆ=h.j 5/ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslashˆ However, we also know that σ2 ˆ≤ E(h), which implies thatj /radicalBigg ˆ2E(h)log(M/δ)4ˆE(h)≤max ,log(M/δ)n 3n  ˆ with probability 1 −δ, and solving for E(h) gives the improved rate log(M/δ)ˆE(h)≤2 .n 6MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_4.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 4 Scribe:Cheng Mao Sep. 21, 2015 In this lecture, we continue to discuss the eﬀect of noise on the rate of the excess risk ˆ ˆ ˆ E(h) =R(h)−R(h∗) wherehis the empirical risk minimizer. In the binary classiﬁcation model, noise roughly means how close the regression function ηis from1 2.In particular, if η=1then we observe only noise, and if η∈ {0,1}we are in the noiseless case which has2 been studied last time. Especially, we achieved the fast ratelogMin the noiseless case byn¯ assuming h∗∈ Hwhich implies that h=h∗. This assumption was essential for the proof and we will see why it is necessary again in the following section. 3.2 Noise conditions The noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess risk is when the noise is present but can be controlled. Instead of the condition η∈ {0,1}, we can control the noise by assuming that ηis uniformly bounded away from1 2,",
    "which is the motivation of the following deﬁnition. Deﬁnition (Massart’s noise condition): The noise in binary classiﬁcation is said to satisfy Massart’s condition with constant γ∈(0,1 2]if|η(X)−1| ≥γalmost surely.2 Once uniform boundedness is assumed, the fast rate simply follows from last proof with appropriate modiﬁcation of constants. ˆ ˆ ˆ Theorem: LetcE(h) denote the excess risk of the empirical risk minimizer h=herm. If Massart’s noise condition is satisﬁed with constant γ, then log(M/δ)ˆE(h)≤γn with probability at least 1 −δ. (In particular γ=1gives exactly the noiseless case.)2 Proof. ¯ ¯ DeﬁneZi(h) = 1I(h(Xi) =Yi)−1I(h(Xi) =Yi). By the assumption h=h∗and the ˆ ˆ deﬁnition of h=herm, ˆ ˆ ¯E(h) =R(h)−R(h) ˆˆˆ¯ˆ¯ˆˆ ¯ ˆ =Rn(h)−Rn(h)+Rn(h)−Rn(h)−R(h)−R(h) (3.1) n1ˆ ≤ )/parenleftbig /parenrightbig /summationdisplay/parenleftbigˆ Zi(h)−IE[Zi(h]/parenrightbig . (3.2)ni=1 Hence it suﬃces to bound the deviation of/summationtext iZifrom its expectation. To this end, we hope to apply",
    "Bernstein’s inequality. Since Var[Zi(h)]≤IE[Z2 ¯i(h) ] = IP[h(Xi) =h(Xi)], 1/ne}ationslash /ne}ationslash /ne}ationslashwe have that for any 1 ≤j≤M, n1/summationdisplay¯ Var[Zi(hj)]≤IP[hj(X) =h(X)] =:σ2 nj. i=1 Bernstein’s inequality implies that n/bracketleftbig1/summationdisplay /bracketrightbig /parenleftbig nt2 IP ( Zi(hj)−IE[Zi(hj)])> t≤exp−n 2σ2 i=1 j+2 3t/parenrightbigδ=:.M Applying a union bound over 1 ≤j≤Mand taking 2σ2 jlog(M/δ)2log(M/δ)t=t0(j) := max/radicalBigg /parenleftbig ,n 3n/parenrightbig , we get that n1/summationdisplay (Zi(hj)−IE[Zi(hj)])≤t0(j) (3.3)ni=1 for all 1≤j≤Mwith probability at least 1 −δ. ˆ Suppose h=hˆ. It follows from (3.2) and (3.3) that with probability at least 1 −δ,j ˆE(h)≤tˆ0(j). (Note that so far the proof is exactly the same as the noiseless case.) Since |η(X)−1 2| ≥γ ¯ a.s. and h=h∗, ˆ ˆ ¯ E(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))]≥2γIP[hˆ(X) =h(X)] = 2γσ2 ˆ.j j Therefore,/radicalBigg ˆE(h)log(M/δ)2log(M/δ)ˆE(h)≤max , , (3.4)γn 3n so we conclude that",
    "with probabilit/parenleftbig y at least 1 −δ,/parenrightbig log(M/δ)ˆE(h)≤ .γn ¯ The assumption that h=h∗was used twice in the proof. First it enables us to ignore the approximation error and only study the stochastic error. More importantly, it makes the excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess risk to get the fast rate. Massart’s noise condition is still somewhat strong because it assumes uniform bounded- ness ofηfrom1 2. Instead, we can allow ηto be close to1 2but only with small probability, and this is the content of next deﬁnition. 2/ne}ationslash /ne}ationslash /ne}ationslashDeﬁnition (Tsybakov’s noise condition or Mammen-Tsybakov noise condi- tion):The noise in binary classiﬁcation is said to satisfy Tsybakov’s condition if there existsα∈(0,1),C10>0 andt0∈(0,2] such that 1 αIP[|η(X)−| ≤t]≤C10t−α 2 for allt∈[0,t0]. αIn particular, as α→1,t1−α→0 α, so this recovers Massart’s condition with γ=t0and we have the fast rate. As",
    "α→0,t1−α→1, so the condition is void and we have the slow rate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose order depends on α. We will see that this is indeed the case. Lemma: Under Tsybakov’s noise condition with constants α,C0andt0, we have IP[h(X) =h∗(X)]≤CE(h)α for any classiﬁer hwhereC=C(α,C0,t0) is a constant. Proof.We have E(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))] 1≥IE[|2η(X)−1|1I(|η(X)−|> t)1I(h(X) =h∗(X))]2 1≥2tIP[|η(X)− |> t,h(X) =h∗(X)]2 1≥2tIP[h(X) =h∗(X)]−2tIP[|η(X)−| ≤t]2 1≥2tIP[h(X) =h∗(X)]−2C0t1−α 1where Tsybakov’s condition was used in the last step. Take t=cIP[h(X) =h∗(X)]−α αfor some positive c=c(α,C0,t0) to be chosen later. We assume that c≤t0to guarantee that t∈[0,t0]. Sinceα∈(0,1), E(h)≥2cIP[h(X) =h∗(X)]1/α1−2C c1−αIP[h(X) =h∗10 (X)]/α ≥cIP[h(X) =h∗(X)]1/α by selecting csuﬃciently small depending on αandC0. Therefore 1IP[h(X) =h∗(X)]≤E(h)α cα and choosing C=C(α,C0,t0) :=c−αcompletes the proof. Having established the key lemma,",
    "we are ready to prove the promised fast rate under Tsybakov’s noise condition. 3/ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslashTheorem: If Tsybakov’s noise condition is satisﬁed with constant α,C0andt0, then there exists a constant C=C(α,C0,t0) such that l )ˆEh)≤C/parenleftbigog(M/δ(1 n/parenrightbig 2−α with probability at least 1 −δ. This rate of excess risk parametrized by αis indeed an interpolation of the slow ( α→0) ˆ and the fast rate ( α→1). Futhermore, note that the empirical risk minimizer hdoes not depend on the parameter αat all! It automatically adjusts to the noise level, which is a very nice feature of the empirical risk minimizer. Proof.The majority of last proof remains valid and we will explain the diﬀerence. After establishing that ˆE(h)≤t0(ˆj), we note that the lemma gives ˆσ2 ¯ ˆˆ= IP[h(X)/ne}ationslash=h(X)]≤CE(h)α.j It follows tha t /radicalBigg ˆ",
    "/parenleftbig2CE(h)αlog(M/δ)2log(M/δ)ˆE(h)≤max ,n 3n/parenrightbig /parenleftBig2ClogM2ˆ1 E(h)≤max/parenleftbigδ/parenrightbig 2−αlog(M/δ),/parenrightBig .n 3na nd thus 4. VAPNIK-CHERVONENKIS (VC) THEORY The upper bounds proved so far are meaningful only for a ﬁnite dictionary H, because if M=|H|is inﬁnite all of the bounds we have will simply be inﬁnity. To extend previous results to the inﬁnite case, we essentially need the condition that only a ﬁnite number of elements in an inﬁnite dictionary Hreally matter. This is the objective of the Vapnik- Chervonenkis (VC) theory which was developed in 1971. 4.1 Empirical measure Recall from previous proofs (see (3.1) for example) that the key quantity we need to control is ˆ2sup/parenleftbig Rn(h)−R(h). h∈H Instead of the union bound which would not work in th/parenrightbig e inﬁnite case, we seek some bound that potentially depends on nand the complexity of the set H. One approach is to consider some metric structure on Hand hope that if",
    "two elements in Hare close, then the quantity evaluated at these two elements are also close. On the other hand, the VC theory is more combinatorial and does not involve any metric space structure as we will see. 4By deﬁnition n1ˆRn(h)−R(h) =/summationdisplay/parenleftbig 1I(h(Xi) =Yi)−IE[1I(h(Xi) =Yi)]ni=1/parenrightbig . LetZ= (X,Y) andZi= (Xi,Yi), and let Adenote the class of measurable sets in the sample space X ×{0,1}. For a classiﬁer h, deﬁneAh∈ Aby {Zi∈Ah}={h(Xi) =Yi}. Moreover, deﬁne measures µnandµonAby n1µn(A) =/summationdisplay 1I(Zi∈A) and µ(A) = IP[Zi∈A]ni=1 forA∈ A. With this notation, the slow rate we proved is just log(2|A|/δ)ˆsupRn(h)−R(h) = sup|µn(A)−µ(A)| ≤ h∈H A∈A/radicalbigg .2n Since this is not accessible in the inﬁnite case, we hope to use one of the concentration inequalities to give an upperbound. Note that µn(A) is a sum of random variables that may not be independent, so the only tool we can use now is the bounded diﬀerence inequality. If we change the",
    "value of only one ziin the function z1,...,zn/mapsto→sup|µn(A)−µ(A)|, A∈A the value of the function will diﬀer by at most 1 /n. Hence it satisﬁes the boundeddiﬀerence assumption with ci= 1/nfor all 1≤i≤n. Applying the bounded diﬀerence inequality, we get that /vextendsinglelog(2/δ) /vextendsinglesup|µn(A)−µ(A)|−IE[sup|µn(A)−µ(A)|]≤ A∈A A∈A/radicalbigg 2n with probability/vextendsingle at least 1 −δ. Note that this already preclu/vextendsingle/vextendsingle /vextendsingle des any fast rate (faster than n−1/2). Toachieve fast rate, weneedTalagrand inequality andlocalization techniques which are beyond the scope of this section. It follows that with probability at least 1 −δ, log(2/δ)sup|µn(A)−µ(A)| ≤IE[sup|µn(A)−µ(A)|]+ A A∈A/radicalbigg . A∈ 2n We will now focus on bounding the ﬁrst term on the right-hand side. To this end, we need a technique called symmetrization, which is the subject of the next section. 4.2 Symmetrization and Rademacher complexity Symmetrization is a frequently",
    "used technique in machine learning. Let D={Z1,...,Z n} be the sample set. To employ symmetrization, we take another independent copy of the sample set D′={Z′ 1,...,Z′ n}. This sample only exists for the proof, so it is sometimes referred to as a ghost sample. Then we have n n1 1µ(A) = IP[Z∈A] = IE[/summationdisplay 1I(Z′ i∈A)] = IE[ 1I( Z′ i∈A)|D] = IE[µ′ n nn(A)|D] i=1/summationdisplay i=1 5/ne}ationslash /ne}ationslash /ne}ationslashnwhereµ′ n:=1/summationtext i=11I(Z′ i∈A). Thus by Jensen’s inequality,n IE[sup|µn(A)−µ(A)|] = IE/bracketleftbig sup/vextendsingle/vextendsingle µn(A)−IE[µ′ n(A)|D] A∈A A∈A ≤IE sup IE[ |µn(A)−µ′ n(A)||D/vextendsingle/vextendsingle ]/bracketrightbig ≤/bracketleftbig A∈A IE/bracketleftbig sup|µ′n(A)−µn(A)|/bracketrightbig A∈A n1/bracketrightbig = IE/bracketleftbig sup/vextendsingle/summationdisplay/parenleftbig 1I(Z′i∈A)−1I(Z A∈Ani∈A) i=1/parenrightbig/vextendsingle/bracketrightbig . SinceD′has the same distribution of D, by sy/vextendsingle mmetry 1I(",
    "Zi∈A)−1I(Z′/vextendsingle i∈A) has the same distribution as σi/parenleftbig 1I(Zi∈A)−1I(Z′ i∈A)/parenrightbig whereσ1,...,σ nare i.i.d. Rad(1 2), i.e. 1IP[σi= 1] = IP[ σi=−1] =,2 andσi’s are taken to be independent of both samples. Therefore, n IE[sup|µn(A)−µ(A)|]≤IE AA/bracketleftbig sup′ ∈ A∈A/vextendsingle/vextendsingle1/summationdisplay σi/parenleftbig 1I(Zi∈A)−1I(Zni∈A) i=1 n/parenrightbig/vextendsingle/bracketrightbig ≤2IE/bracketleftbig1/vextendsingle sup/vextendsingle/summationdisplay σi1I(Zi∈A). A∈Ani=1/vextendsingle/bracketrightbig (4.5) Usingsymmetrization we have boundedIE[sup/vextendsingle A∈A|µn(A)−µ(A)|/vextendsingle ] by amuch nicer quantity. Yet we still need an upper bound of the last quantity that depends only on the structure ofAbut not on the random sample {Zi}. This is achieved by taking the supremum over allzi∈ X ×{0,1}=:Y. Deﬁnition: The Rademacher complexity of a family of sets Ain a space Yis deﬁned to be the quantity n Rn(A) = sup",
    "sup/vextendsingle1IE/bracketleftbig /summationdisplay σi1I(zi∈A) z1,...,zn∈YA∈Ani=1/vextendsingle/bracketrightbig . The Rademacher complexity of a set B⊂I/vextendsingle Rnis deﬁned to b/vextendsingle e n1Rn(B) = IE/bracketleftbig sup b∈B/vextendsingle/vextendsingle n/summationdisplay σibi i=1/vextendsingle/vextendsingle/bracketrightbig . We conclude from (4.5) and the deﬁnition that IE[sup|µn(A)−µ(A)|]≤2Rn(A). A∈A nIn the deﬁnition of Rademacher complexity of a set, the quantity1sni=1σibimeasure how well a vector b∈Bcorrelates with a random sign pattern {σi}. The more complex Bis, the better some vector in Bcan replicate a sign pattern. In/vextendsingle/vextendsingle pa/summationtext rticular, i/vextendsingle/vextendsingle fBis the full hypercube [ −1,1]n, thenRn(B) = 1. However, if B⊂[−1,1]ncontains only k-sparse 6vectors, then Rn(B) =k/n. Hence Rn(B) is indeed a measurement of the complexity of the setB. The set of vectors to our interest in the deﬁnition of Rademacher complexity of",
    "Ais T(z) :={(1I(z1∈A),...,1I(zn∈A))T,A∈ A}. Thus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these vectors can replicate as Aranges over A. Although the cardinality of Amay be inﬁnite, the cardinality of T(z) is bounded by 2n. 7MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_5.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture Scribe:Vira Semenova andPhilippe Rigollet Sep. 23, 2015 In this lecture, we complete the analysis of the performance of the empirical risk mini- mizer under a constraint on the VC dimension of the family of classiﬁers. To that end, we will see how to control Rademacher complexities using shatter coeﬃcients. Moreover, we will see how the problem of controlling uniform deviations of the empirical measure µnfrom the true measure µas done by Vapnik and Chervonenkis relates to our original classiﬁcation problem. 4.1 Shattering Recall from the previous lecture that we are interested in sets of the form T(z) :=/braceleftbig (1I(z1∈A),...,1I(zn∈A)),A∈ A, z= (z1,...,zn). (4.1) In particular, the cardinality of T(z), i.e., the numbe/bracerightbig r of binary patterns these vectors can replicate as Aranges over A, will be of critical importance, as it will arise when controlling the Rademacher complexity. Although the",
    "cardinality of Amay be inﬁnite, the cardinality of T(z) is always at most 2n. When it is of the size 2n, we say that Ashatters the setz1,...,zn. Formally, we have the following deﬁnition. Deﬁnition: A collection of sets Ashatters the set of points {z1,z2,...,zn} card{(1I(z1∈A),...,1I(zn∈A)),A∈ A}= 2n. The sets of points {z1,z2,...,zn}that we are interested are realizations of the pairs Z1= (X1,Y1),...,Z n= (Xn,Yn) and may, in principle take any value over the sample space. Therefore, we deﬁne the shatter coeﬃcient to be the largest cardinality that we may obtain. Deﬁnition: Theshatter coeﬃcients of a class of sets Ais the sequence of numbers {SA(n)}n≥1, where for any n≥1, SA(n) = sup card (1I( z1A),...,1I(znA)),A z1,...,zn{ ∈ ∈ ∈ A} and the suprema are taken over the whole sample space. Bydeﬁnition,the nthshattercoeﬃcient SA(n)isequalto2nifthereexistsaset {z1,z2,...,zn} thatAshatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di- mension. Deﬁnition: The",
    "Vapnik-Chervonenkis dimension, or VC-dimension of dVCAis the largest integerdsuch that SA(d) = 2 . We write ( A) =d. 15IfSA(n) = 2nfor all positive integers n, thenVC(A) :=∞ In words, Ashatters someset of points of cardinality dbut shatters noset of points of cardinality d+1. In particular, Aalso shatters no set of points of cardinality d′> dso that the VC dimension is well deﬁned. Inthesequel, wewillseethattheVCdimensionwillplaytherolesimilartoofcardinality, but on an exponential scale. For interesting classes Asuch that card( A) =∞, we also may haveVC(A)<∞. For example, assume that Ais the class of half-lines ,A={(−∞,a],a∈ IR}∪ {[a,∞),a∈IR}, which is clearly inﬁnite. Then, we can clearly shatter a set of size 2 but we for three points z1,z2,z3,∈IR, if for example z1< z2< z3, we cannot create the pattern (0 ,1,0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros followed by ones or with ones followed by zeros but not an alternating pattern like (0 ,1,0). 00",
    "10 01 11000 100 110 111 001 011 101 Figure 1: If A={halﬂines}, then any set of size n= 2 is shattered because we can create all 2n= 4 0/1 patterns (left); if n= 3 the pattern (0 ,1,0) cannot be reconstructed: SA(3) = 7<23(right). Therefore, VC(A) = 2. 4.2 The VC inequality We have now introducedall the ingredients necessary tostate themain result of this section: the VC inequality. Theorem (VC inequality): For any family of sets Awith VC dimension VC(A) =d, it holds /radicalbigg 2dlog(2en/d)IE sup|µn(A)−µ(A)| ≤2 A∈A n Notethatthisresultholdsevenif AisinﬁniteaslongasitsVCdimensionisﬁnite. Moreover, observe that log( |A|) has been replaced by a term of order dlog 2en/d. To prove the VC inequality, we proceed in three steps:/parenleftbig /parenrightbig 21. Symmetrization, to bound the quantity of interest by the Rademacher complexity: IE[sup|µn(A)−µ(A)|]≤2Rn( ) A∈AA. We have already done this step in the previous lecture. 2. Control of the Rademacher complexity using shatter coeﬃcients.",
    "We are going to show that gR(A)≤/radicaligg 2lo n/parenleftbig 2SA(n) n/parenrightbig 3. We are going to need the Sauer-Shelah lemma to bound the shatter coeﬃcients by the VC dimension. It will yield S(n)≤/parenleftigen/parenrightigd , d=VC A (dA). Put together, these three steps yield the VC inequality. Step 2: Control of the Rademacher complexity We prove the following Lemma. Lemma: For anyB⊂IRn, such that |B|<∞:, it holds n/bracketleftbig/vextendsingle1 2σ/vextendsingle )B/bracketrightbig (Rn( ) = IE max /vextendsingle/summationdisplay log 2B ibi/vextendsingle≤max| | b∈Bn b∈Bi=1|b|2/radicalbig n where|·|2denotes the Euclidean norm. Proof.Note that 1Rn(B) = IEn/bracketleftbig maxZb, b∈B| whereZb=/summationtextn i=1σibi. In particular, since/vextendsingle/vextendsingle/bracketrightbig −|bi| ≤σi|bi| ≤ |bi|, a.s., Hoeﬀding’s lemma implies that the moment generating function of Zbis controlled by n n IE/bracketleftbig exp(sZb)/bracketrightbig =/productdisplay IE i=1/bracketleftbig",
    "exp(sσibi)/bracketrightbig ≤/productdisplay exp(s2b2 i/2) = exp( s2b2 2/2) (4.2) i=1| | Next, to control IE max b∈BZb|, we use the same technique as in Lecture 3, section 1.5. ¯ To that end, deﬁne/bracketleftbig B=B∪/vextendsingle/vextendsingle {−B/bracketrightbig }and observe that for any s >0, IE/bracketleftbigg1max|Zb|/bracketrightbigg = IE/bracketleftbigg maxZb/bracketrightbigg = logexp/parenleftbigg sIE/bracketleftbigg maxZb/bracketrightbigg/parenrightbigg1≤logIE exp smaxZb, b∈B b∈B¯ s b¯∈B s/bracketleftbigg /parenleftbigg b¯∈B/parenrightbigg/bracketrightbigg where the last inequality follows from Jensen’s inequality. Now we bound the max by a sum to get/bracketleftbigg /bracketrightbigg1/summationdisplay log|¯B|s b2 IE max|Zb| ≤log IE[exp( sZb)]≤ +| |2, b∈B s s2n b∈B¯ where in the last inequality, we used (4.2). Optimizing over s >0 yields the desired result. 3We apply this result to our problem by observing that Rn(A) = sup ( ,Rn(T z)) z1,... zn whereT(z) is deﬁned in (4.1). In",
    "particular, since T(z)⊂ {0,1}, we have |b√|2≤n for allb∈T(z). Moreover, by deﬁnition of the shatter coeﬃcients, if B=T(z), then |¯B| ≤2|T(z)| ≤2SA(n). Together with the above lemma, it yields the desired inequality: /radicalbigg 2log(2SA(n))Rn(A)≤ .n Step 3: Sauer-Shelah Lemma We need to use a lemma from combinatorics to relate the shatter coeﬃcients to the VC dimension. A priori, it is not clear from its deﬁnition that the VC dimension may be at all useful to get better bounds. Recall that steps 1 and 2 put together yield the following bound 2log(2S(n))IE[sup n(A) A−µ(A) ] A∈|µ | ≤A2/radicalbigg (4.3)n In particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does not imply that the uniform deviations go to zero as the sample size ngoes to inﬁnity. The VC inequality suggest that this is not the case as soon as VC(A)<∞but it is not clear a priori. Indeed, it may be the case that VCSA(n) = 2nforn≤dandSA(n) = 2n−1 forn > d, which would imply that ( A) =d",
    "<∞but that the right-hand side in (4.3) is larger than 2 for alln. It turns our that this can never be the case: if the VC dimension is ﬁnite, then the shatter coeﬃcients are at most polynomial inn. This result is captured by the Sauer- Shelah lemma, whose proof is omitted. The reading section of the course contains pointers to various proofs, speciﬁcally the one based on shiftingwhich is an important technique in enumerative combinatorics. Lemma (Sauer-Shelah): IfVC(A) =d, then∀n≥1, d SA(n)≤/summationdisplay/parenleftbiggn en d .k/parenrightbigg ≤dk=0/parenleftig /parenrightig Together with (4.3), it clearly yields the VC inequality. By applying the bounded diﬀerence inequality, we also obtain the following VC inequality that holds with high probability. This is often the preferred from for this inequality in the literature. Corollary (VC inequality): For any family of sets Asuch that VC(A) =dand any δ∈(0,1), it holds with probability at least 1 −δ, /radicalbigg",
    "2dlog(2en/d)/radicalbigg log(2/δ)supµnA)−µ(A)| ≤2 + A∈A|( .n 2n 4Note that the logarithmic term log(2 en/d) is actually superﬂuous and can be replaced by a numerical constant using a more careful bounding technique. This is beyond the scope of this class and the interested reader should take a look at the recommending readings. 4.3 Application to ERM The VC inequality provides an upper bound for supA∈A|µn(A)−µ(A)|in terms of the VC dimension of the class of sets A. This result translates directly to our quantity of interest: 2VC( )log2en)ˆsup|Rnh)−VC(A)log(2/δ(R(h) 2n/parenrightbig + h∈H≤/radicaligg A |/parenleftbig /radicalbigg (4.4)2n whereA={Ah:h∈ H}andAh={(x,y)∈ X ×{0,1}:h(x) =y}. Unfortunately, the VC dimension of this class of subsets of X ×{0,1}is not very natural. Since, a classiﬁer h is a{0,1}valued function, it is more natural to consider the VC dimension of the family A=/braceleftbig {h= 1}:h∈ H/bracerightbig . Deﬁnition: LetHbe a collection of classiﬁers and deﬁne A¯={h=",
    "1}:h∈ H We deﬁne the VC d/braceleftbig imension VC( ) o/bracerightbig =/braceleftbig A:∃h∈ H,h(·) = 1I(· ∈A). H ¯ fHto be the VC dimension of/bracerightbig A. ¯ ¯ It is not clear how VC(A) relates to the quantity VC(A), where A={Ah:h∈ H}and Ah={(x,y)∈ X ×{0,1}:h(x) =y}that appears in the VC inequality. Fortunately, these two are actually equal as indicated in the following lemma. Lemma: Deﬁne the two families for sets: = AX×h:h 2{0,1}where { ¯A { ∈ H} ∈ Ah= (x,y)∈ X ×{0,1}:h(x) =y}andA=/braceleftbig {h= 1 :h 2X. S S ≥ VCA¯} ∈ H ∈ Then,A¯(n) =A¯(n) for alln1. It implies ( ) = VC(A/bracerightbig ). Proof.Fixx= (x1,...,xn)∈ Xnandy= (y1,y2,...,yn)∈ {0,1}nand deﬁne T(x,y) ={(1I(h(x1) =y1),...,1I(h(xn) =yn)),h∈ H} and ¯T(x) ={(1I(h(x1) = 1),...,1I(h(xn) = 1)),h∈ H} To that end, ﬁx v∈ {0,1}and recall the XOR (exclusive OR) boolean function from {0,1} to{0,1}deﬁned by u⊕v= 1I(u=v). It is clearly1a bijection since ( u⊕v)⊕v=u. 1One way to see that is to introduce the “spinned” variables u˜ =",
    "2u−1 andv˜ = 2v−1 that live in /tildewider {−1,1}. Thenu⊕v=u˜·v˜, and the claim follows by observing that ( u˜·v˜)·v˜ =u˜. Another way is to simply write a truth table. 5/ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslash /ne}ationslashWhen applying XOR componentwise, we have  1I(h(x1) =y1) 1I(h(x1= 1) y  .. ) 1 ..  . 1I(h(xi) .  =yi) = 1I(h(xi) = 1) .. .⊕ .  . . 1I(h(xn) =yn)     1I(h(xn) = 1)... yi  ... yn  ¯ Since XOR is a bijection, we must have card[ T(x,y)] = card[ T(x)]. The lemma follows by taking the supremum on each side of the equality. It yields the following corollary to the VC inequality. Corollary: LetHbe a family of classiﬁers with VC dimension d. Then the empirical ˆ risk classiﬁer hermoverHsatisﬁes erm/radicalbigg 2dlog(2en/d)ˆR(h)≤minR(h)+4 + h∈H n/radicalbigg log(2/δ) 2n with probability 1 −δ. Proof.Recall from Lecture 3 that ˆR(herm)−min ) ≤ ˆ R(h2sup h∈H h∈H The proof follows directly by",
    "applyi/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle ng (4.4) and the above lemma./vextendsingle 6/ne}ationslash /ne}ationslash /ne}ationslashMIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_6.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 6 Scribe:Ali Makhdoumi Sep. 28, 2015 5. LEARNING WITH A GENERAL LOSS FUNCTION In the previous lectures we have focused on binary losses for the classiﬁcation problem and developed VCtheory forit. Inparticular, theriskforaclassiﬁcation function h:X → {0,1} and binary loss function the risk was R(h) = IP(h(X) =Y) = IE[1I( h(X) =Y)]. In this lecture we will consider a general loss function and a general regression model where Yis not necessarily a binary variable. For the binary classiﬁcation problem, we then used the followings: •Hoeﬀding’s inequality: it requires boundedness of the loss functions. •Bounded diﬀerence inequality: again it requires boundedness of the loss functions. •VC theory: it requires binary nature of the loss function. Limitations of the VC theory: •Hard to ﬁnd the optimal classiﬁcation: the empirical risk minimization optimization, i.e., n1min hn/summationdisplay 1I(h(Xi) =Yi) i=1 is a",
    "diﬃcult optimization. Even though it is a hard optimization, there are some algorithms that try to optimize this function such as Perceptron and Adaboost. •This is not suited for regression. We indeed know that classiﬁcation problem is a subset of Regression problem as in regression the goal is to ﬁnd IE[ Y|X] for a general Y(not necessarily binary). In this section, we assume that Y∈[−1,1] (this is not a limiting assumption as all the results can bederived for any bounded Y) and we have a regression problem where ( X,Y)∈ X ×[−1,1]. Most of the results that we preset here are the analogous to the results we had in binary classiﬁcation. This would be a good place to review those materials and we will refer to the techniques we have used in classiﬁcation when needed. 5.1 Empirical Risk Minimization 5.1.1 Notations Loss function: In binary classiﬁcation the loss function was 1I( h(X) =Y). Here, we replace this loss function by ℓ(Y,f(X)) which we assume is symmetric, where f∈ F, f:X",
    "→[−1,1] is the regression functions. Examples of loss function include 1/\\e}atio\\slash /\\e}atio\\slash /\\e}atio\\slash /\\e}atio\\slash•ℓ(a,b) = 1I( a=b) ( this is the classiﬁcation loss function). •ℓ(a,b) =|a−b|. •ℓ(a,b) = (a−b)2. •ℓ(a,b) =|a−b|p,p≥1. We further assume that 0 ≤ℓ(a,b)≤1. Risk: risk is the expectation of the loss function, i.e., R(f) = IEX,Y[ℓ(Y,f(X))], where the joint distribution is typically unknown and it must be learned from data. Data: we observe a sequence ( X1,Y1),...,(Xn,Yn) ofnindependent draws from a joint distribution PX,Y, where ( X,Y)∈ X ×[−1,1]. We denote the data points by Dn= {(X1,Y1),...,(Xn,Yn)}. Empirical Risk : the empirical risk is deﬁned as n1ˆRn(f) =n/summationdisplay ℓ(Yi,f(Xi)), i=1 ˆ ˆ and the empirical risk minimizer denoted by ferm(orf) is deﬁned as the minimizer of empirical risk, i.e., ˆargminRn(f). f∈F ˆ In order to control the risk of fwe shall compare its performance with the following oracle: ¯f∈argminR(f). f∈F Note that this is an oracle",
    "as in order to ﬁnd it one need to have access to PXYand then ˆ optimize R(f) (we only observe the data Dn). Since fis the minimizer of the empirical ˆˆˆ¯ risk minimizer, we have that Rn(f)≤Rn(f), which leads to ˆR(f)≤ˆR(f)−ˆˆˆˆˆ¯ˆ¯ ¯ ¯ Rn(f)+Rn(f)−Rn(f)+Rn(f)−R(f)+R(f) ≤¯ ˆ ˆˆˆ¯ ¯ ¯ ˆ R(f)+R(f)−Rn(f)+Rn(f)−R(f)≤R(f)+2sup f∈F|Rn(f)−R(f)|. Therefore, the quantity of interest that we need to bound is sup|ˆRn(f)−R(f) f∈F|. Moreover, from theboundeddiﬀerence inequality, we know that sincethe loss function ℓ(·,·) ˆ is bounded by 1, supf∈F|Rn(f)−R(f)|has the bounded diﬀerence property with ci=1 n fori= 1,...,n, and the bounded diﬀerence inequality establishes P/bracketleftigg 2t2 sup|ˆ ˆ Rn(f)−R(f)|−IE/bracketleftigg sup|Rn(f)−R(f) f∈F|/bracketrightigg ≥t f∈F/bracketrightigg −≤exp/parenleftbigg x2 i i/parenrightbigg = e pc−2nt2, which in turn yields/parenleftbig /parenrightbig/summationtext log(1/delta)|ˆsupRn(f)−R(f)| ≤I|ˆE/bracketleftigg supRn(f)−R(f) δ f∈F f|/bracketrightigg +",
    "∈/radicalbigg ,w.p. 1 F 2n−. ˆ As a result we only need to bound the expectation IE[supf∈F|Rn(f)−R(f)|]. 2/\\e}atio\\slash5.1.2 Symmetrization and Rademacher Complexity Similar to the binary loss case we ﬁrst use symmetrization technique and then intro- duce Rademacher random variables. Let Dn={(X1,Y1),...(Xn,Yn)}be the sample set and deﬁne an independent sample (ghost sample) with the same distribution denoted by D′ n={(X′ 1,Y′ 1),...(X′ n,Y′ n)}( for each i, (X′ i,Y′ i) is independent from Dnwith the same distribution as of ( Xi,Yi)). Also, let σi∈ {−1,+1}be i.i.d. Rad(1) random variables2 independent of DnandD′ n. We have IE/bracketleftiggn1sup|ℓ i f∈Fn/summationdisplay (Yi,f(X)) i=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg n n1 1= IE/bracketleftigg sup ℓ(Yi,f(X ℓ(Y′i)) IEi,f(X′ i))Dn f∈F|n/summationdisplay i=1−/bracketleftigg n/summationdisplay i=1|/bracketrightigg |/bracketrightigg n n1 1= IE/bracketleftigg sup|IE/bracketleftigg/summationdisplay ℓ(Yi,f(X′i)) ℓ(Yi,f(X′ i))Dn",
    "f∈Fni=1−n/summationdisplay i=1|/bracketrightigg |/bracketrightigg n(a) ≤IE/bracketleftiggn1 1supIE/bracketleftigg |/summationdisplay ℓ(Yi,f(X′i))−/summationdisplay ℓ(Y ,f(X′ i i))| |Dn f∈Fn ni=1 i=1/bracketrightigg/bracketrightigg ≤IE/bracketleftiggn n1sup|/summationdisplay 1ℓ(Yi,f(Xi)) ℓ(Y′ i,f(X′ f∈Fn ni)) i=1−/summationdisplay i=1|/bracketrightigg (b) 1= IE/bracketleftiggn sup|/summationdisplay σi/parenleftbig ℓ(Yi,f(Xi))−ℓ(Y′X fFni,f(′ i)) ∈i=1/parenrightbig |/bracketrightigg n(c) 1≤2IE/bracketleftigg sup f∈F|n/summationdisplay σiℓ(Yi,f(Xi)) i=1|/bracketrightigg n ≤2supIE/bracketleftigg 1sup|/summationdisplay σiℓ(yi,f(xi)) Dnf∈Fni=1|/bracketrightigg . where (a) follows from Jensen’s inequality with convex function f(x) =x, (b) follows from the fact that ( X ,Y) and (X′ ′| | i i i,Yi) has the same distributions, and (c) follows from triangle inequality. Rademacher complexity: of a class Fof functions for a given loss function ℓ(·,·) and samplesDnis deﬁned as n1Rn(ℓ◦F)",
    "= supIE/bracketleftigg sup|/summationdisplay σiℓ(yi,f(xi)). Dnf∈Fni=1|/bracketrightigg Therefore, we have IE/bracketleftiggn1sup|/summationdisplay ℓ(Yi,f(Xi)) f∈Fni=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg ≤2Rn(ℓ◦F) and we only require to bound the Rademacher complexity. 5.1.3 Finite Class of functions Suppose that the class of functions Fis ﬁnite. We have the following bound. 3Theorem: Assume that Fis ﬁnite and that ℓtakes values in [0 ,1]. We have /radicalbigg 2log(2Rn(ℓ◦F)|F|)≤ .n Proof.From the previous lecture, for B⊆nR, we have that n1 2log(2B)Rn(B) = IE/bracketleftigg max b∈B|n/summationdisplay σibi i=1|/bracketrightigg | |≤max b∈B|b|2/radicalbig .n Here, we have ℓ(y(x 1,f1)) .B= .,. ℓ(yn,f(xn)f∈ F  . ) Sinceℓtakes values in [0 ,1], this im pliesB ⊆ {b:|b|2√≤ n}. Plugging this bound in the previous inequality completes the proof. 5.2 The General Case Recall that for the classiﬁcation problem, we had F ⊂ {0,1}X. We have seen that the cardinality of the set",
    "{(f(x1),...,f(xn)),f ˆerm∈ F}plays an important role in bounding the risk off(this is not exactly what we used but the XOR argument of the previous lecture allows us to show that the cardinality of this set is the same as the cardinality of the set that interests us). In this lecture, this set might be uncountable. Therefore, we need to introduce a metric on this set so that we can treat the close points in the same manner. To this end we will deﬁne covering numbers (which basically plays the role of VC dimension in the classiﬁcation). 5.2.1 Covering Numbers Deﬁnition: Given a set of functions Fand a pseudo metric donF((F,d) is a metric space) and ε >0. Anε-netof (F,d) is a set Vsuch that for any f∈ F, there exists g∈Vsuch that d(f,g)≤ε. Moreover, the covering numbers of (F,d) are deﬁned by N(F,d,ε) = inf{|V|:Vis anε-net}. For instance, for the Fshown in the Figure 5.2.1 the set of points {1,2,3,4,5,6}is a covering. However, the covering number is 5 as point 6 can be removed from Vand",
    "the resulting points are still a covering. Deﬁnition: Givenx= (x1,...,x n), theconditional Rademacher average of a class of 4functions Fis deﬁned as Rˆx n= IE/bracketleftiggn1sup σ f∈F/vextendsingle/vextendsingle n/summationdisplay if(xi) i=1/bracketrightigg /vextendsingle/vextendsingle. Note that in what follows we consider a general class of functions F. However, for applying the results in order to bound empirical risk minimization, we take xito be (xi,yi) andFto beℓ◦F. We deﬁne the empirical l1distance as n dx 1 1(f,g) =n/summationdisplay i =1|f(x) i−g(xi)|. Theorem: If 0≤f≤1 for all f∈ F, then for any x= (x1,...,x n), we have ˆRx n(F)≤inf ε≥0/radicalbigg x /braceleftbig2log(2N(F,dε+1,ε)) n/bracerightbig . Proof.Fixx= (x1,...,x n) andε >0. LetVbe a minimal ε-net of ( F,dx 1). Thus, by deﬁnition we have that |V|=N(F,dx 1,ε). For any f∈ F, deﬁnef◦∈Vsuch that 56 5 432 1 F ǫdx 1(f,f◦)≤ε. We have that n1Rx n(F) = IE/bracketleftigg sup σif(xi) f∈F|n/summationdisplay",
    "i=1|/bracketrightigg ≤IE/bracketleftiggn n1 1sup|/summationdisplay σi(f(xi)f◦(xi)) +IE sup σif◦(xi) f∈Fn f∈Fni=1− |/bracketrightigg /bracketleftigg |/summationdisplay i=1|/bracketrightigg ≤ε+IE/bracketleftiggn1max σif(xi) f∈V|n/summationdisplay i=1|/bracketrightigg /radicalbigg 2log(2≤ε+|V|) n/radicalbigg 2log(2N(=ε+F,dx 1,ε)).n Since the previous bound holds for any ε, we can take the inﬁmum over all ε≥0 to obtain x/radicalbigg /braceleftbig2log(2N(F,dx Rn(F)≤infε+1,ε)) ε≥0 n/bracerightbig . The previous bound clearly establishes a trade-oﬀ because as εdecreases N(F,dx 1,ε) in- creases. 5.2.2 Computing Covering Numbers As a warm-up, we will compute the covering number of the ℓ2ball of radius 1 indRdenoted byB2. We will show that the covering is at most (3 ε)d. There are several techniques to prove this result: one is based on a probabilistic method argument and one is based on greedily ﬁnding an ε-net. We will describe the later approach here. We select points in V one after",
    "another so that at step k, we have uk∈B2\\∪k j=1B(uj,ε). We will continue this procedure until we run out of points. Let it be step N. This means that V={u1,...,u N} is anε-net. We claim that the balls B(ui,ε) andB(uj,ε) for any i,j12 2∈ {,...,N}are disjoint. The reason is that if v∈B(ui,ε)∩B(uj,ε), then we would have2 2 ε ε/bardblui−uj/bardbl2≤ /bardblui−v/bardbl2+/bardblv−uj/bardbl2≤+ =ε,2 2 which contradicts the way we have chosen the points. On the other hand, we have that ∪N j=1B(uj,ε)⊆(1+ε)B2. Comparing the volume of these two sets leads to2 2 ε ε|V|( )dvol(B2)≤(1+ )dvol(B2),2 2 wherevol(B2) denotes the volume of the unit Euclidean ball in ddimensions. It yields, |V| ≤/parenleftbig 1+εd 22d3d = +1 . /parenleftbigε ε 2/parenrightbig/parenrightbigg /parenrightbigd/parenleftbigg ≤/parenleftbigg ε/parenrightbigg 6For anyp≥1, deﬁne 1 dx p(f,g) =/parenleftiggn1p /summationdisplay |f(xi)g(x)pi,ni=1− |/parenrightigg and forp=∞, deﬁne dx ∞(f,g) = max |f(xi)−g(xi) i|. ˆ Using the",
    "previous theorem, in order to bound Rx nwe need to bound the covering number withdx 1norm. We claim that it is suﬃcient to bound the covering number for the inﬁnity- norm. In order to show this, we will compare the covering number of the norms dx p(f,g) = 1 /parenleftbig1 n/summationtextn i=1|f(xpi)−g(xi)|/parenrightbig pforp≥1 and conclude that a bound on N(F,dx ∞,ε) implies a bound on N(F,dx p,ε) for any p≥1. Proposition: For any 1 ≤p≤qandε >0, we have that N(F,dx p,ε)≤N(F,dx q,ε). Proof.First note that if q=∞, then the inequality evidently holds. Because, we have n1(/summationdisplay 1 |zi|p)p≤maxn ii=1|zi|, which leads to B(f,dx ∞,ε)⊆B(f,dx p,ε) andN(f,d∞,ε)≥N(f,dp,ε). Now suppose that 1≤p≤q <∞. Using H¨ older’s inequality with r=q p≥1 we obtain /parenleftigg /parenrightigg 1/parenleftigg /parenrightigg(11)1/parenleftigg /parenrightigg 1/parenleftigg /parenrightigg 1− n n n1p r p pr n1q 1n/summationdisplay 1 |z|pi≤−np/summationdisplay i1/summationdisplay i=1|zi|pr=ni=1",
    "=/summationdisplay . i|zi|q =1 This inequality yeilds B(f,dx q,ε) ={g:dx q(f,g)≤ε} ⊆B(f,dx p,ε), which leads to N(f,dq,ε)≥N(f,dp,ε). Using this propositions we only need to bound N(F,dx ∞,ε). Let the function class be F={f(x) =/a\\}bracketle{tf,x/a\\}bracketri}ht,f∈Bd,x∈Bd}, where1 1 p q + = 1. Thisp q leads to|f| ≤1. Claim: N(F,dx ∞,ε)≤(2)d.ε This leads to x/radicalbigg 2dlog(4/ε)ˆRn(F)≤inf 0{ε+ . ε> n} Takingε=O(/radicalig dlogn), we obtainn ˆRx d n(F)≤O(/radicalbigg logn).n 7MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_7.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 7 Scribe: Zach Izzo Sep. 30, 2015 In this lecture, we continue our discussion of covering numbers and compute upper ^ bounds for speci c conditional Rademacher averages Rx n(F). We then discuss chaining and conclude by applying it to learning. Recall the following de nitions. We de ne the risk function R(f) = I E[`(X;f (X))]; (X;Y )2X\u0002 [\u00001;1]; for some loss function `(\u0001;\u0001). The conditiona Rademacher average that we need to control is n1R(`l\u000eF) = sup I E sup \u001bi`(yi;f(xi)): (x1;y1);:::;(x n;yn)\" f n2FX i=1 # Furthermore, we de ned the conditional Rademacher average for a poin tx= (x1;:::;x n) to be R^x n(F) = I E\" sup f2F n1\u001bif(xi)ni=1 # : Lastly, we de ne the \"-covering number N(X F;d; \") to be the m inimum number of balls (with respect to the metric d) of radius\"needed to cover F. We proved the following theorem: Theorem: Assumejfj\u00141 for allf2F. Then 2 log(2N(;dx;\"))R^x n(F) +r F\u0014inf1 \">0( \"n) ; wheredx 1is",
    "given by n dx 1 1(f;g) =nX i=1jf(xi)\u0000g(xi)j: We make use of this theorem in the following example. De ne Bd p=fx2I Rd:jxjp\u00141g. Then takef(x) =ha;xi, setF=fha;\u0001i:a2Bdg, andX=Bd 1. By H older's inequality,1 we have jf(x)j\u0014jajx1j j1\u00141; so the theorem above holds. We need to compute the covering number N(F;dx 1;\"). Note that for all a2Bd, there exists v= (v1;:::;v n) such that vi=g(xi) and1 n1 nX a;xivi\" i=1jh i\u0000 j\u0014 for some function g. For this case, we will take g(x) =hb;xi, sovi=hb;x ii. Now, note the following. Given this de nition of g, we have n n dx 1 1 1(f;g) = a;x 1b;xi=a b;x ia bnX ni=1jh i\u0000h ijX i=1jh \u0000 ij\u0014j \u0000 j 1 1by H older's inequality and the fact that jxj1= 1. So ifja\u0000bj1\u0014\", we can take vi=hb;x ii. We just need to nd a set of fb1;:::;b Mg\u001aI Rdsuch that, for any athere exists bjsuch thatja\u0000bjj<1. We can do this by dividing Bdinto cubes with side length \"and 1 1 taking theb's to be the set of vertices of these cubes. Then any a2Bdj must land in one1 of these cubes, so",
    "ja\u0000bjj \u0014\"as desired. There are c=\"dof suchb 1 j's for some constant c>0. Thus N(Bd;dx 1 1;\")\u0014c=\"d: We now plug this value into the theorem to obtain R^x 2 log(c=\"d) n(F)\u0014inf : \"( \"+ \u00150r n) Optimizing over all choices of \"gives r dlog(n)\"\u0003=cn) R ^x n(F)\u0014cr dlog(n):n Note that in this nal inequality, the conditional empirical risk no longer depends on x, since we \\sup'd\" xout of the bound during our computations. In general, one should ignorexunless it has properties which will guarantee a bound which is better than the sup. Another important thing to note is that we are only considering one granularity of Fin our nal result, namely the one associated to \"\u0003. It is for this reason that we pick up an extra log factor in our risk bound. In order to remove this term, we will need to use a technique called chaining. 5.4 Chaining We have the following theorem. Theorem: Assume thatjfj\u00141 for allf2F. Then \u001a12Z1 R^x n\u0014inf 4\" + \">0p log(N (;dx))dt :n2;t \"q F\u001b (Note that the integrand decays with",
    "t.) Proof. Fixx= (x1;:::;x n), and for all j= 1;:::;N , letVjbe a minimal 2\u0000j-net ofF under thedx 2metric. (The number Nwill be determined later.) For a xed f2F, this process will give us a \\chain\" of points fi\u000ewhich converges to f:dx 2(fi\u000e;f)\u00142\u0000j. De neF=f(f(x1);:::;f (xn))>; f2Fg\u001a [\u00001;1]n. Note that R^x 1 n(F) = I E supn f2Fh\u001b;fi where\u001b= (\u001b1;:::;\u001b n). Observe that for all N, we can rewriteh\u001b;fias a telescoping sum: h\u001b;fi=h\u001b;f\u0000fN\u000ei+h\u001b;fN\u000e\u0000fN\u000e \u00001i+:::+h\u001b;f1\u000e\u0000f0\u000ei 2wheref0\u000e:= 0. Thus N R^x 1 1 n(F)\u0014I E supjh\u001b;f\u0000fN\u000eij+ I fn f2FX E sup\u001b;nj\u000efj\u000e f F\u00001: j=1jh \u0000 ij 2 We can control the two terms in this inequality separately. Note rst that by the Cauchy- Schwarz inequality, 1 dx 2(f;fN\u000e)I E supjh\u001b;f\u0000fN\u000eij\u0014j\u001bj2n fp:n 2F Sincej\u001bj2=pnanddx 2(f;fN\u000e)\u00142\u0000N, we have 1I E supjh\u001b;f\u0000fN\u000e2n f2Fij\u0014\u0000N: Now we turn our attention to the second term in the inequality, that is N S=X1I E supjh\u001b;fj\u000e\u0000fj\u000e n fj=12F\u00001ij: Note that since fj\u000e2Vjandfj\u000e \u000012Vj V \u00001, there are at most jjjjVj\u00001jpossible di erences",
    "fj\u000e\u0000fj\u000e.\u00001SincejV2j1j\u0014jV\u0000 jj=2,jVjjjVj1j\u0014jVjj=2 and we nd ourselves in the nite\u0000 dictionary case. We employ a risk bound from earlier in the course to obtain the inequality p 2 log(2Rn(B)\u0014max b Bjbj2jBj): 2 n In the present case, B=ffj\u000e\u0000fj\u000e \u00001;f2Fgso thatjBj\u0014jV jj2=2. It yields 2j22 log(jVj) logR2 Vj n(B)j\u0001q j\u0014r = 2rn\u0001p ;n wherer= supf2Fjfj\u000e\u0000fj\u000e \u00001j2. Next, observe that jfj\u000e\u0000fj\u000e 1j2=pn d\u0000\u0001x 2(fj\u000e;fj\u000e \u00001)p p\u0014n(dx 2(fj\u000e;f) +dx 2(f;fj\u000e)) 3 2\u0000jn:\u00001\u0014 \u0001 by the triangle inequality and the fact that dx(f\u000e;f)\u00142\u0000j 2j . Substituting this back into our bound forRn(B), we have log(B)jVj6 2\u0000jnr jj ;2j ( ))\u0014 \u0001 = 6n\u0001\u0000r log(NFdx 2;2\u0000j Rn sinceVjjwas chosen to be a minimal 2\u0000-net. The proof is almost complete. Note that 2\u0000j= 2(2\u0000j\u00002\u0000j\u00001) so that N6pXN122\u0000jq log(N (F;dx 2;2\u0000j)) =pX (2\u0000j\u00002\u0000j\u00001)q log(N (F;dx 2;2\u0000j)):n nj=1 j=1 Next, by comparing sums and integrals (Figure 1), we see that XN (2\u0000j j=1\u00002\u0000j\u00001)q 1 log(N (F;dx 2;2\u0000j))\u0014Z=2 log(N (;dx 2;t))dt: 2\u0000(N+1)q F 3Figure 1: A comparison of the sum and",
    "integral in question. So we choose Nsuch that 2\u0000(N +2)\u0014\"\u00142\u0000(N +1), and by combining our bounds we obtain 121=2 1 R^x)\u00142\u0000N n(F +pnZq log(N (F;dx 2;t))dt 2\u0000( +1)\u00144\"+ NZp log(N; \"F;t)dt since the integrand is non-negative. (Note: this integral is known as the \\Dudley Entropy Integral.\") Returning to our earlier example, since N(F;dx 2;\")\u0014c=\"d, we have 1 R^x n(F)\u0014inf\u001a124\"+pZq log((c0=t)d)dt \">0 n\"\u001b : SinceR1p log(c=t)dt =c\u0016 is nite, we then have0 R^x n(F)\u001412c\u0016p d=n: Using chaining, we've been able to remove the log factor! 5.5 Back to Learning We want to bound n1Rn(`\u000eF) = sup I E sup \u001bi`(yi;f(xi)): (x1;y1);:::;(x n;yn)\" f2F nX i x =1 # R^ We considern(\b\u000eFn) = I E i\b \u0002 supf 1P i=1\u001b\u000ef(x)2F ifor someLn -Lipschitz function \b, that isj\b(a)\u0000\b(b)j\u0014Lja\u0000bjfor alla;b2[\u00001;1]. We \u0003 have the following lemma. 4Theorem: (Contraction Inequality) Let \b be L-Lipschitz and such that \b(0) = 0, then R^x n(\b\u000eF)\u0014 R ^ 2L\u0001x n(F): The proof is omitted and the interested reader should take a look at [LT91, Kol11]",
    "for example. As a nal remark, note that requiring the loss function to be Lipschitz prohibits the use ofR-valued loss functions, for example `(Y;\u0001) = (Y\u0000\u0001)2: References [Kol11] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse \u0013 \u0013 recovery problems. Ecole d'Et\u0013 e de Probabilit\u0013 es de Saint-Flour XXXVIII-2008. Lec- ture Notes in Mathematics 2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011. [LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)] . Springer-Verlag, Berlin, 1991. Isoperimetry and processes. 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ],
  "ml_math_lect_8.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 8 Scribe:Quan Li Oct. 5, 2015 Part II Convexity 1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION ˆ In the previous lectures, we have proved upper bounds on the excess risk R(herm)−R(h∗) of the Empirical Risk Minimizer ˆherm 1= argmin h∈Hn 1I(Yi=h(Xi)). (1.1)n/summationdisplay i=1/ne}ationslash However due to the nonconvexity of the objective function, the optimization problem (1.1) in general can not be solved eﬃciently. For some choices of Hand the classiﬁcation error function (e.g. 1I( ·)), the optimization problem can be NP-hard. However, the problem we deal with has some special features: 1. Since the upper bound we obtained on the excess risk is O(/radicalBig dlogn), we only need ton approximate the optimization problem with error up to O(/radicalBig dlogn n). 2. The optimization problem corresponds to the average case problem where the data i.i.d(Xi,Yi)∼PX,Y. 3.Hcan be chosen to be some ’natural’",
    "classiﬁers, e.g. H={half spaces }. These special features might help us bypass the computational issue. Computational issueinmachinelearninghavebeenstudiedforquitesometime(see, e.g. [Kea90]), especially in the context of PAC learning. However, many of these problems are somewhat abstract and do not shed much light on the practical performance of machine learning algorithms. Toavoid thecomputational problem, thebasicideais to minimizea convex upperbound of the classiﬁcation error function 1I( ·) in (1.1). For the purpose of computation, we shall also require that the function class Hbe a convex set. Hence the resulting minimization becomes a convex optimization problem which can be solved eﬃciently. 1.1 Convexity Deﬁnition: A setCis convex if for all x,y∈Candλ∈[0,1],λx+(1−λ)y∈C. 1Deﬁnition: A function f:D→IR on a convex domain Dis convex if it satisﬁes f(λx+(1−λ)y)≤λf(x)+(1−λ)f(y),∀x,y∈D,andλ∈[0,1]. 1.2 Convex relaxation The convex relaxation takes three steps. Step 1: Spinning. Using",
    "a mapping Y/ma√sto→2Y−1, the i.i.d. data ( X1,Y1),(X2,Y2),...,(Xn,Yn) is transformed to lie inX ×{−1,1}. These new labels are called spinnedlabels. Correspondingly, the task becomes to ﬁnd a classiﬁer h:X /ma√sto→ {−1,1}. By the relation h(X)/ne}ationslash=Y⇔−h(X)Y >0, we can rewrite the objective function in (1.1) by n n1/summationdisplay 11I(h(Xi) =Yi) =/summationdisplay ϕ1 I(hn ni=1 i=1−(Xi)Yi) (1.2) whereϕ1 I(z) = 1I(z >0). Step 2: Soft classiﬁers. The setHof classiﬁers in (1.1) contains only functions taking values in {−1,1}. As a result, it is non convex if it contains at least two distinct classiﬁers. Soft classiﬁers provide a way to remedy this nuisance. Deﬁnition: Asoft classiﬁer is any measurable function f:X →[−1,1]. The hard classiﬁer (or simply “classiﬁer”) associated to a soft classiﬁer fis given by h= sign(f). LetF ⊂IRXbe aconvexset soft classiﬁers. Several popular choices for Fare: •Linear functions: F:={/an}bracketle{ta,x/an}bracketri}ht:a∈ A}. for some convex set A",
    "∈IRd. The associated hard classiﬁer h= sign(f) splits IRdinto two half spaces. •Majority votes: given weak classiﬁers h1,...,h M, M M F:=/braceleftBig/summationdisplay λjhj(x) :λj j=≥0,/summationdisplay λj= 1/bracerightBig . 1 j=1 •Letϕj,j= 1,2,...a family of functions, e.g., Fourier basis or Wavelet basis. Deﬁne ∞ F:={/summationdisplay θjϕj(x) : (θ1,θ2,...) j=1∈Θ}, where Θ is some convex set. 2/ne}ationslashStep 3: Convex surrogate. Given a convex set Fof soft classiﬁers, using the rewriting in (1.2), we need to solve that minimizes the empirical classiﬁcation error 1min f∈Fn ϕ1 I(f(Xi)Yi),n/summationdisplay i=1− However, while we are now working with a convex constraint, our objective is still not convex: we need a surrogate for the classiﬁcation error. Deﬁnition: A function ϕ: IR/ma√sto→IR+is called a convex surrogate if it is a convex non-decreasing function such that ϕ(0) = 1 and ϕ(z)≥ϕ1 I(z) for allz∈IR. The following is a list of convex surrogates of loss functions. •Hinge",
    "loss: ϕ(z) = max(1+ z,0). •Exponential loss: ϕ(z) = exp(z). •Logistic loss: ϕ(z) = log2(1+exp( z)). To bypass the nonconvexity of ϕ1 I(·), we may use a convex surrogate ϕ(·) in place of ˆ ϕ1 I(·) and consider the minimizing the empirical ϕ-riskRn,ϕdeﬁned by 1ˆRn,ϕ(f) =nn/summationdisplay i=1ϕ(−Yif(Xi)) I t is the empirical counterpart of the ϕ-riskRϕdeﬁned by Rϕ(f) = IE[ϕ(−Yf(X))]. 1.3ϕ-risk minimization In this section, we will derive the relation between the ϕ-riskRϕ(f) of a soft classiﬁer fand the classiﬁcation error R(h) = IP(h(X) =Y) of its associated hard classiﬁer h= sign(f) Let f∗ ϕ= argmin E[ϕ(Y f∈IRX−f(X))] where the inﬁmum is taken over all measurable functions f:X →IR. To verify that minimizing the ϕserves our purpose, we will ﬁrst show that if the convex surrogate ϕ(·) is diﬀerentiable, then sign( f∗ ϕ(X))≥0 is equivalent to η(X)≥1/2 where η(X) = IP(Y= 1|X). Conditional on {X=x}, we have IE[ϕ(−Yf(X))|X=x] =η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x)). Let Hη(α) =η(x)ϕ(−α)+(1−η(x))ϕ(α)",
    "(1.3)/ne}ationslash 3so that f∗ ϕ(x) = argmin H∗η(α),andRϕ= minRϕ(f) = minHη) α f∈IRX(x)(α . α∈IR ∈IR Sinceϕ(·) is diﬀerentiable, setting thederivative of H∗η(α) to zero gives fϕ(x) =α¯, where H′ η(α¯) =−η(x)ϕ′(−α¯)+(1−η(x))ϕ′(α¯) = 0, which gives η(x)ϕ′(α¯)=1−η(x)ϕ′(−α¯) Sinceϕ(·)isaconvex function, itsderivative ϕ′(·) isnon-decreasing. Thenfromtheequation above, we have the following equivalence relation 1η(x)≥ ⇔α¯≥0⇔sign(f∗ 2ϕ(x))≥0. (1.4) Since the equivalence relation holds for all x∈ X, 1η(X)≥ ⇔sign(f∗ ϕ(X))2≥0. The following lemma shows that if the excessϕ-riskR(f)−R∗ϕ ϕof a soft classiﬁer fis small, then the excess-risk of its associated hard classiﬁer sign( f) is also small. Lemma (Zhang’s Lemma [Zha04]): Letϕ: IR/ma√sto→IR+be a convex non-decreasing function such that ϕ(0) = 1. Deﬁne for any η∈[0,1], τ(η) := inf Hη(α). α∈IR If there exists c >0 andγ∈[0,1] such that 1|η−c2| ≤(1−τ(η))γ,∀η∈[0,1], (1.5) then R(sign(f))−R∗≤2c(Rϕ(f)−R∗ ϕ)γ Proof.Note ﬁrst that τ(η)≤Hη(0) =ϕ(0) = 1",
    "so that condition (2.5) is well deﬁned. Next, let h∗= argminh∈{−1,1}XIP[h(X) =Y] = sign(η−1/2) denote the Bayes classiﬁer, whereη= IP[Y= 1|X=x], . Then it is easy to verify that R(sign(f))−R∗= IE[|2η(X)−1|1I(sign(f(X)) =h∗(X))] = IE[|2η(X)−1|1I(f(X)(η(X)−1/2)<0)] ≤2cIE[((1−τ(η(X)))1I(f(X)(η(X)−1/2)<0))γ] ≤2c(IE[(1−τ(η(X)))1I(f(X)(η(X)−1/2)<0)])γ, where the last inequality above follows from Jensen’s inequality. 4/ne}ationslash /ne}ationslashWe are going to show that for any x∈ X, it holds (1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤IE[ϕ(−Yf(x))|X=x]−R∗ ϕ.(1.6) This will clearly imply the result by integrating with respect to x. Recall ﬁrst that IE[ϕ(−Yf(x))|X=x] =Hη(x)(f(x)) and R∗ ϕ= minHη(x)(α) =τ(η(x)). α∈IR so that (2.6) is equivalent to (1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤Hη(x)(α)−τ(η(x)) Since the right-hand side above is nonnegative, the case where f(x)(η(x)−1/2)≥0 follows trivially. If f(x)(η(x)−1/2)<0, (2.6) follows if we prove that Hη(x)(α)≥1. The convexity ofϕ(·) gives Hη(x)(α)",
    "=η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x)) ≥ϕ(−η(x)f(x)+(1−η(x))f(x)) =ϕ((1−2η(x))f(x)) ≥ϕ(0) = 1, where the last inequality follows from the fact that ϕis non decreasing and f(x)(η(x)− 1/2)<0. This completes the proof of (2.6) and thus of the Lemma. IT is not hard to check the following values for the quantities τ(η),candγfor the three losses introduced above: •Hinge loss: τ(η) = 1−|1−2η|withc= 1/2 andγ= 1. •Exponential loss: τ(η) = 2/radicalbig η(1−η) withc= 1/√ 2 andγ= 1/2. •Logistic loss: τ(η) =−ηlogη−(1−η)log(1−η) withc= 1/√ 2 andγ= 1/2. References [Kea90] Michael J Kearns. The computational complexity of machine learning . PhD thesis, Harvard University, 1990. [Zha04] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Ann. Statist. , 32(1):56–85, 2004. 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms",
    "."
  ],
  "ml_math_lect_9.pdf": [
    "18.657: Mathematics of Machine Learning Lecturer: Philippe Rigollet Lecture 9 Scribe:Xuhong Zhang Oct. 7, 2015 Recall that last lecture we talked about convex relaxation of the original problem n1ˆh= argmin 1I(h(Xi) =Yi) h∈Hn/summationdisplay i=1 by considering soft classiﬁers (i.e. whose output is in [ −1,1] rather than in {0,1}) and convex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss): n1ˆ ˆ f= argminRϕ,n(f) = argmin ϕ(Yif(Xi)) f∈F f∈Fn/summationdisplay i=1− ˆ ˆAndh= sign(f) will be used as the ‘hard’ classiﬁer. ˆ ¯ ¯ We want to bound the quantity Rϕ(f)−Rϕ(f), wheref= argminf∈FRϕ(f). ˆ ˆ (1)f= argminf∈FRϕ,n(f), thus ˆ ¯ ˆ¯ˆ¯ˆˆˆˆ ˆ ¯ Rϕ(f) =Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f) ≤¯ˆ¯ˆˆ ˆ ¯ Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f) ≤¯ ˆ Rϕ(f)+2sup |Rϕ,n(f)−Rϕ(f) f∈F| ˆ (2) Let us ﬁrst focus on E[supf∈F|Rϕ,n(f)−Rϕ(f)|]. Using the symmetrization trick as before, weknowit isupper-boundedby2 Rn(ϕ◦F), wheretheRademacher complexity n1Rn(ϕ◦F) = sup",
    "E[sup|/summationdisplay σiϕ(−Yif(Xi)) ] X1,...,Xn,Y1,...,Ynf∈Fni=1| One thing to notice is that ϕ(0) = 1 for the loss functions we consider (hinge loss, exponential loss and logistic loss), but in order to apply contraction inequality later, we requireϕ(0) = 0. Let us deﬁne ψ(·) =ϕ(·)−1. Clearlyψ(0) = 0, and n1E[sup|/summationdisplay (ϕ(−Yif(Xi))−E[ϕ(−Yif(Xi))]) ] f∈Fni=1| n1=E[sup|/summationdisplay (ψ(−Yif(X)−Ei) [ψ( i 1−Yif(X))]) f∈Fni=|] ≤2Rn(ψ◦F) (3) The Rademacher complexity of ψ◦ Fis still diﬃcult to deal with. Let us assume thatϕ(·) isL-Lipschitz, (as a result, ψ(·) is alsoL-Lipschitz), apply the contraction inequality, we have Rn(ψ◦F)≤2LRn(F) 1/ne}ationslash(4) LetZi= (Xi,Yi),i= 1,2,...,nand n1g(Z1,Z2ˆ ,...,Zn) = sup|Rϕ,n(f)−Rϕ(f) = f|sup ∈F f∈F|n/summationdisplay (ϕ( i=1−Yif(Xi))−E[ϕ(−Yif(Xi))])| Sinceϕ(·) ismonotonically increasing, itisnotdiﬃculttoverifythat ∀Z1,Z2,...,Zn,Z′ i 1 2L|g(Z1,...,Zi,...,Zn)−g(Z1,...,Z′ i,...,Zn)| ≤(ϕ(1)−ϕ(n−1))≤n The last inequality holds since",
    "gisL-Lipschitz. Apply Bounded Diﬀerence Inequality, 2t2 P(|s|ˆ ˆ upRϕ,n(f)−Rϕ(f)|−E[sup|Rϕ,n(f)−Rϕ(f)|]> F ∈F|t)≤2exp( ∈− ) f/summationtextnf i=(2L)21n Set the RHS of above equation to δ, we get: log(2/δ)ˆ ˆ supR E ϕ,n(f)Rϕ(f) [sup Rϕ,n(f) f∈F| − | ≤ f∈F| −Rϕ(f)|]+2L/radicalbigg 2n with probability 1 −δ. (5) Combining (1) - (4), we have ˆ ¯Rϕ(f)≤Rϕ(f)+8LRn(F)+2L/radicalbigg log(2/δ) 2n with probability 1 −δ. 1.4 Boosting Inthissection, wewillspecializetheaboveanalysistoaparticularlearningmodel: Boosting. The basic idea of Boosting is to convert a set of weak learners (i.e. classiﬁers that do better than random, but have high error probability) into a strong one by using the weighted average of weak learners’ opinions. More precisely, we consider the following function class M F={/summationdisplay θjhj(·) :|θ|1≤1,hj:X /ma√sto→[−1,1],j∈ {1,2,...,Ma j=1}re classiﬁers } and we want to upper bound Rn(F) for this choice of F. n M n1 1Rn(F) = sup E[sup σiYif(Xi) ] = sup E[ supθjYiσihj(Xi) ]",
    "Z1,...,Znf∈F|n/summationdisplay nZ |θ|1≤11| 1,...,Zi=n|/summationdisplay j=1/summationdisplay i=1| Letg(θ) =|/summationtextM j=1θj/summationtextn i=1Yiσihj(Xi)|. It is easy to see that g(θ) is a convex function, thus sup|θ|1≤1g(θ) is achieved at a vertex of the unit ℓ1ball{θ:/bardblθ/bardbl1≤1}. Deﬁne the ﬁnite set Y1h1(X1) Y1h2(X1) Y1hM(X1)/braceleftiggY2h1(X2) Y2h2(X2)  Y2hM(X2) BX,Y/defines., ,...,. ± .± ±/bracerightigg Ynh1(Xn)   . .. Ynh2(Xn)  .. . YnhM(Xn) 2Then Rn(F) = supRn(BX,Y). X,Y Notice max b∈BX,Yb√| |2≤nand|BX,Y|= 2M. Therefore, using a lemma from Lecture 5, we get 2log(2B 2 4RX,Y)log(M) n(BX,Y)≤max b∈BX,Y|b|2/radicalbig | | n≤/radicalbigg n Thus for Boosting,/bracketleftbig /bracketrightbig 2log(4M) log(2 /δ)ˆ ¯Rϕ(f)≤Rϕ(f)+8L/radicalbigg +2L/radicalbigg with probability 1 - δn 2n To get some ideas of what values Lusually takes, consider the following examples: (1) for hinge loss, i.e. ϕ(x) = (1+x)+,L= 1. (2) for exponential loss, i.e.",
    "ϕ(x) =ex,L=e. (3) for logistic loss, i.e. ϕ(x) = log2(1+ex),L=e 1+elog2(e)≈2.43 ˆ ¯ Now we have bounded Rϕ(f)−Rϕ(f), but this is not yet the excess risk. Excess risk is ˆ deﬁned asR(f)−R(f∗), wheref∗= argminfRϕ(f). The following theorem provides a bound for excess risk for Boosting. Theorem: LetF={/summationtextM j=1θjhj:/bardblθ/bardbl1≤1,hjsare weak classiﬁers }andϕis anL- ˆ ˆ ˆ Lipschitz convex surrogate. Deﬁne f= argminf∈FRϕ,n(f) andh= sign(f). Then γ γ ∗/parenleftbig∗/parenrightbigγ/parenleftigg 2log(4M) log(2/δ)ˆR(h)−R≤2cinfRϕ(f)−Rϕ(f) +2c8L + f∈F/radicalbigg n/parenrightigg 2c/parenleftigg 2L/radicalbigg 2n/parenrightigg with probability 1 −δ Proof. ˆR(h)−R∗≤2c/parenleftbigγRϕ(f)−Rϕ(f∗)/parenrightbig /parenleftiggγ ∗ 2log(4M) log(2 /δ)≤2cinfRϕ(f)Rϕ(f)+8L +2L f∈F−/radicalbigg n/radicalbigg 2n/parenrightigg γ ∗γ 2log(4M) log(2/δ)≤2cinfRϕ(f)−Rϕ(f) +2c f∈F/parenleftigg 8L/radicalbigg n/parenrightigg +2c/parenleftigg 2L/radicalbigg 2n/parenrightiggγ/parenleftbig",
    "/parenrightbig Here the ﬁrst inequality uses Zhang’s lemma and the last one uses the fact that for ai≥0 andγ∈[0,1], (a1+aγ2+a3)≤aγ 1+aγ 2+aγ 3. 1.5 Support Vector Machines In this section, we will apply our analysis to another important learning model: Support Vector Machines (SVMs). We will see that hinge loss ϕ(x) = (1 +x)+is used and the associated function class is F={f:/bardblf/bardblW≤λ}whereWis a Hilbert space. Before analyzing SVMs, let us ﬁrst introduce Reproducing Kernel Hilbert Spaces (RKHS). 31.5.1 Reproducing Kernel Hilbert Spaces (RKHS) Deﬁnition: A function K:X ×X /ma√sto→ IR is called a positive symmetric deﬁnite kernel (PSD kernel) if (1)∀x,x′∈ X,K(x,x′) =K(x′,x) (2)∀n∈Z+,∀x1,x2,...,xn, then th×nmatrix with K(xi,xj) as its element in ithrow andjcolumn is positive semi-deﬁnite. In other words, for any a1,a2,...,an∈IR, /summationdisplay aiajK(xi,xj) 0 i,j≥ Let us look at a few examples of PSD kernels. Example 1 LetX= IR,K(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRdis",
    "a PSD kernel, since ∀a1,a2,...,an∈IR /summationdisplay aiaj/an}bracketle{txi,xj/an}bracketri}htIRd=/summationdisplay /an}bracketle{taixi,ajxj/an}bracketri}htIRd=/an}bracketle{t/summationdisplay aixi,/summationdisplay ajxj/an}bracketri}htIRd=/bardbl/summationdisplay a2ixi/bardblIRd0 i,j i,j i j i≥ Example 2 The Gaussian kernel K(x,x′) = exp(−1 2 2/bardbl′ 2x−x/bardblIRd) is also a PSD kernel.σ Note that here and in the sequel, /bardbl · /bardblWand/an}bracketle{t·,·/an}bracketri}htWdenote the norm and inner product of Hilbert space W. Deﬁnition: LetWbe a Hilbert space of functions X /ma√sto→IR. A symmetric kernel K(·,·) is called reproducing kernel ofWif (1)∀x∈ X, the function K(x,·)∈W. (2)∀x∈ X,f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x). If such aK(x,·) exists,Wis called a reproducing kernel Hilbert space (RKHS). Claim: IfK(·,·) is a reproducing kernel for some Hilbert space W, thenK(·,·) is a PSD kernel. Proof.∀a1,a2,...,an∈IR, we have /summationdisplay aiajK(xi,xj)",
    "=/summationdisplay aiaj/an}bracketle{tK(xi,·),K(xj,·)/an}bracketri}ht(sinceK(,) is reproducing) i,j i,j· · =/an}bracketle{t/summationdisplay aiK(xi,), ajK(xj,)W i·/summationdisplay j· /an}bracketri}ht =/bardbl/summationdisplay aiK(xi, i·)/bardbl2 W≥0 4In fact, the above claim holds both directions, i.e. if a kernel K(·,·) is PSD, it is also a reproducing kernel. A natural question to ask is, given a PSD kernel K(·,·), how can we build the corresponding Hilbert space (for which K(·,·) is a reproducing kernel)? Let us look at a few examples. Example 3 Letϕ1,ϕ2,...,ϕMbe a set of orthonormal functions in L2([0,1]), i.e. for any j,k∈ {1,2,...,M}/integraldisplay ϕj(x)ϕk(x)dx= x/an}bracketle{tϕj,ϕk/an}bracketri}ht=δjk LetK(x,x′) =/summationtextM j=1ϕj(x)ϕj(x′). We claim that the Hilbert space M W={/summationdisplay ajϕj( : =1·)a1,a2,...,aM j∈IR} equipped with inner product /an}bracketle{t·,·/an}bracketri}htL2is a RKHS with reproducing kernel K(·,·). MProof. (1)K(x,·) =j=1ϕj(x)ϕj(·)∈W.",
    "(Chooseaj=ϕj(x)). (2) Iff(·) =/summationtextM j=1aj/summationtext ϕj(·), M M M /an}bracketle{tf(·),K(x,·)/an}bracketri}htL2=/an}bracketle{t/summationdisplay ajϕj(·),/summationdisplay ϕk(x)ϕk(·)/an}bracketri}htL2=/summationdisplay ajϕj(x) =f(x) j=1 k=1 j=1 (3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR, /summationdisplay aiajK(x2i,xj) =/summationdisplay aiajϕk(xi)ϕk(xj) =/summationdisplay (/summationdisplay aiϕk(xi)) i,j i,j,k ki≥0 Example 4 IfX= IRd, andK(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRd, the corresponding Hilbert space is W={/an}bracketle{tw,·/an}bracketri}ht:w∈IRd}(i.e. all linear functions) equipped with the following inner product: iff=/an}bracketle{tw,·/an}bracketri}ht,g=/an}bracketle{tv,·/an}bracketri}ht,/an}bracketle{tf,g/an}bracketri}ht/defines/an}bracketle{tw,v/an}bracketri}htIRd. Proof. (1)∀x∈IRd,K(x,·) =/an}bracketle{tx,·/an}bracketri}htIRd∈W.",
    "(2)∀f=/an}bracketle{tw,·/an}bracketri}htIRd∈W,∀x∈IRd,/an}bracketle{tf,K(x,·)/an}bracketri}ht=/an}bracketle{tw,x/an}bracketri}htIRd=f(x) (3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR, /summationdisplay aiajK(xi,xj) =/summationdisplay aiaj, ,j i,j/an}bracketle{txixj i/an}bracketri}ht=/an}bracketle{t/summationdisplay aixi, i/summationdisplay ajxj j/an}bracketri}htIRd=/bardbl/summationdisplay aix2iIRd0 i/bardbl ≥ 5MIT OpenCourseWare http://ocw.mit.edu 18.657 Mathematics of Machine Learning Fall 2015 For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms ."
  ]
}