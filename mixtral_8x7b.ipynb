{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm4tuKsDzVdYXOBq/g8tWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastianruizsebas/NeuroAI-HackNation/blob/main/mixtral_8x7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "import openai\n",
        "from typing import List\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with our key\n",
        "\n",
        "DATA_DIR = \"./courses\"\n",
        "EMBEDDINGS_FILE = os.path.join(DATA_DIR, \"external_embeddings.jsonl\")\n",
        "INDEX_FILE = \"courses.index\"\n",
        "METADATA_FILE = \"courses_metadata.json\"\n",
        "CHUNK_SIZE = 3000\n",
        "CHUNK_OVERLAP = 200\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "EMBEDDING_DIM = 1536  # ada-002 embeddings\n",
        "TOP_K = 5  # no. chunks to retrieve per query\n",
        "\n",
        "# Extraction Functions\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_html(html_path: str) -> str:\n",
        "    try:\n",
        "        with open(html_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract {html_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Chunking\n",
        "\n",
        "def chunk_text(text: str, max_len=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + max_len, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += max_len - overlap\n",
        "    return chunks\n",
        "\n",
        "# Embedding\n",
        "\n",
        "def get_embedding(text: str) -> List[float]:\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=EMBEDDING_MODEL\n",
        "    )\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "# Processing raw files into embeddings\n",
        "\n",
        "def process_and_embed_data(data_dir: str):\n",
        "    entries = []\n",
        "    for root, _, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            path = os.path.join(root, file)\n",
        "            ext = file.lower().split('.')[-1]\n",
        "            text = \"\"\n",
        "\n",
        "            if ext == \"pdf\":\n",
        "                text = extract_text_from_pdf(path)\n",
        "            elif ext in [\"html\", \"htm\"]:\n",
        "                text = extract_text_from_html(path)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if not text.strip():\n",
        "                continue\n",
        "\n",
        "            chunks = chunk_text(text)\n",
        "            print(f\"Processing {file}: {len(chunks)} chunks\")\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                embedding = get_embedding(chunk)\n",
        "                entry = {\n",
        "                    \"source_file\": path,\n",
        "                    \"chunk_index\": i,\n",
        "                    \"text\": chunk,\n",
        "                    \"embedding\": embedding\n",
        "                }\n",
        "                entries.append(entry)\n",
        "\n",
        "    print(f\"Total chunks embedded: {len(entries)}\")\n",
        "\n",
        "    with open(EMBEDDINGS_FILE, \"w\", encoding='utf-8') as f:\n",
        "        for entry in entries:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE}\")\n",
        "\n",
        "# Build FAISS index from saved embeddings\n",
        "\n",
        "def build_faiss_index(embeddings_file: str):\n",
        "    index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
        "    metadata = []\n",
        "\n",
        "    with open(embeddings_file, \"r\", encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            entry = json.loads(line)\n",
        "            vec = np.array(entry[\"embedding\"], dtype='float32')\n",
        "            index.add(vec.reshape(1, -1))\n",
        "            metadata.append({\n",
        "                \"source_file\": entry[\"source_file\"],\n",
        "                \"chunk_index\": entry[\"chunk_index\"],\n",
        "                \"text\": entry[\"text\"]\n",
        "            })\n",
        "\n",
        "    faiss.write_index(index, INDEX_FILE)\n",
        "    with open(METADATA_FILE, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(metadata, f)\n",
        "    print(f\"Saved FAISS index to {INDEX_FILE} and metadata to {METADATA_FILE}\")\n",
        "\n",
        "# Query / Retrieval pipeline\n",
        "\n",
        "def embed_query(query: str) -> np.ndarray:\n",
        "    response = openai.Embedding.create(\n",
        "        input=query,\n",
        "        model=EMBEDDING_MODEL\n",
        "    )\n",
        "    embedding = response['data'][0]['embedding']\n",
        "    return np.array(embedding, dtype='float32').reshape(1, -1)\n",
        "\n",
        "def load_index_and_metadata():\n",
        "    index = faiss.read_index(INDEX_FILE)\n",
        "    with open(METADATA_FILE, \"r\", encoding='utf-8') as f:\n",
        "        metadata = json.load(f)\n",
        "    return index, metadata\n",
        "\n",
        "def search_index(query: str, top_k=TOP_K):\n",
        "    index, metadata = load_index_and_metadata()\n",
        "    query_vec = embed_query(query)\n",
        "    distances, indices = index.search(query_vec, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        data = metadata[idx]\n",
        "        results.append({\n",
        "            \"source_file\": data[\"source_file\"],\n",
        "            \"chunk_index\": data[\"chunk_index\"],\n",
        "            \"text\": data[\"text\"],\n",
        "            \"score\": float(dist)\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Example lesson content generation integrating retrieval\n",
        "\n",
        "def generate_lesson_with_context(topic: str, user_profile: dict):\n",
        "    # Retrieve relevant chunks from vector DB to provide context\n",
        "    retrieved_chunks = search_index(topic, top_k=5)\n",
        "    context_texts = \"\\n\\n\".join([chunk['text'] for chunk in retrieved_chunks])\n",
        "\n",
        "    competency = user_profile.get('competency_scores', {}).get(topic, 0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a knowledgeable AI tutor.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Context from course materials:\n",
        "{context_texts}\n",
        "\n",
        "Create a lesson for someone with competency level {competency}/10.\n",
        "Structure:\n",
        "1. Brief overview\n",
        "2. Four main learning chunks (2-3 mins each)\n",
        "3. Key takeaways\n",
        "\n",
        "Return ONLY a JSON object like this:\n",
        "{{\n",
        "  \"topic\": \"{topic}\",\n",
        "  \"overview\": \"...\",\n",
        "  \"chunks\": [\n",
        "    {{\n",
        "      \"title\": \"...\",\n",
        "      \"content\": \"...\",\n",
        "      \"key_point\": \"...\"\n",
        "    }}\n",
        "  ],\n",
        "  \"key_takeaways\": [\"...\", \"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    content = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Clean possible markdown JSON block\n",
        "    if content.startswith(\"```json\"):\n",
        "        content = content[7:-3].strip()\n",
        "    elif content.startswith(\"```\"):\n",
        "        content = content[3:-3].strip()\n",
        "\n",
        "    lesson = json.loads(content)\n",
        "    return lesson\n",
        "\n",
        "# Main entry point for full pipeline\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Extract + embed all course files (run once or when updating data)\n",
        "    print(\"Starting data processing and embedding...\")\n",
        "    process_and_embed_data(DATA_DIR)\n",
        "\n",
        "    # Step 2: Build FAISS index from saved embeddings\n",
        "    print(\"Building FAISS index...\")\n",
        "    build_faiss_index(EMBEDDINGS_FILE)\n",
        "\n",
        "    # Step 3: Example interactive query (replace with API integration)\n",
        "    while True:\n",
        "        user_topic = input(\"\\nEnter your learning topic or question (or 'exit'): \")\n",
        "        if user_topic.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Dummy user profile example; replace with real user data\n",
        "        user_profile = {\"competency_scores\": {user_topic: 3}}\n",
        "\n",
        "        print(\"Generating lesson based on your topic...\")\n",
        "        lesson = generate_lesson_with_context(user_topic, user_profile)\n",
        "        print(json.dumps(lesson, indent=2))\n"
      ],
      "metadata": {
        "id": "kOAY7E8uKSXj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}